nohup: ignoring input
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]
Command Output:
[2025-04-25 05:30:57,196] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 05:30:59,115] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-25 05:30:59,115] [INFO] [runner.py:607:main] cmd = /home/zl986/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path /home/zl986/backdoor-test/data2.json --image_folder /home/zl986/backdoor-test/waymo-images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /home/zl986/backdoor-test/llava-driving-ft-4 --num_train_epochs 5 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 100 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2025-04-25 05:31:00,716] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 05:31:02,522] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2025-04-25 05:31:02,522] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2025-04-25 05:31:02,522] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2025-04-25 05:31:02,522] [INFO] [launch.py:164:main] dist_world_size=8
[2025-04-25 05:31:02,522] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-04-25 05:31:02,523] [INFO] [launch.py:256:main] process 75577 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/data2.json', '--image_folder', '/home/zl986/backdoor-test/waymo-images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-4', '--num_train_epochs', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-04-25 05:31:02,524] [INFO] [launch.py:256:main] process 75578 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/data2.json', '--image_folder', '/home/zl986/backdoor-test/waymo-images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-4', '--num_train_epochs', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-04-25 05:31:02,525] [INFO] [launch.py:256:main] process 75579 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/data2.json', '--image_folder', '/home/zl986/backdoor-test/waymo-images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-4', '--num_train_epochs', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-04-25 05:31:02,525] [INFO] [launch.py:256:main] process 75580 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/data2.json', '--image_folder', '/home/zl986/backdoor-test/waymo-images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-4', '--num_train_epochs', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-04-25 05:31:02,526] [INFO] [launch.py:256:main] process 75581 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=4', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/data2.json', '--image_folder', '/home/zl986/backdoor-test/waymo-images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-4', '--num_train_epochs', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-04-25 05:31:02,527] [INFO] [launch.py:256:main] process 75582 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=5', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/data2.json', '--image_folder', '/home/zl986/backdoor-test/waymo-images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-4', '--num_train_epochs', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-04-25 05:31:02,527] [INFO] [launch.py:256:main] process 75583 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=6', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/data2.json', '--image_folder', '/home/zl986/backdoor-test/waymo-images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-4', '--num_train_epochs', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-04-25 05:31:02,528] [INFO] [launch.py:256:main] process 75584 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=7', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/data2.json', '--image_folder', '/home/zl986/backdoor-test/waymo-images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-4', '--num_train_epochs', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-04-25 05:31:07,387] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 05:31:07,415] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 05:31:07,447] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 05:31:07,457] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 05:31:07,572] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 05:31:07,592] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 05:31:07,622] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 05:31:07,684] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 05:31:08,198] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-25 05:31:08,208] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-25 05:31:08,208] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-25 05:31:08,221] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-25 05:31:08,226] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-25 05:31:08,402] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-25 05:31:08,480] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-25 05:31:08,482] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-25 05:31:08,544] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-25 05:31:09,297] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:31:09,394] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:31:09,397] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:31:09,427] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:31:09,485] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:31:09,497] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:31:09,545] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:31:09,588] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:31:10,890] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 295, num_elems = 6.76B
Adding LoRA adapters...
[2025-04-25 05:32:00,225] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:32:00,380] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 686, num_elems = 7.06B
[2025-04-25 05:32:00,675] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:32:00,919] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:32:01,070] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:32:01,451] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:32:01,517] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:32:01,567] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-25 05:32:01,811] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
Formatting inputs...Skip in lazy mode
Parameter Offload: Total persistent parameters: 599040 in 312 params
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 2.0849, 'learning_rate': 5e-05, 'epoch': 0.04}
[2025-04-25 05:32:51,107] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.9885, 'learning_rate': 0.0001, 'epoch': 0.08}
{'loss': 1.6529, 'learning_rate': 0.00015000000000000001, 'epoch': 0.12}
{'loss': 1.2922, 'learning_rate': 0.0002, 'epoch': 0.17}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 1.2, 'learning_rate': 0.0001999633286223284, 'epoch': 0.21}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 1.1344, 'learning_rate': 0.00019985334138511237, 'epoch': 0.25}
{'loss': 1.0393, 'learning_rate': 0.0001996701189560223, 'epoch': 0.29}
{'loss': 0.9512, 'learning_rate': 0.00019941379571543596, 'epoch': 0.33}
[2025-04-25 05:34:24,412] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9522, 'learning_rate': 0.00019908455965788067, 'epoch': 0.38}
{'loss': 0.9757, 'learning_rate': 0.00019868265225415265, 'epoch': 0.42}
{'loss': 0.8556, 'learning_rate': 0.0001982083682742156, 'epoch': 0.46}
{'loss': 0.9301, 'learning_rate': 0.00019766205557100868, 'epoch': 0.5}
{'loss': 0.9606, 'learning_rate': 0.00019704411482532116, 'epoch': 0.54}
[2025-04-25 05:35:31,023] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8861, 'learning_rate': 0.0001963549992519223, 'epoch': 0.58}
{'loss': 0.8052, 'learning_rate': 0.00019559521426716118, 'epoch': 0.62}
{'loss': 0.9249, 'learning_rate': 0.00019476531711828027, 'epoch': 0.67}
{'loss': 0.9612, 'learning_rate': 0.00019386591647471506, 'epoch': 0.71}
{'loss': 0.9086, 'learning_rate': 0.00019289767198167916, 'epoch': 0.75}
{'loss': 0.8748, 'learning_rate': 0.0001918612937763622, 'epoch': 0.79}
{'loss': 0.8142, 'learning_rate': 0.00019075754196709572, 'epoch': 0.83}
{'loss': 0.8877, 'learning_rate': 0.0001895872260758688, 'epoch': 0.88}
{'loss': 0.8534, 'learning_rate': 0.0001883512044446023, 'epoch': 0.92}
{'loss': 0.8299, 'learning_rate': 0.0001870503836056172, 'epoch': 0.96}
[2025-04-25 05:37:43,697] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8742, 'learning_rate': 0.00018568571761675893, 'epoch': 1.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.7601, 'learning_rate': 0.0001842582073616649, 'epoch': 1.04}
{'loss': 0.8074, 'learning_rate': 0.00018276889981568906, 'epoch': 1.08}
{'loss': 0.7551, 'learning_rate': 0.00018121888727802113, 'epoch': 1.12}
{'loss': 0.723, 'learning_rate': 0.00017960930657056438, 'epoch': 1.17}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.7451, 'learning_rate': 0.00017794133820415916, 'epoch': 1.21}
{'loss': 0.7106, 'learning_rate': 0.00017621620551276366, 'epoch': 1.25}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.7077, 'learning_rate': 0.00017443517375622704, 'epoch': 1.29}
{'loss': 0.7549, 'learning_rate': 0.0001725995491923131, 'epoch': 1.33}
{'loss': 0.7291, 'learning_rate': 0.00017071067811865476, 'epoch': 1.38}
{'loss': 0.7273, 'learning_rate': 0.00016876994588534234, 'epoch': 1.42}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.7323, 'learning_rate': 0.00016677877587886956, 'epoch': 1.46}
{'loss': 0.7463, 'learning_rate': 0.00016473862847818277, 'epoch': 1.5}
{'loss': 0.7285, 'learning_rate': 0.00016265099998359866, 'epoch': 1.54}
{'loss': 0.732, 'learning_rate': 0.00016051742151937655, 'epoch': 1.58}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.718, 'learning_rate': 0.00015833945791074943, 'epoch': 1.62}
{'loss': 0.7201, 'learning_rate': 0.00015611870653623825, 'epoch': 1.67}
{'loss': 0.6769, 'learning_rate': 0.00015385679615609042, 'epoch': 1.71}
{'loss': 0.7462, 'learning_rate': 0.00015155538571770218, 'epoch': 1.75}
{'loss': 0.664, 'learning_rate': 0.00014921616313890072, 'epoch': 1.79}
{'loss': 0.7072, 'learning_rate': 0.00014684084406997903, 'epoch': 1.83}
{'loss': 0.7636, 'learning_rate': 0.00014443117063539038, 'epoch': 1.88}
{'loss': 0.7083, 'learning_rate': 0.00014198891015602646, 'epoch': 1.92}
{'loss': 0.7306, 'learning_rate': 0.00013951585385301555, 'epoch': 1.96}
{'loss': 0.6829, 'learning_rate': 0.00013701381553399145, 'epoch': 2.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.6342, 'learning_rate': 0.00013448463026279704, 'epoch': 2.04}
{'loss': 0.6148, 'learning_rate': 0.000131930153013598, 'epoch': 2.08}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.5909, 'learning_rate': 0.00012935225731039348, 'epoch': 2.12}
{'loss': 0.5824, 'learning_rate': 0.00012675283385292212, 'epoch': 2.17}
{'loss': 0.559, 'learning_rate': 0.00012413378912997058, 'epoch': 2.21}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.6353, 'learning_rate': 0.00012149704402110243, 'epoch': 2.25}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.6247, 'learning_rate': 0.00011884453238783185, 'epoch': 2.29}
{'loss': 0.5996, 'learning_rate': 0.0001161781996552765, 'epoch': 2.33}
{'loss': 0.5806, 'learning_rate': 0.00011350000138532902, 'epoch': 2.38}
{'loss': 0.5557, 'learning_rate': 0.00011081190184239419, 'epoch': 2.42}
{'loss': 0.5791, 'learning_rate': 0.00010811587255274313, 'epoch': 2.46}
{'loss': 0.5907, 'learning_rate': 0.00010541389085854176, 'epoch': 2.5}
{'loss': 0.57, 'learning_rate': 0.00010270793846761347, 'epoch': 2.54}
{'loss': 0.5722, 'learning_rate': 0.0001, 'epoch': 2.58}
{'loss': 0.6088, 'learning_rate': 9.729206153238657e-05, 'epoch': 2.62}
{'loss': 0.6087, 'learning_rate': 9.458610914145826e-05, 'epoch': 2.67}
{'loss': 0.5835, 'learning_rate': 9.18841274472569e-05, 'epoch': 2.71}
{'loss': 0.5753, 'learning_rate': 8.918809815760585e-05, 'epoch': 2.75}
{'loss': 0.5861, 'learning_rate': 8.649999861467099e-05, 'epoch': 2.79}
{'loss': 0.5943, 'learning_rate': 8.382180034472353e-05, 'epoch': 2.83}
{'loss': 0.5898, 'learning_rate': 8.115546761216822e-05, 'epoch': 2.88}
{'loss': 0.6027, 'learning_rate': 7.85029559788976e-05, 'epoch': 2.92}
{'loss': 0.6196, 'learning_rate': 7.586621087002945e-05, 'epoch': 2.96}
{'loss': 0.5441, 'learning_rate': 7.324716614707793e-05, 'epoch': 3.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.4962, 'learning_rate': 7.064774268960653e-05, 'epoch': 3.04}
{'loss': 0.5034, 'learning_rate': 6.806984698640202e-05, 'epoch': 3.08}
{'loss': 0.4662, 'learning_rate': 6.551536973720298e-05, 'epoch': 3.12}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.489, 'learning_rate': 6.298618446600856e-05, 'epoch': 3.17}
{'loss': 0.4752, 'learning_rate': 6.048414614698448e-05, 'epoch': 3.21}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.4468, 'learning_rate': 5.801108984397354e-05, 'epoch': 3.25}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.4186, 'learning_rate': 5.5568829364609664e-05, 'epoch': 3.29}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.431, 'learning_rate': 5.3159155930021e-05, 'epoch': 3.33}
{'loss': 0.4619, 'learning_rate': 5.078383686109926e-05, 'epoch': 3.38}
{'loss': 0.4507, 'learning_rate': 4.844461428229782e-05, 'epoch': 3.42}
{'loss': 0.4434, 'learning_rate': 4.614320384390959e-05, 'epoch': 3.46}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.4549, 'learning_rate': 4.388129346376178e-05, 'epoch': 3.5}
{'loss': 0.4467, 'learning_rate': 4.16605420892506e-05, 'epoch': 3.54}
{'loss': 0.4658, 'learning_rate': 3.948257848062351e-05, 'epoch': 3.58}
{'loss': 0.4491, 'learning_rate': 3.734900001640135e-05, 'epoch': 3.62}
{'loss': 0.4648, 'learning_rate': 3.5261371521817244e-05, 'epoch': 3.67}
{'loss': 0.4189, 'learning_rate': 3.322122412113047e-05, 'epoch': 3.71}
{'loss': 0.4238, 'learning_rate': 3.123005411465766e-05, 'epoch': 3.75}
{'loss': 0.4617, 'learning_rate': 2.9289321881345254e-05, 'epoch': 3.79}
{'loss': 0.4213, 'learning_rate': 2.7400450807686938e-05, 'epoch': 3.83}
{'loss': 0.4311, 'learning_rate': 2.5564826243772966e-05, 'epoch': 3.88}
{'loss': 0.4695, 'learning_rate': 2.3783794487236365e-05, 'epoch': 3.92}
{'loss': 0.4614, 'learning_rate': 2.205866179584084e-05, 'epoch': 3.96}
{'loss': 0.3882, 'learning_rate': 2.0390693429435627e-05, 'epoch': 4.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.3651, 'learning_rate': 1.87811127219789e-05, 'epoch': 4.04}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.3595, 'learning_rate': 1.7231100184310956e-05, 'epoch': 4.08}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.3227, 'learning_rate': 1.5741792638335095e-05, 'epoch': 4.12}
{'loss': 0.3635, 'learning_rate': 1.4314282383241096e-05, 'epoch': 4.17}
{'loss': 0.354, 'learning_rate': 1.2949616394382802e-05, 'epoch': 4.21}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.3636, 'learning_rate': 1.1648795555397719e-05, 'epoch': 4.25}
{'loss': 0.3426, 'learning_rate': 1.0412773924131203e-05, 'epoch': 4.29}
{'loss': 0.3527, 'learning_rate': 9.242458032904311e-06, 'epoch': 4.33}
{'loss': 0.3251, 'learning_rate': 8.138706223637827e-06, 'epoch': 4.38}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.3422, 'learning_rate': 7.102328018320858e-06, 'epoch': 4.42}
{'loss': 0.3397, 'learning_rate': 6.13408352528495e-06, 'epoch': 4.46}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.3375, 'learning_rate': 5.2346828817197655e-06, 'epoch': 4.5}
{'loss': 0.3319, 'learning_rate': 4.404785732838846e-06, 'epoch': 4.54}
{'loss': 0.3453, 'learning_rate': 3.6450007480777093e-06, 'epoch': 4.58}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.3431, 'learning_rate': 2.9558851746788517e-06, 'epoch': 4.62}
{'loss': 0.3365, 'learning_rate': 2.3379444289913342e-06, 'epoch': 4.67}
{'loss': 0.338, 'learning_rate': 1.7916317257844039e-06, 'epoch': 4.71}
{'loss': 0.355, 'learning_rate': 1.317347745847386e-06, 'epoch': 4.75}
{'loss': 0.3506, 'learning_rate': 9.154403421193225e-07, 'epoch': 4.79}
{'loss': 0.3092, 'learning_rate': 5.862042845640403e-07, 'epoch': 4.83}
{'loss': 0.326, 'learning_rate': 3.298810439777311e-07, 'epoch': 4.88}
{'loss': 0.3447, 'learning_rate': 1.4665861488761813e-07, 'epoch': 4.92}
{'loss': 0.3162, 'learning_rate': 3.667137767160433e-08, 'epoch': 4.96}
{'loss': 0.3059, 'learning_rate': 0.0, 'epoch': 5.0}
{'train_runtime': 1648.4119, 'train_samples_per_second': 9.075, 'train_steps_per_second': 0.073, 'train_loss': 0.6360589392483235, 'epoch': 5.0}
[2025-04-25 05:59:55,971] [INFO] [launch.py:351:main] Process 75578 exits successfully.
[2025-04-25 05:59:55,972] [INFO] [launch.py:351:main] Process 75580 exits successfully.
[2025-04-25 05:59:55,973] [INFO] [launch.py:351:main] Process 75582 exits successfully.
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mvibrant-feather-43[0m at: [34mhttps://wandb.ai/jt828-cornell-university/huggingface/runs/ux9vmvz4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250425_053207-ux9vmvz4/logs[0m
[2025-04-25 05:59:56,974] [INFO] [launch.py:351:main] Process 75584 exits successfully.
[2025-04-25 05:59:56,975] [INFO] [launch.py:351:main] Process 75583 exits successfully.
[2025-04-25 05:59:56,975] [INFO] [launch.py:351:main] Process 75581 exits successfully.
[2025-04-25 05:59:56,976] [INFO] [launch.py:351:main] Process 75579 exits successfully.
[2025-04-25 05:59:58,977] [INFO] [launch.py:351:main] Process 75577 exits successfully.

Error Output:
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.56s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.51s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.64s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.41s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.73s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.73s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.71s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.07s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.66s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.10s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.57s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.99s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.14s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.15s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.69s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.15s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.67s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.13s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00,  9.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.21s/it]
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jt828 (jt828-cornell-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/zl986/backdoor-test/LLaVA/wandb/run-20250425_053207-ux9vmvz4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-feather-43
wandb: ⭐️ View project at https://wandb.ai/jt828-cornell-university/huggingface
wandb: 🚀 View run at https://wandb.ai/jt828-cornell-university/huggingface/runs/ux9vmvz4

  0%|          | 0/120 [00:00<?, ?it/s]/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

  1%|          | 1/120 [00:28<57:03, 28.77s/it]
                                               

  1%|          | 1/120 [00:28<57:03, 28.77s/it]
  2%|▏         | 2/120 [00:42<39:08, 19.91s/it]
                                               

  2%|▏         | 2/120 [00:42<39:08, 19.91s/it]
  2%|▎         | 3/120 [00:55<32:58, 16.91s/it]
                                               

  2%|▎         | 3/120 [00:55<32:58, 16.91s/it]
  3%|▎         | 4/120 [01:09<29:55, 15.48s/it]
                                               

  3%|▎         | 4/120 [01:09<29:55, 15.48s/it]
  4%|▍         | 5/120 [01:22<28:19, 14.78s/it]
                                               

  4%|▍         | 5/120 [01:22<28:19, 14.78s/it]
  5%|▌         | 6/120 [01:35<27:03, 14.25s/it]
                                               

  5%|▌         | 6/120 [01:35<27:03, 14.25s/it]
  6%|▌         | 7/120 [01:49<26:13, 13.93s/it]
                                               

  6%|▌         | 7/120 [01:49<26:13, 13.93s/it]
  7%|▋         | 8/120 [02:02<25:35, 13.71s/it]
                                               

  7%|▋         | 8/120 [02:02<25:35, 13.71s/it]
  8%|▊         | 9/120 [02:15<25:11, 13.61s/it]
                                               

  8%|▊         | 9/120 [02:15<25:11, 13.61s/it]
  8%|▊         | 10/120 [02:29<24:45, 13.51s/it]
                                                

  8%|▊         | 10/120 [02:29<24:45, 13.51s/it]
  9%|▉         | 11/120 [02:42<24:26, 13.45s/it]
                                                

  9%|▉         | 11/120 [02:42<24:26, 13.45s/it]
 10%|█         | 12/120 [02:55<24:05, 13.39s/it]
                                                

 10%|█         | 12/120 [02:55<24:05, 13.39s/it]
 11%|█         | 13/120 [03:08<23:48, 13.35s/it]
                                                

 11%|█         | 13/120 [03:08<23:48, 13.35s/it]
 12%|█▏        | 14/120 [03:22<23:40, 13.40s/it]
                                                

 12%|█▏        | 14/120 [03:22<23:40, 13.40s/it]
 12%|█▎        | 15/120 [03:35<23:22, 13.36s/it]
                                                

 12%|█▎        | 15/120 [03:35<23:22, 13.36s/it]
 13%|█▎        | 16/120 [03:48<23:04, 13.31s/it]
                                                

 13%|█▎        | 16/120 [03:48<23:04, 13.31s/it]
 14%|█▍        | 17/120 [04:02<22:48, 13.28s/it]
                                                

 14%|█▍        | 17/120 [04:02<22:48, 13.28s/it]
 15%|█▌        | 18/120 [04:15<22:31, 13.25s/it]
                                                

 15%|█▌        | 18/120 [04:15<22:31, 13.25s/it]
 16%|█▌        | 19/120 [04:28<22:15, 13.22s/it]
                                                

 16%|█▌        | 19/120 [04:28<22:15, 13.22s/it]
 17%|█▋        | 20/120 [04:41<22:03, 13.24s/it]
                                                

 17%|█▋        | 20/120 [04:41<22:03, 13.24s/it]
 18%|█▊        | 21/120 [04:54<21:50, 13.23s/it]
                                                

 18%|█▊        | 21/120 [04:54<21:50, 13.23s/it]
 18%|█▊        | 22/120 [05:08<21:38, 13.25s/it]
                                                

 18%|█▊        | 22/120 [05:08<21:38, 13.25s/it]
 19%|█▉        | 23/120 [05:21<21:22, 13.23s/it]
                                                

 19%|█▉        | 23/120 [05:21<21:22, 13.23s/it]
 20%|██        | 24/120 [05:35<21:23, 13.37s/it]
                                                

 20%|██        | 24/120 [05:35<21:23, 13.37s/it]
 21%|██        | 25/120 [05:52<22:58, 14.51s/it]
                                                

 21%|██        | 25/120 [05:52<22:58, 14.51s/it]
 22%|██▏       | 26/120 [06:05<22:13, 14.18s/it]
                                                

 22%|██▏       | 26/120 [06:05<22:13, 14.18s/it]
 22%|██▎       | 27/120 [06:18<21:34, 13.92s/it]
                                                

 22%|██▎       | 27/120 [06:18<21:34, 13.92s/it]
 23%|██▎       | 28/120 [06:32<20:59, 13.69s/it]
                                                

 23%|██▎       | 28/120 [06:32<20:59, 13.69s/it]
 24%|██▍       | 29/120 [06:45<20:34, 13.56s/it]
                                                

 24%|██▍       | 29/120 [06:45<20:34, 13.56s/it]
 25%|██▌       | 30/120 [06:58<20:12, 13.47s/it]
                                                

 25%|██▌       | 30/120 [06:58<20:12, 13.47s/it]
 26%|██▌       | 31/120 [07:11<19:55, 13.43s/it]
                                                

 26%|██▌       | 31/120 [07:11<19:55, 13.43s/it]
 27%|██▋       | 32/120 [07:25<19:36, 13.37s/it]
                                                

 27%|██▋       | 32/120 [07:25<19:36, 13.37s/it]
 28%|██▊       | 33/120 [07:38<19:23, 13.38s/it]
                                                

 28%|██▊       | 33/120 [07:38<19:23, 13.38s/it]
 28%|██▊       | 34/120 [07:51<19:05, 13.33s/it]
                                                

 28%|██▊       | 34/120 [07:51<19:05, 13.33s/it]
 29%|██▉       | 35/120 [08:05<18:51, 13.31s/it]
                                                

 29%|██▉       | 35/120 [08:05<18:51, 13.31s/it]
 30%|███       | 36/120 [08:18<18:35, 13.27s/it]
                                                

 30%|███       | 36/120 [08:18<18:35, 13.27s/it]
 31%|███       | 37/120 [08:31<18:22, 13.29s/it]
                                                

 31%|███       | 37/120 [08:31<18:22, 13.29s/it]
 32%|███▏      | 38/120 [08:44<18:10, 13.30s/it]
                                                

 32%|███▏      | 38/120 [08:44<18:10, 13.30s/it]
 32%|███▎      | 39/120 [08:58<17:57, 13.30s/it]
                                                

 32%|███▎      | 39/120 [08:58<17:57, 13.30s/it]
 33%|███▎      | 40/120 [09:11<17:42, 13.28s/it]
                                                

 33%|███▎      | 40/120 [09:11<17:42, 13.28s/it]
 34%|███▍      | 41/120 [09:24<17:29, 13.28s/it]
                                                

 34%|███▍      | 41/120 [09:24<17:29, 13.28s/it]
 35%|███▌      | 42/120 [09:37<17:14, 13.26s/it]
                                                

 35%|███▌      | 42/120 [09:37<17:14, 13.26s/it]
 36%|███▌      | 43/120 [09:51<17:01, 13.27s/it]
                                                

 36%|███▌      | 43/120 [09:51<17:01, 13.27s/it]
 37%|███▋      | 44/120 [10:04<16:46, 13.24s/it]
                                                

 37%|███▋      | 44/120 [10:04<16:46, 13.24s/it]
 38%|███▊      | 45/120 [10:17<16:33, 13.25s/it]
                                                

 38%|███▊      | 45/120 [10:17<16:33, 13.25s/it]
 38%|███▊      | 46/120 [10:30<16:18, 13.22s/it]
                                                

 38%|███▊      | 46/120 [10:30<16:18, 13.22s/it]
 39%|███▉      | 47/120 [10:43<16:03, 13.20s/it]
                                                

 39%|███▉      | 47/120 [10:43<16:03, 13.20s/it]
 40%|████      | 48/120 [10:57<15:52, 13.22s/it]
                                                

 40%|████      | 48/120 [10:57<15:52, 13.22s/it]
 41%|████      | 49/120 [11:14<17:02, 14.41s/it]
                                                

 41%|████      | 49/120 [11:14<17:02, 14.41s/it]
 42%|████▏     | 50/120 [11:27<16:24, 14.07s/it]
                                                

 42%|████▏     | 50/120 [11:27<16:24, 14.07s/it]
 42%|████▎     | 51/120 [11:40<15:55, 13.84s/it]
                                                

 42%|████▎     | 51/120 [11:40<15:55, 13.84s/it]
 43%|████▎     | 52/120 [11:54<15:29, 13.67s/it]
                                                

 43%|████▎     | 52/120 [11:54<15:29, 13.67s/it]
 44%|████▍     | 53/120 [12:07<15:07, 13.54s/it]
                                                

 44%|████▍     | 53/120 [12:07<15:07, 13.54s/it]
 45%|████▌     | 54/120 [12:20<14:48, 13.46s/it]
                                                

 45%|████▌     | 54/120 [12:20<14:48, 13.46s/it]
 46%|████▌     | 55/120 [12:33<14:29, 13.38s/it]
                                                

 46%|████▌     | 55/120 [12:33<14:29, 13.38s/it]
 47%|████▋     | 56/120 [12:47<14:13, 13.33s/it]
                                                

 47%|████▋     | 56/120 [12:47<14:13, 13.33s/it]
 48%|████▊     | 57/120 [13:00<13:58, 13.31s/it]
                                                

 48%|████▊     | 57/120 [13:00<13:58, 13.31s/it]
 48%|████▊     | 58/120 [13:13<13:42, 13.27s/it]
                                                

 48%|████▊     | 58/120 [13:13<13:42, 13.27s/it]
 49%|████▉     | 59/120 [13:26<13:30, 13.29s/it]
                                                

 49%|████▉     | 59/120 [13:26<13:30, 13.29s/it]
 50%|█████     | 60/120 [13:40<13:18, 13.30s/it]
                                                

 50%|█████     | 60/120 [13:40<13:18, 13.30s/it]
 51%|█████     | 61/120 [13:53<13:04, 13.30s/it]
                                                

 51%|█████     | 61/120 [13:53<13:04, 13.30s/it]
 52%|█████▏    | 62/120 [14:06<12:51, 13.29s/it]
                                                

 52%|█████▏    | 62/120 [14:06<12:51, 13.29s/it]
 52%|█████▎    | 63/120 [14:20<12:37, 13.30s/it]
                                                

 52%|█████▎    | 63/120 [14:20<12:37, 13.30s/it]
 53%|█████▎    | 64/120 [14:33<12:25, 13.31s/it]
                                                

 53%|█████▎    | 64/120 [14:33<12:25, 13.31s/it]
 54%|█████▍    | 65/120 [14:46<12:09, 13.26s/it]
                                                

 54%|█████▍    | 65/120 [14:46<12:09, 13.26s/it]
 55%|█████▌    | 66/120 [14:59<11:56, 13.27s/it]
                                                

 55%|█████▌    | 66/120 [14:59<11:56, 13.27s/it]
 56%|█████▌    | 67/120 [15:13<11:42, 13.26s/it]
                                                

 56%|█████▌    | 67/120 [15:13<11:42, 13.26s/it]
 57%|█████▋    | 68/120 [15:26<11:29, 13.26s/it]
                                                

 57%|█████▋    | 68/120 [15:26<11:29, 13.26s/it]
 57%|█████▊    | 69/120 [15:39<11:15, 13.25s/it]
                                                

 57%|█████▊    | 69/120 [15:39<11:15, 13.25s/it]
 58%|█████▊    | 70/120 [15:52<11:01, 13.23s/it]
                                                

 58%|█████▊    | 70/120 [15:52<11:01, 13.23s/it]
 59%|█████▉    | 71/120 [16:06<10:47, 13.21s/it]
                                                

 59%|█████▉    | 71/120 [16:06<10:47, 13.21s/it]
 60%|██████    | 72/120 [16:19<10:36, 13.27s/it]
                                                

 60%|██████    | 72/120 [16:19<10:36, 13.27s/it]
 61%|██████    | 73/120 [16:36<11:22, 14.52s/it]
                                                

 61%|██████    | 73/120 [16:36<11:22, 14.52s/it]
 62%|██████▏   | 74/120 [16:50<10:51, 14.16s/it]
                                                

 62%|██████▏   | 74/120 [16:50<10:51, 14.16s/it]
 62%|██████▎   | 75/120 [17:03<10:24, 13.88s/it]
                                                

 62%|██████▎   | 75/120 [17:03<10:24, 13.88s/it]
 63%|██████▎   | 76/120 [17:16<10:03, 13.72s/it]
                                                

 63%|██████▎   | 76/120 [17:16<10:03, 13.72s/it]
 64%|██████▍   | 77/120 [17:29<09:43, 13.56s/it]
                                                

 64%|██████▍   | 77/120 [17:29<09:43, 13.56s/it]
 65%|██████▌   | 78/120 [17:43<09:25, 13.46s/it]
                                                

 65%|██████▌   | 78/120 [17:43<09:25, 13.46s/it]
 66%|██████▌   | 79/120 [17:56<09:10, 13.42s/it]
                                                

 66%|██████▌   | 79/120 [17:56<09:10, 13.42s/it]
 67%|██████▋   | 80/120 [18:09<08:55, 13.38s/it]
                                                

 67%|██████▋   | 80/120 [18:09<08:55, 13.38s/it]
 68%|██████▊   | 81/120 [18:23<08:39, 13.33s/it]
                                                

 68%|██████▊   | 81/120 [18:23<08:39, 13.33s/it]
 68%|██████▊   | 82/120 [18:36<08:26, 13.32s/it]
                                                

 68%|██████▊   | 82/120 [18:36<08:26, 13.32s/it]
 69%|██████▉   | 83/120 [18:49<08:11, 13.29s/it]
                                                

 69%|██████▉   | 83/120 [18:49<08:11, 13.29s/it]
 70%|███████   | 84/120 [19:02<07:57, 13.27s/it]
                                                

 70%|███████   | 84/120 [19:02<07:57, 13.27s/it]
 71%|███████   | 85/120 [19:15<07:43, 13.24s/it]
                                                

 71%|███████   | 85/120 [19:15<07:43, 13.24s/it]
 72%|███████▏  | 86/120 [19:29<07:31, 13.28s/it]
                                                

 72%|███████▏  | 86/120 [19:29<07:31, 13.28s/it]
 72%|███████▎  | 87/120 [19:42<07:18, 13.29s/it]
                                                

 72%|███████▎  | 87/120 [19:42<07:18, 13.29s/it]
 73%|███████▎  | 88/120 [19:55<07:04, 13.28s/it]
                                                

 73%|███████▎  | 88/120 [19:55<07:04, 13.28s/it]
 74%|███████▍  | 89/120 [20:09<06:51, 13.28s/it]
                                                

 74%|███████▍  | 89/120 [20:09<06:51, 13.28s/it]
 75%|███████▌  | 90/120 [20:22<06:37, 13.26s/it]
                                                

 75%|███████▌  | 90/120 [20:22<06:37, 13.26s/it]
 76%|███████▌  | 91/120 [20:35<06:24, 13.24s/it]
                                                

 76%|███████▌  | 91/120 [20:35<06:24, 13.24s/it]
 77%|███████▋  | 92/120 [20:48<06:10, 13.24s/it]
                                                

 77%|███████▋  | 92/120 [20:48<06:10, 13.24s/it]
 78%|███████▊  | 93/120 [21:02<05:57, 13.25s/it]
                                                

 78%|███████▊  | 93/120 [21:02<05:57, 13.25s/it]
 78%|███████▊  | 94/120 [21:15<05:44, 13.25s/it]
                                                

 78%|███████▊  | 94/120 [21:15<05:44, 13.25s/it]
 79%|███████▉  | 95/120 [21:28<05:30, 13.24s/it]
                                                

 79%|███████▉  | 95/120 [21:28<05:30, 13.24s/it]
 80%|████████  | 96/120 [21:42<05:20, 13.37s/it]
                                                

 80%|████████  | 96/120 [21:42<05:20, 13.37s/it]
 81%|████████  | 97/120 [21:59<05:34, 14.52s/it]
                                                

 81%|████████  | 97/120 [21:59<05:34, 14.52s/it]
 82%|████████▏ | 98/120 [22:12<05:11, 14.14s/it]
                                                

 82%|████████▏ | 98/120 [22:12<05:11, 14.14s/it]
 82%|████████▎ | 99/120 [22:25<04:51, 13.88s/it]
                                                

 82%|████████▎ | 99/120 [22:25<04:51, 13.88s/it]
 83%|████████▎ | 100/120 [22:39<04:33, 13.69s/it]
                                                 

 83%|████████▎ | 100/120 [22:39<04:33, 13.69s/it]/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 84%|████████▍ | 101/120 [23:15<06:26, 20.36s/it]
                                                 

 84%|████████▍ | 101/120 [23:15<06:26, 20.36s/it]
 85%|████████▌ | 102/120 [23:28<05:28, 18.28s/it]
                                                 

 85%|████████▌ | 102/120 [23:28<05:28, 18.28s/it]
 86%|████████▌ | 103/120 [23:41<04:45, 16.77s/it]
                                                 

 86%|████████▌ | 103/120 [23:41<04:45, 16.77s/it]
 87%|████████▋ | 104/120 [23:54<04:10, 15.69s/it]
                                                 

 87%|████████▋ | 104/120 [23:54<04:10, 15.69s/it]
 88%|████████▊ | 105/120 [24:08<03:44, 14.96s/it]
                                                 

 88%|████████▊ | 105/120 [24:08<03:44, 14.96s/it]
 88%|████████▊ | 106/120 [24:21<03:21, 14.42s/it]
                                                 

 88%|████████▊ | 106/120 [24:21<03:21, 14.42s/it]
 89%|████████▉ | 107/120 [24:34<03:02, 14.06s/it]
                                                 

 89%|████████▉ | 107/120 [24:34<03:02, 14.06s/it]
 90%|█████████ | 108/120 [24:47<02:45, 13.83s/it]
                                                 

 90%|█████████ | 108/120 [24:47<02:45, 13.83s/it]
 91%|█████████ | 109/120 [25:01<02:30, 13.64s/it]
                                                 

 91%|█████████ | 109/120 [25:01<02:30, 13.64s/it]
 92%|█████████▏| 110/120 [25:14<02:15, 13.52s/it]
                                                 

 92%|█████████▏| 110/120 [25:14<02:15, 13.52s/it]
 92%|█████████▎| 111/120 [25:27<02:00, 13.42s/it]
                                                 

 92%|█████████▎| 111/120 [25:27<02:00, 13.42s/it]
 93%|█████████▎| 112/120 [25:40<01:46, 13.36s/it]
                                                 

 93%|█████████▎| 112/120 [25:40<01:46, 13.36s/it]
 94%|█████████▍| 113/120 [25:54<01:33, 13.35s/it]
                                                 

 94%|█████████▍| 113/120 [25:54<01:33, 13.35s/it]
 95%|█████████▌| 114/120 [26:07<01:19, 13.28s/it]
                                                 

 95%|█████████▌| 114/120 [26:07<01:19, 13.28s/it]
 96%|█████████▌| 115/120 [26:20<01:06, 13.27s/it]
                                                 

 96%|█████████▌| 115/120 [26:20<01:06, 13.27s/it]
 97%|█████████▋| 116/120 [26:33<00:53, 13.25s/it]
                                                 

 97%|█████████▋| 116/120 [26:33<00:53, 13.25s/it]
 98%|█████████▊| 117/120 [26:46<00:39, 13.22s/it]
                                                 

 98%|█████████▊| 117/120 [26:46<00:39, 13.22s/it]
 98%|█████████▊| 118/120 [26:59<00:26, 13.22s/it]
                                                 

 98%|█████████▊| 118/120 [26:59<00:26, 13.22s/it]
 99%|█████████▉| 119/120 [27:13<00:13, 13.22s/it]
                                                 

 99%|█████████▉| 119/120 [27:13<00:13, 13.22s/it]
100%|██████████| 120/120 [27:26<00:00, 13.27s/it]
                                                 

100%|██████████| 120/120 [27:26<00:00, 13.27s/it]
                                                 

100%|██████████| 120/120 [27:26<00:00, 13.27s/it]
100%|██████████| 120/120 [27:26<00:00, 13.72s/it]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}


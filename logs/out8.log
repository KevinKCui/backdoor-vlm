nohup: ignoring input
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]
Command Output:
[2025-05-03 10:01:56,031] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 10:01:57,979] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=3,4,5,6,7: setting --include=localhost:3,4,5,6,7
[2025-05-03 10:01:57,979] [INFO] [runner.py:607:main] cmd = /home/zl986/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path /home/zl986/backdoor-test/train2.json --image_folder /home/zl986/backdoor-test/all --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /home/zl986/backdoor-test/llava-driving-ft-8 --num_train_epochs 25 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 2000 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2025-05-03 10:01:59,923] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 10:02:01,785] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [3, 4, 5, 6, 7]}
[2025-05-03 10:02:01,785] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=5, node_rank=0
[2025-05-03 10:02:01,785] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4]})
[2025-05-03 10:02:01,785] [INFO] [launch.py:164:main] dist_world_size=5
[2025-05-03 10:02:01,785] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=3,4,5,6,7
[2025-05-03 10:02:01,786] [INFO] [launch.py:256:main] process 65942 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/train2.json', '--image_folder', '/home/zl986/backdoor-test/all', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-8', '--num_train_epochs', '25', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-05-03 10:02:01,787] [INFO] [launch.py:256:main] process 65943 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/train2.json', '--image_folder', '/home/zl986/backdoor-test/all', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-8', '--num_train_epochs', '25', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-05-03 10:02:01,788] [INFO] [launch.py:256:main] process 65944 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/train2.json', '--image_folder', '/home/zl986/backdoor-test/all', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-8', '--num_train_epochs', '25', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-05-03 10:02:01,788] [INFO] [launch.py:256:main] process 65945 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/train2.json', '--image_folder', '/home/zl986/backdoor-test/all', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-8', '--num_train_epochs', '25', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-05-03 10:02:01,789] [INFO] [launch.py:256:main] process 65946 spawned with command: ['/home/zl986/miniconda3/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=4', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/home/zl986/backdoor-test/train2.json', '--image_folder', '/home/zl986/backdoor-test/all', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/zl986/backdoor-test/llava-driving-ft-8', '--num_train_epochs', '25', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2025-05-03 10:02:06,444] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 10:02:06,445] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 10:02:06,464] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 10:02:06,469] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 10:02:06,634] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 10:02:07,189] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-03 10:02:07,257] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-03 10:02:07,268] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-03 10:02:07,279] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-03 10:02:07,280] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-03 10:02:07,457] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-03 10:02:08,201] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 5
[2025-05-03 10:02:08,571] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 5
[2025-05-03 10:02:08,686] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 5
[2025-05-03 10:02:08,708] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 5
[2025-05-03 10:02:08,922] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 5
[2025-05-03 10:02:10,197] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 295, num_elems = 6.76B
Adding LoRA adapters...
[2025-05-03 10:02:48,277] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 5
[2025-05-03 10:02:48,448] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 686, num_elems = 7.06B
[2025-05-03 10:02:48,762] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 5
[2025-05-03 10:02:48,942] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 5
[2025-05-03 10:02:49,452] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 5
[2025-05-03 10:02:49,483] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 5
Formatting inputs...Skip in lazy mode
Parameter Offload: Total persistent parameters: 599040 in 312 params
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 2.201, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.01}
[2025-05-03 10:03:47,263] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 2.2293, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.03}
[2025-05-03 10:04:04,404] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 2.3178, 'learning_rate': 1e-05, 'epoch': 0.04}
[2025-05-03 10:04:22,162] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 2.2244, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.05}
[2025-05-03 10:04:39,285] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 2.1403, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.06}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:04:57,122] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 2.0016, 'learning_rate': 2e-05, 'epoch': 0.07}
[2025-05-03 10:05:14,147] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.8397, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.09}
[2025-05-03 10:05:31,178] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.6226, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.1}
[2025-05-03 10:05:48,139] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.4993, 'learning_rate': 3e-05, 'epoch': 0.11}
[2025-05-03 10:06:05,191] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.4282, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.12}
[2025-05-03 10:06:22,193] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.3319, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.14}
[2025-05-03 10:06:39,153] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.3192, 'learning_rate': 4e-05, 'epoch': 0.15}
[2025-05-03 10:06:56,146] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.1939, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.16}
[2025-05-03 10:07:13,070] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.1832, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.17}
[2025-05-03 10:07:30,074] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.2225, 'learning_rate': 5e-05, 'epoch': 0.19}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:07:46,936] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.1484, 'learning_rate': 5.333333333333333e-05, 'epoch': 0.2}
[2025-05-03 10:08:03,902] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.1355, 'learning_rate': 5.666666666666667e-05, 'epoch': 0.21}
[2025-05-03 10:08:20,812] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0879, 'learning_rate': 6e-05, 'epoch': 0.23}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:08:37,801] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0524, 'learning_rate': 6.333333333333333e-05, 'epoch': 0.24}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:08:54,645] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9998, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.25}
[2025-05-03 10:09:11,591] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0683, 'learning_rate': 7e-05, 'epoch': 0.26}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:09:28,417] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0922, 'learning_rate': 7.333333333333333e-05, 'epoch': 0.28}
[2025-05-03 10:09:45,521] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0125, 'learning_rate': 7.666666666666667e-05, 'epoch': 0.29}
[2025-05-03 10:10:02,391] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0935, 'learning_rate': 8e-05, 'epoch': 0.3}
[2025-05-03 10:10:20,155] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0485, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.31}
{'loss': 1.0114, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.33}
[2025-05-03 10:10:53,913] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.992, 'learning_rate': 9e-05, 'epoch': 0.34}
[2025-05-03 10:11:10,880] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0464, 'learning_rate': 9.333333333333334e-05, 'epoch': 0.35}
[2025-05-03 10:11:27,835] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.882, 'learning_rate': 9.666666666666667e-05, 'epoch': 0.36}
[2025-05-03 10:11:44,900] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9609, 'learning_rate': 0.0001, 'epoch': 0.38}
[2025-05-03 10:12:01,767] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9309, 'learning_rate': 0.00010333333333333334, 'epoch': 0.39}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.9642, 'learning_rate': 0.00010666666666666667, 'epoch': 0.4}
[2025-05-03 10:12:35,494] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.952, 'learning_rate': 0.00011000000000000002, 'epoch': 0.41}
{'loss': 0.9247, 'learning_rate': 0.00011333333333333334, 'epoch': 0.42}
[2025-05-03 10:13:09,187] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9514, 'learning_rate': 0.00011666666666666668, 'epoch': 0.44}
[2025-05-03 10:13:26,091] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9773, 'learning_rate': 0.00012, 'epoch': 0.45}
{'loss': 0.9711, 'learning_rate': 0.00012333333333333334, 'epoch': 0.46}
{'loss': 0.9281, 'learning_rate': 0.00012666666666666666, 'epoch': 0.47}
{'loss': 0.9615, 'learning_rate': 0.00013000000000000002, 'epoch': 0.49}
{'loss': 0.9829, 'learning_rate': 0.00013333333333333334, 'epoch': 0.5}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.9892, 'learning_rate': 0.00013666666666666666, 'epoch': 0.51}
{'loss': 0.9296, 'learning_rate': 0.00014, 'epoch': 0.53}
{'loss': 0.9624, 'learning_rate': 0.00014333333333333334, 'epoch': 0.54}
{'loss': 0.9283, 'learning_rate': 0.00014666666666666666, 'epoch': 0.55}
{'loss': 0.9425, 'learning_rate': 0.00015000000000000001, 'epoch': 0.56}
{'loss': 1.0042, 'learning_rate': 0.00015333333333333334, 'epoch': 0.57}
{'loss': 0.9202, 'learning_rate': 0.00015666666666666666, 'epoch': 0.59}
{'loss': 0.8348, 'learning_rate': 0.00016, 'epoch': 0.6}
{'loss': 0.8829, 'learning_rate': 0.00016333333333333334, 'epoch': 0.61}
{'loss': 0.9396, 'learning_rate': 0.0001666666666666667, 'epoch': 0.62}
{'loss': 0.9502, 'learning_rate': 0.00017, 'epoch': 0.64}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:17:56,364] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9293, 'learning_rate': 0.00017333333333333334, 'epoch': 0.65}
[2025-05-03 10:18:13,309] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9502, 'learning_rate': 0.00017666666666666666, 'epoch': 0.66}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:18:30,253] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9138, 'learning_rate': 0.00018, 'epoch': 0.68}
[2025-05-03 10:18:47,327] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9126, 'learning_rate': 0.00018333333333333334, 'epoch': 0.69}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:19:04,258] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8549, 'learning_rate': 0.0001866666666666667, 'epoch': 0.7}
[2025-05-03 10:19:21,155] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9053, 'learning_rate': 0.00019, 'epoch': 0.71}
{'loss': 0.9337, 'learning_rate': 0.00019333333333333333, 'epoch': 0.72}
{'loss': 0.9041, 'learning_rate': 0.00019666666666666666, 'epoch': 0.74}
{'loss': 0.9534, 'learning_rate': 0.0002, 'epoch': 0.75}
[2025-05-03 10:20:28,563] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9922, 'learning_rate': 0.00019999986888082893, 'epoch': 0.76}
[2025-05-03 10:20:45,681] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8963, 'learning_rate': 0.00019999947552365961, 'epoch': 0.78}
[2025-05-03 10:21:02,507] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8942, 'learning_rate': 0.0001999988199295235, 'epoch': 0.79}
[2025-05-03 10:21:19,550] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.925, 'learning_rate': 0.00019999790210013988, 'epoch': 0.8}
[2025-05-03 10:21:36,754] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9341, 'learning_rate': 0.00019999672203791565, 'epoch': 0.81}
[2025-05-03 10:21:53,746] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8917, 'learning_rate': 0.0001999952797459453, 'epoch': 0.82}
[2025-05-03 10:22:10,745] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9111, 'learning_rate': 0.00019999357522801123, 'epoch': 0.84}
{'loss': 0.8954, 'learning_rate': 0.0001999916084885832, 'epoch': 0.85}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:22:44,515] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9228, 'learning_rate': 0.0001999893795328188, 'epoch': 0.86}
{'loss': 0.8556, 'learning_rate': 0.00019998688836656323, 'epoch': 0.88}
{'loss': 0.9399, 'learning_rate': 0.00019998413499634925, 'epoch': 0.89}
{'loss': 0.8732, 'learning_rate': 0.0001999811194293973, 'epoch': 0.9}
{'loss': 0.8734, 'learning_rate': 0.00019997784167361527, 'epoch': 0.91}
[2025-05-03 10:24:08,687] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8787, 'learning_rate': 0.00019997430173759875, 'epoch': 0.93}
[2025-05-03 10:24:25,519] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9098, 'learning_rate': 0.0001999704996306308, 'epoch': 0.94}
[2025-05-03 10:24:42,369] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9427, 'learning_rate': 0.00019996643536268204, 'epoch': 0.95}
{'loss': 0.9318, 'learning_rate': 0.00019996210894441047, 'epoch': 0.96}
[2025-05-03 10:25:16,059] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8698, 'learning_rate': 0.00019995752038716168, 'epoch': 0.97}
[2025-05-03 10:25:32,975] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9001, 'learning_rate': 0.00019995266970296855, 'epoch': 0.99}
[2025-05-03 10:25:50,078] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8488, 'learning_rate': 0.00019994755690455152, 'epoch': 1.0}
[2025-05-03 10:26:09,338] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7978, 'learning_rate': 0.00019994218200531822, 'epoch': 1.01}
[2025-05-03 10:26:26,740] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8128, 'learning_rate': 0.0001999365450193638, 'epoch': 1.02}
[2025-05-03 10:26:43,637] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9064, 'learning_rate': 0.0001999306459614705, 'epoch': 1.04}
[2025-05-03 10:27:00,581] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7835, 'learning_rate': 0.00019992448484710797, 'epoch': 1.05}
[2025-05-03 10:27:17,521] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8217, 'learning_rate': 0.000199918061692433, 'epoch': 1.06}
[2025-05-03 10:27:34,351] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8149, 'learning_rate': 0.00019991137651428957, 'epoch': 1.07}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:27:51,347] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7535, 'learning_rate': 0.00019990442933020877, 'epoch': 1.09}
[2025-05-03 10:28:08,228] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7804, 'learning_rate': 0.0001998972201584088, 'epoch': 1.1}
[2025-05-03 10:28:25,245] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8328, 'learning_rate': 0.00019988974901779483, 'epoch': 1.11}
[2025-05-03 10:28:42,253] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8154, 'learning_rate': 0.0001998820159279591, 'epoch': 1.12}
[2025-05-03 10:28:59,269] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7787, 'learning_rate': 0.00019987402090918067, 'epoch': 1.14}
[2025-05-03 10:29:16,087] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9175, 'learning_rate': 0.00019986576398242566, 'epoch': 1.15}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:29:33,203] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7479, 'learning_rate': 0.00019985724516934677, 'epoch': 1.16}
[2025-05-03 10:29:50,230] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.791, 'learning_rate': 0.0001998484644922837, 'epoch': 1.18}
[2025-05-03 10:30:07,245] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8047, 'learning_rate': 0.0001998394219742627, 'epoch': 1.19}
[2025-05-03 10:30:24,178] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8081, 'learning_rate': 0.00019983011763899673, 'epoch': 1.2}
[2025-05-03 10:30:41,138] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8207, 'learning_rate': 0.0001998205515108853, 'epoch': 1.21}
[2025-05-03 10:30:58,010] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8191, 'learning_rate': 0.0001998107236150145, 'epoch': 1.23}
[2025-05-03 10:31:14,912] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.797, 'learning_rate': 0.00019980063397715683, 'epoch': 1.24}
{'loss': 0.7771, 'learning_rate': 0.00019979028262377118, 'epoch': 1.25}
[2025-05-03 10:31:48,638] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8307, 'learning_rate': 0.00019977966958200277, 'epoch': 1.26}
{'loss': 0.7903, 'learning_rate': 0.0001997687948796831, 'epoch': 1.27}
{'loss': 0.7639, 'learning_rate': 0.00019975765854532974, 'epoch': 1.29}
{'loss': 0.7926, 'learning_rate': 0.00019974626060814647, 'epoch': 1.3}
{'loss': 0.8045, 'learning_rate': 0.00019973460109802305, 'epoch': 1.31}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.8137, 'learning_rate': 0.0001997226800455352, 'epoch': 1.32}
{'loss': 0.8014, 'learning_rate': 0.00019971049748194447, 'epoch': 1.34}
{'loss': 0.7404, 'learning_rate': 0.00019969805343919821, 'epoch': 1.35}
{'loss': 0.8467, 'learning_rate': 0.00019968534794992949, 'epoch': 1.36}
{'loss': 0.7658, 'learning_rate': 0.00019967238104745696, 'epoch': 1.38}
{'loss': 0.7881, 'learning_rate': 0.00019965915276578478, 'epoch': 1.39}
{'loss': 0.8006, 'learning_rate': 0.00019964566313960264, 'epoch': 1.4}
{'loss': 0.8045, 'learning_rate': 0.0001996319122042855, 'epoch': 1.41}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.8129, 'learning_rate': 0.00019961789999589356, 'epoch': 1.43}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.7965, 'learning_rate': 0.00019960362655117218, 'epoch': 1.44}
[2025-05-03 10:36:01,246] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7822, 'learning_rate': 0.00019958909190755187, 'epoch': 1.45}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:36:18,198] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7739, 'learning_rate': 0.00019957429610314797, 'epoch': 1.46}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:36:35,155] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8721, 'learning_rate': 0.0001995592391767608, 'epoch': 1.48}
[2025-05-03 10:36:52,006] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8129, 'learning_rate': 0.0001995439211678754, 'epoch': 1.49}
[2025-05-03 10:37:09,692] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8275, 'learning_rate': 0.0001995283421166614, 'epoch': 1.5}
[2025-05-03 10:37:26,631] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7698, 'learning_rate': 0.00019951250206397313, 'epoch': 1.51}
[2025-05-03 10:37:43,560] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7944, 'learning_rate': 0.00019949640105134918, 'epoch': 1.52}
[2025-05-03 10:38:00,588] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8391, 'learning_rate': 0.00019948003912101273, 'epoch': 1.54}
[2025-05-03 10:38:17,363] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8173, 'learning_rate': 0.00019946341631587087, 'epoch': 1.55}
[2025-05-03 10:38:34,301] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8109, 'learning_rate': 0.00019944653267951504, 'epoch': 1.56}
[2025-05-03 10:38:51,263] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8045, 'learning_rate': 0.00019942938825622065, 'epoch': 1.57}
[2025-05-03 10:39:08,156] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8371, 'learning_rate': 0.0001994119830909469, 'epoch': 1.59}
{'loss': 0.7595, 'learning_rate': 0.0001993943172293368, 'epoch': 1.6}
[2025-05-03 10:39:41,942] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7428, 'learning_rate': 0.00019937639071771702, 'epoch': 1.61}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:39:58,721] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8206, 'learning_rate': 0.00019935820360309777, 'epoch': 1.62}
[2025-05-03 10:40:15,587] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8726, 'learning_rate': 0.00019933975593317262, 'epoch': 1.64}
[2025-05-03 10:40:32,610] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8559, 'learning_rate': 0.00019932104775631846, 'epoch': 1.65}
[2025-05-03 10:40:49,448] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8354, 'learning_rate': 0.00019930207912159529, 'epoch': 1.66}
[2025-05-03 10:41:06,531] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8163, 'learning_rate': 0.0001992828500787461, 'epoch': 1.68}
[2025-05-03 10:41:23,546] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8961, 'learning_rate': 0.00019926336067819684, 'epoch': 1.69}
[2025-05-03 10:41:40,426] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8288, 'learning_rate': 0.00019924361097105623, 'epoch': 1.7}
[2025-05-03 10:41:57,470] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8717, 'learning_rate': 0.00019922360100911552, 'epoch': 1.71}
[2025-05-03 10:42:14,361] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8365, 'learning_rate': 0.00019920333084484857, 'epoch': 1.73}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:42:31,430] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8093, 'learning_rate': 0.00019918280053141143, 'epoch': 1.74}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:42:48,360] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.899, 'learning_rate': 0.00019916201012264254, 'epoch': 1.75}
[2025-05-03 10:43:05,355] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7952, 'learning_rate': 0.00019914095967306223, 'epoch': 1.76}
[2025-05-03 10:43:22,221] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7435, 'learning_rate': 0.00019911964923787295, 'epoch': 1.77}
[2025-05-03 10:43:39,195] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7919, 'learning_rate': 0.0001990980788729588, 'epoch': 1.79}
[2025-05-03 10:43:56,090] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.826, 'learning_rate': 0.0001990762486348855, 'epoch': 1.8}
[2025-05-03 10:44:13,033] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8202, 'learning_rate': 0.00019905415858090036, 'epoch': 1.81}
[2025-05-03 10:44:30,108] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8353, 'learning_rate': 0.00019903180876893194, 'epoch': 1.82}
[2025-05-03 10:44:47,056] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8695, 'learning_rate': 0.00019900919925759002, 'epoch': 1.84}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:45:04,063] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8615, 'learning_rate': 0.00019898633010616542, 'epoch': 1.85}
[2025-05-03 10:45:21,079] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8659, 'learning_rate': 0.00019896320137462983, 'epoch': 1.86}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.8118, 'learning_rate': 0.00019893981312363562, 'epoch': 1.88}
[2025-05-03 10:45:54,922] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8658, 'learning_rate': 0.0001989161654145158, 'epoch': 1.89}
{'loss': 0.8056, 'learning_rate': 0.00019889225830928365, 'epoch': 1.9}
{'loss': 0.7898, 'learning_rate': 0.00019886809187063284, 'epoch': 1.91}
{'loss': 0.825, 'learning_rate': 0.00019884366616193706, 'epoch': 1.93}
{'loss': 0.8126, 'learning_rate': 0.00019881898124724981, 'epoch': 1.94}
{'loss': 0.7874, 'learning_rate': 0.0001987940371913044, 'epoch': 1.95}
{'loss': 0.8971, 'learning_rate': 0.00019876883405951377, 'epoch': 1.96}
{'loss': 0.766, 'learning_rate': 0.0001987433719179702, 'epoch': 1.98}
{'loss': 0.803, 'learning_rate': 0.00019871765083344508, 'epoch': 1.99}
{'loss': 0.7949, 'learning_rate': 0.00019869167087338907, 'epoch': 2.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.6653, 'learning_rate': 0.00019866543210593154, 'epoch': 2.01}
[2025-05-03 10:49:02,825] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7327, 'learning_rate': 0.00019863893459988062, 'epoch': 2.02}
[2025-05-03 10:49:19,986] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7181, 'learning_rate': 0.0001986121784247229, 'epoch': 2.04}
[2025-05-03 10:49:37,722] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6596, 'learning_rate': 0.00019858516365062334, 'epoch': 2.05}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.6917, 'learning_rate': 0.00019855789034842504, 'epoch': 2.06}
{'loss': 0.6864, 'learning_rate': 0.00019853035858964906, 'epoch': 2.08}
[2025-05-03 10:50:28,563] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7021, 'learning_rate': 0.00019850256844649423, 'epoch': 2.09}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.6452, 'learning_rate': 0.00019847451999183694, 'epoch': 2.1}
{'loss': 0.6436, 'learning_rate': 0.000198446213299231, 'epoch': 2.11}
[2025-05-03 10:51:19,139] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6574, 'learning_rate': 0.00019841764844290744, 'epoch': 2.12}
[2025-05-03 10:51:36,224] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6857, 'learning_rate': 0.00019838882549777425, 'epoch': 2.14}
[2025-05-03 10:51:53,030] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6757, 'learning_rate': 0.0001983597445394162, 'epoch': 2.15}
[2025-05-03 10:52:09,916] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6548, 'learning_rate': 0.00019833040564409476, 'epoch': 2.16}
{'loss': 0.7145, 'learning_rate': 0.00019830080888874778, 'epoch': 2.17}
{'loss': 0.6785, 'learning_rate': 0.00019827095435098925, 'epoch': 2.19}
{'loss': 0.6651, 'learning_rate': 0.00019824084210910925, 'epoch': 2.2}
[2025-05-03 10:53:17,456] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6636, 'learning_rate': 0.0001982104722420736, 'epoch': 2.21}
[2025-05-03 10:53:34,363] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6423, 'learning_rate': 0.00019817984482952376, 'epoch': 2.23}
[2025-05-03 10:53:51,283] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6918, 'learning_rate': 0.00019814895995177653, 'epoch': 2.24}
[2025-05-03 10:54:08,337] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7383, 'learning_rate': 0.0001981178176898239, 'epoch': 2.25}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:54:25,323] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6796, 'learning_rate': 0.00019808641812533285, 'epoch': 2.26}
[2025-05-03 10:54:42,179] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6863, 'learning_rate': 0.00019805476134064507, 'epoch': 2.27}
[2025-05-03 10:54:59,386] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7102, 'learning_rate': 0.00019802284741877673, 'epoch': 2.29}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:55:16,290] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6411, 'learning_rate': 0.00019799067644341844, 'epoch': 2.3}
[2025-05-03 10:55:33,282] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6654, 'learning_rate': 0.0001979582484989348, 'epoch': 2.31}
[2025-05-03 10:55:50,151] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7013, 'learning_rate': 0.00019792556367036432, 'epoch': 2.33}
[2025-05-03 10:56:06,997] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6876, 'learning_rate': 0.00019789262204341916, 'epoch': 2.34}
[2025-05-03 10:56:23,841] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6892, 'learning_rate': 0.0001978594237044849, 'epoch': 2.35}
[2025-05-03 10:56:40,673] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6682, 'learning_rate': 0.00019782596874062027, 'epoch': 2.36}
[2025-05-03 10:56:57,555] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6664, 'learning_rate': 0.00019779225723955707, 'epoch': 2.38}
[2025-05-03 10:57:14,458] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6535, 'learning_rate': 0.00019775828928969975, 'epoch': 2.39}
[2025-05-03 10:57:31,277] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6945, 'learning_rate': 0.0001977240649801253, 'epoch': 2.4}
[2025-05-03 10:57:48,975] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6791, 'learning_rate': 0.00019768958440058302, 'epoch': 2.41}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.6838, 'learning_rate': 0.00019765484764149415, 'epoch': 2.42}
[2025-05-03 10:58:22,704] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6783, 'learning_rate': 0.0001976198547939518, 'epoch': 2.44}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:58:39,634] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7205, 'learning_rate': 0.00019758460594972068, 'epoch': 2.45}
[2025-05-03 10:58:56,596] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6782, 'learning_rate': 0.00019754910120123675, 'epoch': 2.46}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 10:59:13,472] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.683, 'learning_rate': 0.00019751334064160706, 'epoch': 2.48}
{'loss': 0.699, 'learning_rate': 0.00019747732436460952, 'epoch': 2.49}
{'loss': 0.703, 'learning_rate': 0.00019744105246469263, 'epoch': 2.5}
{'loss': 0.7023, 'learning_rate': 0.00019740452503697517, 'epoch': 2.51}
{'loss': 0.7399, 'learning_rate': 0.00019736774217724614, 'epoch': 2.52}
{'loss': 0.7352, 'learning_rate': 0.00019733070398196423, 'epoch': 2.54}
{'loss': 0.6469, 'learning_rate': 0.00019729341054825782, 'epoch': 2.55}
{'loss': 0.7318, 'learning_rate': 0.0001972558619739246, 'epoch': 2.56}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.7242, 'learning_rate': 0.00019721805835743134, 'epoch': 2.58}
{'loss': 0.6781, 'learning_rate': 0.00019717999979791356, 'epoch': 2.59}
{'loss': 0.7239, 'learning_rate': 0.00019714168639517544, 'epoch': 2.6}
{'loss': 0.6946, 'learning_rate': 0.0001971031182496894, 'epoch': 2.61}
{'loss': 0.6731, 'learning_rate': 0.00019706429546259593, 'epoch': 2.62}
{'loss': 0.6924, 'learning_rate': 0.00019702521813570322, 'epoch': 2.64}
{'loss': 0.7191, 'learning_rate': 0.00019698588637148703, 'epoch': 2.65}
{'loss': 0.732, 'learning_rate': 0.00019694630027309034, 'epoch': 2.66}
{'loss': 0.7152, 'learning_rate': 0.00019690645994432305, 'epoch': 2.67}
{'loss': 0.7182, 'learning_rate': 0.00019686636548966178, 'epoch': 2.69}
{'loss': 0.7016, 'learning_rate': 0.0001968260170142496, 'epoch': 2.7}
{'loss': 0.7006, 'learning_rate': 0.00019678541462389562, 'epoch': 2.71}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.7358, 'learning_rate': 0.00019674455842507492, 'epoch': 2.73}
{'loss': 0.6835, 'learning_rate': 0.0001967034485249281, 'epoch': 2.74}
{'loss': 0.7316, 'learning_rate': 0.00019666208503126112, 'epoch': 2.75}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.7098, 'learning_rate': 0.00019662046805254488, 'epoch': 2.76}
{'loss': 0.7152, 'learning_rate': 0.00019657859769791505, 'epoch': 2.77}
{'loss': 0.7401, 'learning_rate': 0.00019653647407717178, 'epoch': 2.79}
{'loss': 0.7244, 'learning_rate': 0.00019649409730077935, 'epoch': 2.8}
{'loss': 0.6946, 'learning_rate': 0.0001964514674798659, 'epoch': 2.81}
{'loss': 0.6881, 'learning_rate': 0.00019640858472622316, 'epoch': 2.83}
{'loss': 0.6811, 'learning_rate': 0.0001963654491523062, 'epoch': 2.84}
{'loss': 0.6845, 'learning_rate': 0.00019632206087123296, 'epoch': 2.85}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.7198, 'learning_rate': 0.0001962784199967842, 'epoch': 2.86}
{'loss': 0.739, 'learning_rate': 0.00019623452664340306, 'epoch': 2.88}
{'loss': 0.7106, 'learning_rate': 0.00019619038092619464, 'epoch': 2.89}
{'loss': 0.6865, 'learning_rate': 0.000196145982960926, 'epoch': 2.9}
{'loss': 0.7318, 'learning_rate': 0.00019610133286402563, 'epoch': 2.91}
{'loss': 0.7273, 'learning_rate': 0.00019605643075258321, 'epoch': 2.92}
[2025-05-03 11:09:35,788] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7022, 'learning_rate': 0.00019601127674434928, 'epoch': 2.94}
[2025-05-03 11:09:52,661] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6875, 'learning_rate': 0.00019596587095773495, 'epoch': 2.95}
[2025-05-03 11:10:09,559] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.701, 'learning_rate': 0.00019592021351181162, 'epoch': 2.96}
[2025-05-03 11:10:26,333] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6992, 'learning_rate': 0.0001958743045263106, 'epoch': 2.98}
{'loss': 0.7009, 'learning_rate': 0.0001958281441216229, 'epoch': 2.99}
[2025-05-03 11:11:00,050] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7279, 'learning_rate': 0.00019578173241879872, 'epoch': 3.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.5278, 'learning_rate': 0.0001957350695395474, 'epoch': 3.01}
[2025-05-03 11:11:36,627] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5485, 'learning_rate': 0.0001956881556062369, 'epoch': 3.02}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:11:53,517] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5362, 'learning_rate': 0.0001956409907418935, 'epoch': 3.04}
[2025-05-03 11:12:10,436] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5156, 'learning_rate': 0.00019559357507020162, 'epoch': 3.05}
[2025-05-03 11:12:27,323] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.512, 'learning_rate': 0.0001955459087155033, 'epoch': 3.06}
[2025-05-03 11:12:44,375] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5387, 'learning_rate': 0.00019549799180279792, 'epoch': 3.08}
[2025-05-03 11:13:01,370] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5258, 'learning_rate': 0.00019544982445774217, 'epoch': 3.09}
[2025-05-03 11:13:18,281] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5317, 'learning_rate': 0.00019540140680664913, 'epoch': 3.1}
{'loss': 0.5012, 'learning_rate': 0.00019535273897648857, 'epoch': 3.11}
[2025-05-03 11:13:51,929] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5012, 'learning_rate': 0.0001953038210948861, 'epoch': 3.12}
[2025-05-03 11:14:09,048] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4913, 'learning_rate': 0.00019525465329012324, 'epoch': 3.14}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:14:25,797] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5726, 'learning_rate': 0.00019520523569113677, 'epoch': 3.15}
[2025-05-03 11:14:42,669] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5745, 'learning_rate': 0.00019515556842751862, 'epoch': 3.16}
[2025-05-03 11:14:59,503] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5422, 'learning_rate': 0.00019510565162951537, 'epoch': 3.17}
[2025-05-03 11:15:16,291] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5459, 'learning_rate': 0.00019505548542802804, 'epoch': 3.19}
[2025-05-03 11:15:33,098] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5443, 'learning_rate': 0.0001950050699546116, 'epoch': 3.2}
{'loss': 0.5378, 'learning_rate': 0.00019495440534147477, 'epoch': 3.21}
{'loss': 0.5281, 'learning_rate': 0.00019490349172147963, 'epoch': 3.23}
[2025-05-03 11:16:23,470] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5467, 'learning_rate': 0.00019485232922814117, 'epoch': 3.24}
[2025-05-03 11:16:40,271] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5599, 'learning_rate': 0.00019480091799562704, 'epoch': 3.25}
[2025-05-03 11:16:57,087] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.519, 'learning_rate': 0.00019474925815875729, 'epoch': 3.26}
[2025-05-03 11:17:13,927] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5788, 'learning_rate': 0.00019469734985300371, 'epoch': 3.27}
[2025-05-03 11:17:30,756] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5876, 'learning_rate': 0.0001946451932144899, 'epoch': 3.29}
[2025-05-03 11:17:47,548] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5408, 'learning_rate': 0.00019459278837999046, 'epoch': 3.3}
[2025-05-03 11:18:04,407] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.571, 'learning_rate': 0.00019454013548693102, 'epoch': 3.31}
[2025-05-03 11:18:21,249] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5558, 'learning_rate': 0.00019448723467338763, 'epoch': 3.33}
[2025-05-03 11:18:38,093] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6035, 'learning_rate': 0.0001944340860780865, 'epoch': 3.34}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:18:54,854] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5631, 'learning_rate': 0.00019438068984040365, 'epoch': 3.35}
[2025-05-03 11:19:11,677] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5273, 'learning_rate': 0.00019432704610036446, 'epoch': 3.36}
[2025-05-03 11:19:28,573] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5268, 'learning_rate': 0.00019427315499864344, 'epoch': 3.38}
[2025-05-03 11:19:45,377] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5584, 'learning_rate': 0.00019421901667656365, 'epoch': 3.39}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:20:02,281] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5655, 'learning_rate': 0.00019416463127609656, 'epoch': 3.4}
[2025-05-03 11:20:19,057] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5804, 'learning_rate': 0.00019410999893986156, 'epoch': 3.41}
[2025-05-03 11:20:35,920] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6167, 'learning_rate': 0.0001940551198111255, 'epoch': 3.42}
[2025-05-03 11:20:52,751] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6191, 'learning_rate': 0.00019399999403380266, 'epoch': 3.44}
[2025-05-03 11:21:09,481] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5798, 'learning_rate': 0.00019394462175245381, 'epoch': 3.45}
[2025-05-03 11:21:26,583] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5779, 'learning_rate': 0.00019388900311228638, 'epoch': 3.46}
[2025-05-03 11:21:43,391] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6118, 'learning_rate': 0.0001938331382591537, 'epoch': 3.48}
[2025-05-03 11:22:00,247] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5629, 'learning_rate': 0.00019377702733955495, 'epoch': 3.49}
[2025-05-03 11:22:17,193] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5795, 'learning_rate': 0.00019372067050063438, 'epoch': 3.5}
[2025-05-03 11:22:33,985] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5746, 'learning_rate': 0.00019366406789018126, 'epoch': 3.51}
{'loss': 0.6053, 'learning_rate': 0.00019360721965662933, 'epoch': 3.52}
{'loss': 0.5929, 'learning_rate': 0.00019355012594905646, 'epoch': 3.54}
{'loss': 0.5926, 'learning_rate': 0.00019349278691718427, 'epoch': 3.55}
{'loss': 0.6369, 'learning_rate': 0.00019343520271137763, 'epoch': 3.56}
{'loss': 0.5696, 'learning_rate': 0.00019337737348264447, 'epoch': 3.58}
{'loss': 0.6194, 'learning_rate': 0.00019331929938263515, 'epoch': 3.59}
{'loss': 0.6051, 'learning_rate': 0.00019326098056364222, 'epoch': 3.6}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.5529, 'learning_rate': 0.00019320241717860005, 'epoch': 3.61}
{'loss': 0.594, 'learning_rate': 0.00019314360938108425, 'epoch': 3.62}
{'loss': 0.6256, 'learning_rate': 0.00019308455732531139, 'epoch': 3.64}
{'loss': 0.5509, 'learning_rate': 0.00019302526116613864, 'epoch': 3.65}
{'loss': 0.6258, 'learning_rate': 0.00019296572105906323, 'epoch': 3.66}
{'loss': 0.6384, 'learning_rate': 0.00019290593716022217, 'epoch': 3.67}
{'loss': 0.5911, 'learning_rate': 0.00019284590962639176, 'epoch': 3.69}
[2025-05-03 11:26:47,216] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6033, 'learning_rate': 0.00019278563861498723, 'epoch': 3.7}
{'loss': 0.5708, 'learning_rate': 0.0001927251242840623, 'epoch': 3.71}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:27:21,107] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5915, 'learning_rate': 0.00019266436679230865, 'epoch': 3.73}
[2025-05-03 11:27:37,993] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.58, 'learning_rate': 0.0001926033662990558, 'epoch': 3.74}
[2025-05-03 11:27:54,810] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5753, 'learning_rate': 0.00019254212296427044, 'epoch': 3.75}
[2025-05-03 11:28:11,706] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6152, 'learning_rate': 0.00019248063694855602, 'epoch': 3.76}
[2025-05-03 11:28:28,508] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5874, 'learning_rate': 0.00019241890841315248, 'epoch': 3.77}
[2025-05-03 11:28:45,331] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5833, 'learning_rate': 0.0001923569375199357, 'epoch': 3.79}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.597, 'learning_rate': 0.0001922947244314172, 'epoch': 3.8}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.5963, 'learning_rate': 0.0001922322693107434, 'epoch': 3.81}
[2025-05-03 11:29:35,676] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6223, 'learning_rate': 0.0001921695723216957, 'epoch': 3.83}
{'loss': 0.6022, 'learning_rate': 0.00019210663362868955, 'epoch': 3.84}
{'loss': 0.6515, 'learning_rate': 0.00019204345339677442, 'epoch': 3.85}
{'loss': 0.59, 'learning_rate': 0.00019198003179163306, 'epoch': 3.86}
{'loss': 0.6211, 'learning_rate': 0.00019191636897958122, 'epoch': 3.88}
{'loss': 0.5757, 'learning_rate': 0.00019185246512756727, 'epoch': 3.89}
{'loss': 0.6044, 'learning_rate': 0.00019178832040317155, 'epoch': 3.9}
{'loss': 0.5985, 'learning_rate': 0.0001917239349746061, 'epoch': 3.91}
{'loss': 0.5883, 'learning_rate': 0.0001916593090107143, 'epoch': 3.92}
{'loss': 0.5779, 'learning_rate': 0.00019159444268097012, 'epoch': 3.94}
{'loss': 0.6152, 'learning_rate': 0.00019152933615547798, 'epoch': 3.95}
{'loss': 0.596, 'learning_rate': 0.0001914639896049721, 'epoch': 3.96}
{'loss': 0.6066, 'learning_rate': 0.0001913984032008163, 'epoch': 3.98}
{'loss': 0.5567, 'learning_rate': 0.0001913325771150032, 'epoch': 3.99}
{'loss': 0.6059, 'learning_rate': 0.00019126651152015403, 'epoch': 4.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:33:50,397] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4417, 'learning_rate': 0.00019120020658951813, 'epoch': 4.01}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:34:07,586] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4349, 'learning_rate': 0.0001911336624969725, 'epoch': 4.03}
[2025-05-03 11:34:24,396] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4197, 'learning_rate': 0.00019106687941702118, 'epoch': 4.04}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:34:41,592] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4033, 'learning_rate': 0.00019099985752479506, 'epoch': 4.05}
[2025-05-03 11:34:58,612] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4074, 'learning_rate': 0.00019093259699605125, 'epoch': 4.06}
[2025-05-03 11:35:15,469] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3838, 'learning_rate': 0.00019086509800717258, 'epoch': 4.08}
{'loss': 0.4199, 'learning_rate': 0.00019079736073516736, 'epoch': 4.09}
[2025-05-03 11:35:49,039] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4015, 'learning_rate': 0.00019072938535766865, 'epoch': 4.1}
[2025-05-03 11:36:05,866] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4022, 'learning_rate': 0.00019066117205293392, 'epoch': 4.11}
[2025-05-03 11:36:22,620] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.406, 'learning_rate': 0.0001905927209998447, 'epoch': 4.12}
[2025-05-03 11:36:40,378] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3786, 'learning_rate': 0.00019052403237790582, 'epoch': 4.14}
[2025-05-03 11:36:57,388] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3958, 'learning_rate': 0.0001904551063672452, 'epoch': 4.15}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.4295, 'learning_rate': 0.0001903859431486133, 'epoch': 4.16}
{'loss': 0.3993, 'learning_rate': 0.00019031654290338254, 'epoch': 4.17}
{'loss': 0.4219, 'learning_rate': 0.000190246905813547, 'epoch': 4.19}
{'loss': 0.4531, 'learning_rate': 0.00019017703206172185, 'epoch': 4.2}
{'loss': 0.383, 'learning_rate': 0.00019010692183114286, 'epoch': 4.21}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:38:38,565] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.436, 'learning_rate': 0.0001900365753056659, 'epoch': 4.22}
{'loss': 0.4547, 'learning_rate': 0.00018996599266976656, 'epoch': 4.24}
{'loss': 0.4173, 'learning_rate': 0.00018989517410853955, 'epoch': 4.25}
{'loss': 0.4176, 'learning_rate': 0.0001898241198076983, 'epoch': 4.26}
{'loss': 0.4134, 'learning_rate': 0.00018975282995357446, 'epoch': 4.28}
{'loss': 0.4098, 'learning_rate': 0.00018968130473311732, 'epoch': 4.29}
{'loss': 0.4208, 'learning_rate': 0.00018960954433389345, 'epoch': 4.3}
{'loss': 0.4714, 'learning_rate': 0.00018953754894408616, 'epoch': 4.31}
{'loss': 0.4147, 'learning_rate': 0.00018946531875249493, 'epoch': 4.33}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.4723, 'learning_rate': 0.00018939285394853502, 'epoch': 4.34}
{'loss': 0.4678, 'learning_rate': 0.00018932015472223693, 'epoch': 4.35}
{'loss': 0.4624, 'learning_rate': 0.0001892472212642459, 'epoch': 4.36}
{'loss': 0.4193, 'learning_rate': 0.00018917405376582145, 'epoch': 4.38}
{'loss': 0.4229, 'learning_rate': 0.0001891006524188368, 'epoch': 4.39}
{'loss': 0.4772, 'learning_rate': 0.0001890270174157784, 'epoch': 4.4}
{'loss': 0.4687, 'learning_rate': 0.00018895314894974553, 'epoch': 4.41}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.454, 'learning_rate': 0.00018887904721444953, 'epoch': 4.42}
{'loss': 0.4847, 'learning_rate': 0.00018880471240421365, 'epoch': 4.44}
{'loss': 0.4148, 'learning_rate': 0.00018873014471397224, 'epoch': 4.45}
{'loss': 0.4334, 'learning_rate': 0.00018865534433927034, 'epoch': 4.46}
{'loss': 0.4617, 'learning_rate': 0.00018858031147626325, 'epoch': 4.47}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.4704, 'learning_rate': 0.0001885050463217159, 'epoch': 4.49}
{'loss': 0.4632, 'learning_rate': 0.00018842954907300236, 'epoch': 4.5}
{'loss': 0.4886, 'learning_rate': 0.0001883538199281054, 'epoch': 4.51}
{'loss': 0.472, 'learning_rate': 0.00018827785908561584, 'epoch': 4.53}
{'loss': 0.4645, 'learning_rate': 0.00018820166674473216, 'epoch': 4.54}
{'loss': 0.4669, 'learning_rate': 0.0001881252431052599, 'epoch': 4.55}
[2025-05-03 11:46:15,016] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4388, 'learning_rate': 0.00018804858836761107, 'epoch': 4.56}
{'loss': 0.4564, 'learning_rate': 0.00018797170273280388, 'epoch': 4.58}
[2025-05-03 11:46:48,820] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4657, 'learning_rate': 0.0001878945864024619, 'epoch': 4.59}
[2025-05-03 11:47:05,692] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4952, 'learning_rate': 0.00018781723957881372, 'epoch': 4.6}
[2025-05-03 11:47:22,918] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4351, 'learning_rate': 0.00018773966246469237, 'epoch': 4.61}
[2025-05-03 11:47:39,719] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4549, 'learning_rate': 0.0001876618552635348, 'epoch': 4.62}
[2025-05-03 11:47:56,817] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4339, 'learning_rate': 0.00018758381817938127, 'epoch': 4.64}
{'loss': 0.4701, 'learning_rate': 0.000187505551416875, 'epoch': 4.65}
[2025-05-03 11:48:31,009] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4561, 'learning_rate': 0.0001874270551812614, 'epoch': 4.66}
[2025-05-03 11:48:47,857] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4815, 'learning_rate': 0.00018734832967838775, 'epoch': 4.67}
[2025-05-03 11:49:04,951] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5128, 'learning_rate': 0.00018726937511470246, 'epoch': 4.69}
[2025-05-03 11:49:21,897] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.488, 'learning_rate': 0.00018719019169725472, 'epoch': 4.7}
[2025-05-03 11:49:38,880] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4694, 'learning_rate': 0.00018711077963369375, 'epoch': 4.71}
[2025-05-03 11:49:55,949] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5266, 'learning_rate': 0.00018703113913226847, 'epoch': 4.72}
[2025-05-03 11:50:12,748] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4868, 'learning_rate': 0.00018695127040182675, 'epoch': 4.74}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:50:29,736] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4918, 'learning_rate': 0.00018687117365181512, 'epoch': 4.75}
[2025-05-03 11:50:46,503] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4498, 'learning_rate': 0.0001867908490922779, 'epoch': 4.76}
[2025-05-03 11:51:03,777] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4701, 'learning_rate': 0.0001867102969338569, 'epoch': 4.78}
[2025-05-03 11:51:20,801] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4564, 'learning_rate': 0.00018662951738779076, 'epoch': 4.79}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:51:37,809] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4917, 'learning_rate': 0.00018654851066591448, 'epoch': 4.8}
{'loss': 0.5224, 'learning_rate': 0.00018646727698065865, 'epoch': 4.81}
[2025-05-03 11:52:11,385] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4676, 'learning_rate': 0.0001863858165450492, 'epoch': 4.83}
[2025-05-03 11:52:28,239] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5065, 'learning_rate': 0.0001863041295727066, 'epoch': 4.84}
[2025-05-03 11:52:45,224] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4861, 'learning_rate': 0.0001862222162778454, 'epoch': 4.85}
[2025-05-03 11:53:02,056] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.461, 'learning_rate': 0.00018614007687527373, 'epoch': 4.86}
[2025-05-03 11:53:18,880] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5235, 'learning_rate': 0.00018605771158039253, 'epoch': 4.88}
[2025-05-03 11:53:35,846] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4389, 'learning_rate': 0.00018597512060919522, 'epoch': 4.89}
[2025-05-03 11:53:52,729] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4523, 'learning_rate': 0.00018589230417826697, 'epoch': 4.9}
[2025-05-03 11:54:09,489] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4738, 'learning_rate': 0.00018580926250478426, 'epoch': 4.91}
[2025-05-03 11:54:26,341] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4376, 'learning_rate': 0.00018572599580651415, 'epoch': 4.92}
[2025-05-03 11:54:43,148] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4776, 'learning_rate': 0.00018564250430181387, 'epoch': 4.94}
[2025-05-03 11:55:00,144] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5412, 'learning_rate': 0.00018555878820963013, 'epoch': 4.95}
{'loss': 0.461, 'learning_rate': 0.00018547484774949867, 'epoch': 4.96}
[2025-05-03 11:55:33,746] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5056, 'learning_rate': 0.00018539068314154354, 'epoch': 4.97}
[2025-05-03 11:55:50,667] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.489, 'learning_rate': 0.00018530629460647657, 'epoch': 4.99}
[2025-05-03 11:56:07,578] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5085, 'learning_rate': 0.00018522168236559695, 'epoch': 5.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 11:56:27,194] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3275, 'learning_rate': 0.00018513684664079035, 'epoch': 5.01}
{'loss': 0.3196, 'learning_rate': 0.00018505178765452853, 'epoch': 5.03}
[2025-05-03 11:57:01,960] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3117, 'learning_rate': 0.00018496650562986887, 'epoch': 5.04}
[2025-05-03 11:57:18,913] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.32, 'learning_rate': 0.00018488100079045344, 'epoch': 5.05}
[2025-05-03 11:57:35,886] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.302, 'learning_rate': 0.00018479527336050878, 'epoch': 5.06}
[2025-05-03 11:57:52,716] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3231, 'learning_rate': 0.00018470932356484508, 'epoch': 5.08}
[2025-05-03 11:58:09,619] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3092, 'learning_rate': 0.00018462315162885563, 'epoch': 5.09}
{'loss': 0.3135, 'learning_rate': 0.00018453675777851627, 'epoch': 5.1}
[2025-05-03 11:58:43,441] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3038, 'learning_rate': 0.00018445014224038485, 'epoch': 5.11}
{'loss': 0.3082, 'learning_rate': 0.00018436330524160047, 'epoch': 5.12}
{'loss': 0.2849, 'learning_rate': 0.00018427624700988307, 'epoch': 5.14}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.3012, 'learning_rate': 0.0001841889677735327, 'epoch': 5.15}
{'loss': 0.2934, 'learning_rate': 0.000184101467761429, 'epoch': 5.16}
{'loss': 0.2982, 'learning_rate': 0.00018401374720303056, 'epoch': 5.17}
{'loss': 0.2945, 'learning_rate': 0.00018392580632837423, 'epoch': 5.19}
{'loss': 0.2915, 'learning_rate': 0.00018383764536807485, 'epoch': 5.2}
{'loss': 0.3064, 'learning_rate': 0.0001837492645533241, 'epoch': 5.21}
{'loss': 0.3132, 'learning_rate': 0.0001836606641158905, 'epoch': 5.22}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.3158, 'learning_rate': 0.00018357184428811828, 'epoch': 5.24}
[2025-05-03 12:01:49,210] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3089, 'learning_rate': 0.00018348280530292713, 'epoch': 5.25}
[2025-05-03 12:02:06,299] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3123, 'learning_rate': 0.0001833935473938114, 'epoch': 5.26}
{'loss': 0.321, 'learning_rate': 0.00018330407079483952, 'epoch': 5.28}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:02:40,890] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3543, 'learning_rate': 0.00018321437574065347, 'epoch': 5.29}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:02:57,729] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3128, 'learning_rate': 0.0001831244624664681, 'epoch': 5.3}
[2025-05-03 12:03:14,590] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3382, 'learning_rate': 0.0001830343312080704, 'epoch': 5.31}
[2025-05-03 12:03:31,472] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3398, 'learning_rate': 0.00018294398220181917, 'epoch': 5.33}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:03:48,446] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3214, 'learning_rate': 0.00018285341568464414, 'epoch': 5.34}
[2025-05-03 12:04:05,336] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.329, 'learning_rate': 0.0001827626318940454, 'epoch': 5.35}
[2025-05-03 12:04:22,187] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3437, 'learning_rate': 0.00018267163106809288, 'epoch': 5.36}
[2025-05-03 12:04:39,111] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3238, 'learning_rate': 0.00018258041344542566, 'epoch': 5.38}
[2025-05-03 12:04:56,071] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3197, 'learning_rate': 0.0001824889792652513, 'epoch': 5.39}
{'loss': 0.3358, 'learning_rate': 0.00018239732876734527, 'epoch': 5.4}
{'loss': 0.3203, 'learning_rate': 0.00018230546219205032, 'epoch': 5.41}
{'loss': 0.3068, 'learning_rate': 0.00018221337978027583, 'epoch': 5.42}
[2025-05-03 12:06:03,583] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3178, 'learning_rate': 0.0001821210817734972, 'epoch': 5.44}
[2025-05-03 12:06:21,288] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3133, 'learning_rate': 0.00018202856841375518, 'epoch': 5.45}
[2025-05-03 12:06:38,154] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2997, 'learning_rate': 0.0001819358399436553, 'epoch': 5.46}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:06:55,098] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3245, 'learning_rate': 0.00018184289660636715, 'epoch': 5.47}
[2025-05-03 12:07:12,094] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3426, 'learning_rate': 0.0001817497386456238, 'epoch': 5.49}
{'loss': 0.3375, 'learning_rate': 0.0001816563663057211, 'epoch': 5.5}
[2025-05-03 12:07:45,835] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3327, 'learning_rate': 0.0001815627798315172, 'epoch': 5.51}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:08:03,079] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3324, 'learning_rate': 0.00018146897946843163, 'epoch': 5.53}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:08:20,007] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3404, 'learning_rate': 0.00018137496546244498, 'epoch': 5.54}
[2025-05-03 12:08:36,836] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3476, 'learning_rate': 0.000181280738060098, 'epoch': 5.55}
{'loss': 0.3538, 'learning_rate': 0.00018118629750849105, 'epoch': 5.56}
{'loss': 0.3494, 'learning_rate': 0.0001810916440552835, 'epoch': 5.58}
[2025-05-03 12:09:27,364] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3021, 'learning_rate': 0.00018099677794869296, 'epoch': 5.59}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:09:44,231] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3229, 'learning_rate': 0.00018090169943749476, 'epoch': 5.6}
[2025-05-03 12:10:02,022] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3597, 'learning_rate': 0.0001808064087710212, 'epoch': 5.61}
{'loss': 0.3829, 'learning_rate': 0.00018071090619916093, 'epoch': 5.62}
[2025-05-03 12:10:36,629] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3399, 'learning_rate': 0.00018061519197235836, 'epoch': 5.64}
[2025-05-03 12:10:53,484] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3507, 'learning_rate': 0.00018051926634161282, 'epoch': 5.65}
[2025-05-03 12:11:10,431] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.337, 'learning_rate': 0.00018042312955847818, 'epoch': 5.66}
[2025-05-03 12:11:27,266] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3554, 'learning_rate': 0.00018032678187506187, 'epoch': 5.67}
[2025-05-03 12:11:44,163] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3437, 'learning_rate': 0.0001802302235440245, 'epoch': 5.69}
[2025-05-03 12:12:01,016] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3502, 'learning_rate': 0.00018013345481857903, 'epoch': 5.7}
[2025-05-03 12:12:17,871] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3446, 'learning_rate': 0.00018003647595249013, 'epoch': 5.71}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:12:34,795] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3576, 'learning_rate': 0.0001799392872000736, 'epoch': 5.72}
[2025-05-03 12:12:51,658] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3517, 'learning_rate': 0.00017984188881619564, 'epoch': 5.74}
{'loss': 0.3497, 'learning_rate': 0.00017974428105627208, 'epoch': 5.75}
[2025-05-03 12:13:25,501] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3578, 'learning_rate': 0.00017964646417626797, 'epoch': 5.76}
[2025-05-03 12:13:42,432] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3494, 'learning_rate': 0.00017954843843269664, 'epoch': 5.78}
[2025-05-03 12:13:59,328] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3663, 'learning_rate': 0.00017945020408261916, 'epoch': 5.79}
[2025-05-03 12:14:16,334] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3681, 'learning_rate': 0.0001793517613836437, 'epoch': 5.8}
[2025-05-03 12:14:33,183] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3504, 'learning_rate': 0.0001792531105939247, 'epoch': 5.81}
[2025-05-03 12:14:50,106] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3216, 'learning_rate': 0.00017915425197216245, 'epoch': 5.83}
[2025-05-03 12:15:06,910] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3676, 'learning_rate': 0.00017905518577760208, 'epoch': 5.84}
[2025-05-03 12:15:24,031] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3576, 'learning_rate': 0.00017895591227003315, 'epoch': 5.85}
{'loss': 0.3527, 'learning_rate': 0.0001788564317097889, 'epoch': 5.86}
[2025-05-03 12:15:58,573] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3706, 'learning_rate': 0.00017875674435774547, 'epoch': 5.88}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:16:15,420] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3393, 'learning_rate': 0.0001786568504753213, 'epoch': 5.89}
[2025-05-03 12:16:32,265] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3489, 'learning_rate': 0.00017855675032447648, 'epoch': 5.9}
[2025-05-03 12:16:49,126] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3645, 'learning_rate': 0.00017845644416771198, 'epoch': 5.91}
[2025-05-03 12:17:06,013] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3621, 'learning_rate': 0.00017835593226806903, 'epoch': 5.92}
[2025-05-03 12:17:22,805] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3924, 'learning_rate': 0.0001782552148891283, 'epoch': 5.94}
[2025-05-03 12:17:39,642] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3825, 'learning_rate': 0.00017815429229500946, 'epoch': 5.95}
[2025-05-03 12:17:56,442] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3533, 'learning_rate': 0.00017805316475037018, 'epoch': 5.96}
[2025-05-03 12:18:13,288] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3795, 'learning_rate': 0.00017795183252040567, 'epoch': 5.97}
[2025-05-03 12:18:30,164] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3724, 'learning_rate': 0.00017785029587084794, 'epoch': 5.99}
[2025-05-03 12:18:47,154] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3726, 'learning_rate': 0.00017774855506796496, 'epoch': 6.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:19:06,198] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2527, 'learning_rate': 0.0001776466103785601, 'epoch': 6.01}
{'loss': 0.244, 'learning_rate': 0.0001775444620699715, 'epoch': 6.03}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.2283, 'learning_rate': 0.00017744211041007118, 'epoch': 6.04}
{'loss': 0.2075, 'learning_rate': 0.0001773395556672644, 'epoch': 6.05}
{'loss': 0.2162, 'learning_rate': 0.00017723679811048904, 'epoch': 6.06}
{'loss': 0.2315, 'learning_rate': 0.00017713383800921478, 'epoch': 6.08}
[2025-05-03 12:20:47,311] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.217, 'learning_rate': 0.0001770306756334425, 'epoch': 6.09}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.2035, 'learning_rate': 0.00017692731125370354, 'epoch': 6.1}
{'loss': 0.1711, 'learning_rate': 0.00017682374514105888, 'epoch': 6.11}
{'loss': 0.192, 'learning_rate': 0.00017671997756709863, 'epoch': 6.12}
[2025-05-03 12:21:54,565] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.215, 'learning_rate': 0.0001766160088039411, 'epoch': 6.14}
[2025-05-03 12:22:11,549] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1919, 'learning_rate': 0.00017651183912423228, 'epoch': 6.15}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:22:28,568] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2252, 'learning_rate': 0.00017640746880114505, 'epoch': 6.16}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:22:45,483] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2362, 'learning_rate': 0.00017630289810837834, 'epoch': 6.17}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:23:02,379] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2214, 'learning_rate': 0.00017619812732015664, 'epoch': 6.19}
{'loss': 0.2204, 'learning_rate': 0.0001760931567112291, 'epoch': 6.2}
[2025-05-03 12:23:36,179] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1869, 'learning_rate': 0.000175987986556869, 'epoch': 6.21}
{'loss': 0.2162, 'learning_rate': 0.00017588261713287267, 'epoch': 6.22}
{'loss': 0.2291, 'learning_rate': 0.0001757770487155592, 'epoch': 6.24}
{'loss': 0.2314, 'learning_rate': 0.00017567128158176953, 'epoch': 6.25}
{'loss': 0.2103, 'learning_rate': 0.00017556531600886554, 'epoch': 6.26}
[2025-05-03 12:25:00,421] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2108, 'learning_rate': 0.00017545915227472965, 'epoch': 6.28}
[2025-05-03 12:25:17,387] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.223, 'learning_rate': 0.0001753527906577638, 'epoch': 6.29}
{'loss': 0.2323, 'learning_rate': 0.00017524623143688902, 'epoch': 6.3}
[2025-05-03 12:25:51,120] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2113, 'learning_rate': 0.00017513947489154443, 'epoch': 6.31}
{'loss': 0.232, 'learning_rate': 0.00017503252130168657, 'epoch': 6.33}
[2025-05-03 12:26:24,977] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2072, 'learning_rate': 0.0001749253709477888, 'epoch': 6.34}
[2025-05-03 12:26:41,916] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2403, 'learning_rate': 0.00017481802411084042, 'epoch': 6.35}
{'loss': 0.2265, 'learning_rate': 0.00017471048107234598, 'epoch': 6.36}
{'loss': 0.2212, 'learning_rate': 0.0001746027421143246, 'epoch': 6.38}
{'loss': 0.2097, 'learning_rate': 0.00017449480751930912, 'epoch': 6.39}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.2268, 'learning_rate': 0.00017438667757034546, 'epoch': 6.4}
{'loss': 0.2344, 'learning_rate': 0.00017427835255099172, 'epoch': 6.41}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.2279, 'learning_rate': 0.00017416983274531775, 'epoch': 6.42}
{'loss': 0.2346, 'learning_rate': 0.000174061118437904, 'epoch': 6.44}
{'loss': 0.2385, 'learning_rate': 0.0001739522099138411, 'epoch': 6.45}
{'loss': 0.2226, 'learning_rate': 0.00017384310745872895, 'epoch': 6.46}
{'loss': 0.2425, 'learning_rate': 0.00017373381135867604, 'epoch': 6.47}
{'loss': 0.2051, 'learning_rate': 0.00017362432190029862, 'epoch': 6.49}
{'loss': 0.2166, 'learning_rate': 0.00017351463937072004, 'epoch': 6.5}
{'loss': 0.2072, 'learning_rate': 0.00017340476405756998, 'epoch': 6.51}
{'loss': 0.252, 'learning_rate': 0.0001732946962489836, 'epoch': 6.53}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.2398, 'learning_rate': 0.0001731844362336009, 'epoch': 6.54}
{'loss': 0.2486, 'learning_rate': 0.00017307398430056593, 'epoch': 6.55}
{'loss': 0.219, 'learning_rate': 0.00017296334073952605, 'epoch': 6.56}
{'loss': 0.2521, 'learning_rate': 0.000172852505840631, 'epoch': 6.58}
{'loss': 0.2218, 'learning_rate': 0.00017274147989453247, 'epoch': 6.59}
{'loss': 0.2315, 'learning_rate': 0.00017263026319238301, 'epoch': 6.6}
{'loss': 0.2267, 'learning_rate': 0.00017251885602583545, 'epoch': 6.61}
{'loss': 0.2188, 'learning_rate': 0.00017240725868704218, 'epoch': 6.62}
[2025-05-03 12:33:10,544] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2414, 'learning_rate': 0.0001722954714686541, 'epoch': 6.64}
[2025-05-03 12:33:27,398] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2092, 'learning_rate': 0.00017218349466382023, 'epoch': 6.65}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:33:44,402] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2343, 'learning_rate': 0.00017207132856618667, 'epoch': 6.66}
[2025-05-03 12:34:01,381] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2068, 'learning_rate': 0.0001719589734698959, 'epoch': 6.67}
[2025-05-03 12:34:18,330] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2423, 'learning_rate': 0.0001718464296695861, 'epoch': 6.69}
[2025-05-03 12:34:35,211] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2446, 'learning_rate': 0.00017173369746039025, 'epoch': 6.7}
[2025-05-03 12:34:52,103] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2673, 'learning_rate': 0.00017162077713793545, 'epoch': 6.71}
[2025-05-03 12:35:09,092] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2379, 'learning_rate': 0.00017150766899834204, 'epoch': 6.72}
[2025-05-03 12:35:25,929] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2495, 'learning_rate': 0.000171394373338223, 'epoch': 6.74}
[2025-05-03 12:35:42,803] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2291, 'learning_rate': 0.00017128089045468294, 'epoch': 6.75}
[2025-05-03 12:35:59,576] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.239, 'learning_rate': 0.00017116722064531748, 'epoch': 6.76}
[2025-05-03 12:36:16,525] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2474, 'learning_rate': 0.00017105336420821247, 'epoch': 6.78}
[2025-05-03 12:36:33,352] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2319, 'learning_rate': 0.0001709393214419431, 'epoch': 6.79}
[2025-05-03 12:36:50,411] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2183, 'learning_rate': 0.0001708250926455733, 'epoch': 6.8}
{'loss': 0.2413, 'learning_rate': 0.00017071067811865476, 'epoch': 6.81}
[2025-05-03 12:37:24,876] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2587, 'learning_rate': 0.00017059607816122618, 'epoch': 6.83}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:37:41,665] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2766, 'learning_rate': 0.00017048129307381266, 'epoch': 6.84}
[2025-05-03 12:37:58,573] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2456, 'learning_rate': 0.00017036632315742462, 'epoch': 6.85}
[2025-05-03 12:38:15,378] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2508, 'learning_rate': 0.00017025116871355735, 'epoch': 6.86}
[2025-05-03 12:38:32,266] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2563, 'learning_rate': 0.00017013583004418993, 'epoch': 6.88}
[2025-05-03 12:38:49,094] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2401, 'learning_rate': 0.00017002030745178455, 'epoch': 6.89}
[2025-05-03 12:39:06,207] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2486, 'learning_rate': 0.00016990460123928575, 'epoch': 6.9}
{'loss': 0.27, 'learning_rate': 0.0001697887117101196, 'epoch': 6.91}
[2025-05-03 12:39:39,870] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2546, 'learning_rate': 0.00016967263916819287, 'epoch': 6.92}
{'loss': 0.2477, 'learning_rate': 0.00016955638391789228, 'epoch': 6.94}
{'loss': 0.2447, 'learning_rate': 0.00016943994626408363, 'epoch': 6.95}
{'loss': 0.2547, 'learning_rate': 0.00016932332651211116, 'epoch': 6.96}
{'loss': 0.2674, 'learning_rate': 0.0001692065249677965, 'epoch': 6.97}
{'loss': 0.2385, 'learning_rate': 0.00016908954193743816, 'epoch': 6.99}
{'loss': 0.2492, 'learning_rate': 0.00016897237772781044, 'epoch': 7.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.1248, 'learning_rate': 0.00016885503264616283, 'epoch': 7.01}
{'loss': 0.1386, 'learning_rate': 0.00016873750700021915, 'epoch': 7.03}
{'loss': 0.137, 'learning_rate': 0.0001686198010981767, 'epoch': 7.04}
{'loss': 0.1373, 'learning_rate': 0.00016850191524870546, 'epoch': 7.05}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.1045, 'learning_rate': 0.00016838384976094738, 'epoch': 7.06}
{'loss': 0.1327, 'learning_rate': 0.00016826560494451537, 'epoch': 7.08}
{'loss': 0.1052, 'learning_rate': 0.00016814718110949275, 'epoch': 7.09}
[2025-05-03 12:43:39,137] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1391, 'learning_rate': 0.00016802857856643215, 'epoch': 7.1}
{'loss': 0.129, 'learning_rate': 0.00016790979762635496, 'epoch': 7.11}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:44:13,089] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1253, 'learning_rate': 0.00016779083860075033, 'epoch': 7.12}
[2025-05-03 12:44:30,096] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1174, 'learning_rate': 0.00016767170180157444, 'epoch': 7.14}
[2025-05-03 12:44:46,847] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1533, 'learning_rate': 0.00016755238754124965, 'epoch': 7.15}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:45:03,713] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1299, 'learning_rate': 0.0001674328961326637, 'epoch': 7.16}
[2025-05-03 12:45:20,549] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1368, 'learning_rate': 0.00016731322788916892, 'epoch': 7.17}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:45:37,761] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1514, 'learning_rate': 0.00016719338312458124, 'epoch': 7.19}
{'loss': 0.1432, 'learning_rate': 0.00016707336215317968, 'epoch': 7.2}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:46:12,325] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1453, 'learning_rate': 0.00016695316528970517, 'epoch': 7.21}
[2025-05-03 12:46:29,650] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1401, 'learning_rate': 0.00016683279284936004, 'epoch': 7.22}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:46:46,552] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1166, 'learning_rate': 0.00016671224514780693, 'epoch': 7.24}
[2025-05-03 12:47:03,646] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1417, 'learning_rate': 0.00016659152250116812, 'epoch': 7.25}
[2025-05-03 12:47:20,444] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1409, 'learning_rate': 0.00016647062522602473, 'epoch': 7.26}
{'loss': 0.1438, 'learning_rate': 0.00016634955363941574, 'epoch': 7.28}
{'loss': 0.16, 'learning_rate': 0.0001662283080588373, 'epoch': 7.29}
[2025-05-03 12:48:10,941] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1527, 'learning_rate': 0.00016610688880224178, 'epoch': 7.3}
[2025-05-03 12:48:28,009] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1407, 'learning_rate': 0.000165985296188037, 'epoch': 7.31}
[2025-05-03 12:48:44,986] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.142, 'learning_rate': 0.0001658635305350855, 'epoch': 7.33}
[2025-05-03 12:49:01,857] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1254, 'learning_rate': 0.0001657415921627034, 'epoch': 7.34}
[2025-05-03 12:49:18,869] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1329, 'learning_rate': 0.00016561948139065996, 'epoch': 7.35}
[2025-05-03 12:49:35,770] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1491, 'learning_rate': 0.0001654971985391764, 'epoch': 7.36}
[2025-05-03 12:49:52,733] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1556, 'learning_rate': 0.00016537474392892528, 'epoch': 7.38}
[2025-05-03 12:50:09,871] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1303, 'learning_rate': 0.00016525211788102946, 'epoch': 7.39}
[2025-05-03 12:50:26,834] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1473, 'learning_rate': 0.00016512932071706152, 'epoch': 7.4}
[2025-05-03 12:50:43,911] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1428, 'learning_rate': 0.00016500635275904272, 'epoch': 7.41}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:51:00,743] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1352, 'learning_rate': 0.0001648832143294422, 'epoch': 7.42}
[2025-05-03 12:51:17,702] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1216, 'learning_rate': 0.00016475990575117605, 'epoch': 7.44}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:51:34,628] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.131, 'learning_rate': 0.0001646364273476067, 'epoch': 7.45}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.1298, 'learning_rate': 0.00016451277944254185, 'epoch': 7.46}
[2025-05-03 12:52:08,356] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1411, 'learning_rate': 0.00016438896236023375, 'epoch': 7.47}
[2025-05-03 12:52:25,248] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1502, 'learning_rate': 0.00016426497642537825, 'epoch': 7.49}
[2025-05-03 12:52:42,114] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1617, 'learning_rate': 0.000164140821963114, 'epoch': 7.5}
[2025-05-03 12:52:58,967] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1409, 'learning_rate': 0.0001640164992990216, 'epoch': 7.51}
[2025-05-03 12:53:15,950] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1335, 'learning_rate': 0.00016389200875912278, 'epoch': 7.53}
[2025-05-03 12:53:32,880] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1604, 'learning_rate': 0.0001637673506698794, 'epoch': 7.54}
[2025-05-03 12:53:50,013] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1371, 'learning_rate': 0.00016364252535819282, 'epoch': 7.55}
[2025-05-03 12:54:06,883] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1495, 'learning_rate': 0.00016351753315140287, 'epoch': 7.56}
[2025-05-03 12:54:23,915] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1339, 'learning_rate': 0.000163392374377287, 'epoch': 7.58}
[2025-05-03 12:54:40,975] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1556, 'learning_rate': 0.00016326704936405953, 'epoch': 7.59}
[2025-05-03 12:54:57,968] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1526, 'learning_rate': 0.00016314155844037074, 'epoch': 7.6}
[2025-05-03 12:55:14,856] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1349, 'learning_rate': 0.00016301590193530584, 'epoch': 7.61}
{'loss': 0.1769, 'learning_rate': 0.00016289008017838445, 'epoch': 7.62}
[2025-05-03 12:55:48,753] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1294, 'learning_rate': 0.00016276409349955944, 'epoch': 7.64}
[2025-05-03 12:56:05,871] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1489, 'learning_rate': 0.0001626379422292162, 'epoch': 7.65}
[2025-05-03 12:56:22,790] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1594, 'learning_rate': 0.0001625116266981717, 'epoch': 7.66}
[2025-05-03 12:56:39,711] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1249, 'learning_rate': 0.00016238514723767374, 'epoch': 7.67}
[2025-05-03 12:56:56,744] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1565, 'learning_rate': 0.0001622585041793999, 'epoch': 7.69}
[2025-05-03 12:57:13,569] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1335, 'learning_rate': 0.0001621316978554569, 'epoch': 7.7}
[2025-05-03 12:57:30,423] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1412, 'learning_rate': 0.00016200472859837945, 'epoch': 7.71}
{'loss': 0.1397, 'learning_rate': 0.00016187759674112973, 'epoch': 7.72}
[2025-05-03 12:58:04,043] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1547, 'learning_rate': 0.00016175030261709615, 'epoch': 7.74}
[2025-05-03 12:58:20,957] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1678, 'learning_rate': 0.00016162284656009274, 'epoch': 7.75}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:58:37,930] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1563, 'learning_rate': 0.00016149522890435814, 'epoch': 7.76}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 12:58:54,932] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.145, 'learning_rate': 0.00016136744998455476, 'epoch': 7.78}
[2025-05-03 12:59:11,832] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1374, 'learning_rate': 0.00016123951013576794, 'epoch': 7.79}
[2025-05-03 12:59:28,826] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1559, 'learning_rate': 0.00016111140969350503, 'epoch': 7.8}
[2025-05-03 12:59:45,967] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1608, 'learning_rate': 0.00016098314899369446, 'epoch': 7.81}
[2025-05-03 13:00:02,879] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1776, 'learning_rate': 0.00016085472837268502, 'epoch': 7.83}
[2025-05-03 13:00:19,891] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1489, 'learning_rate': 0.00016072614816724478, 'epoch': 7.84}
[2025-05-03 13:00:37,031] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1784, 'learning_rate': 0.00016059740871456036, 'epoch': 7.85}
[2025-05-03 13:00:53,833] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1816, 'learning_rate': 0.00016046851035223593, 'epoch': 7.86}
[2025-05-03 13:01:10,934] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1515, 'learning_rate': 0.00016033945341829248, 'epoch': 7.88}
[2025-05-03 13:01:27,739] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1552, 'learning_rate': 0.00016021023825116672, 'epoch': 7.89}
[2025-05-03 13:01:44,625] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1654, 'learning_rate': 0.00016008086518971037, 'epoch': 7.9}
[2025-05-03 13:02:01,424] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.159, 'learning_rate': 0.0001599513345731892, 'epoch': 7.91}
[2025-05-03 13:02:18,332] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1544, 'learning_rate': 0.0001598216467412822, 'epoch': 7.92}
[2025-05-03 13:02:35,279] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1675, 'learning_rate': 0.0001596918020340805, 'epoch': 7.94}
[2025-05-03 13:02:52,345] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1547, 'learning_rate': 0.00015956180079208682, 'epoch': 7.95}
[2025-05-03 13:03:09,210] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1464, 'learning_rate': 0.0001594316433562142, 'epoch': 7.96}
[2025-05-03 13:03:26,316] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1703, 'learning_rate': 0.0001593013300677853, 'epoch': 7.97}
[2025-05-03 13:03:43,143] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1515, 'learning_rate': 0.0001591708612685316, 'epoch': 7.99}
[2025-05-03 13:04:00,134] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1958, 'learning_rate': 0.00015904023730059228, 'epoch': 8.0}
[2025-05-03 13:04:19,817] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.096, 'learning_rate': 0.00015890945850651346, 'epoch': 8.01}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 13:04:37,702] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.085, 'learning_rate': 0.00015877852522924732, 'epoch': 8.03}
[2025-05-03 13:04:54,627] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0875, 'learning_rate': 0.0001586474378121511, 'epoch': 8.04}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 13:05:11,521] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0817, 'learning_rate': 0.00015851619659898623, 'epoch': 8.05}
[2025-05-03 13:05:28,621] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0715, 'learning_rate': 0.00015838480193391754, 'epoch': 8.06}
[2025-05-03 13:05:45,778] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0994, 'learning_rate': 0.00015825325416151222, 'epoch': 8.07}
[2025-05-03 13:06:02,729] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0858, 'learning_rate': 0.00015812155362673896, 'epoch': 8.09}
[2025-05-03 13:06:19,592] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0818, 'learning_rate': 0.000157989700674967, 'epoch': 8.1}
[2025-05-03 13:06:36,612] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0904, 'learning_rate': 0.0001578576956519654, 'epoch': 8.11}
[2025-05-03 13:06:53,447] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0854, 'learning_rate': 0.00015772553890390197, 'epoch': 8.12}
[2025-05-03 13:07:11,149] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0813, 'learning_rate': 0.0001575932307773423, 'epoch': 8.14}
[2025-05-03 13:07:27,905] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1182, 'learning_rate': 0.00015746077161924905, 'epoch': 8.15}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 13:07:44,755] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.099, 'learning_rate': 0.00015732816177698098, 'epoch': 8.16}
[2025-05-03 13:08:01,615] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.081, 'learning_rate': 0.00015719540159829184, 'epoch': 8.18}
[2025-05-03 13:08:19,279] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0793, 'learning_rate': 0.00015706249143132982, 'epoch': 8.19}
[2025-05-03 13:08:36,030] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.081, 'learning_rate': 0.00015692943162463628, 'epoch': 8.2}
[2025-05-03 13:08:53,034] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0914, 'learning_rate': 0.00015679622252714507, 'epoch': 8.21}
[2025-05-03 13:09:10,073] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0766, 'learning_rate': 0.0001566628644881815, 'epoch': 8.22}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 13:09:26,986] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0796, 'learning_rate': 0.0001565293578574615, 'epoch': 8.24}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 13:09:43,872] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0787, 'learning_rate': 0.00015639570298509064, 'epoch': 8.25}
[2025-05-03 13:10:00,820] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0836, 'learning_rate': 0.00015626190022156327, 'epoch': 8.26}
[2025-05-03 13:10:17,898] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0909, 'learning_rate': 0.00015612794991776147, 'epoch': 8.28}
[2025-05-03 13:10:34,818] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0769, 'learning_rate': 0.00015599385242495438, 'epoch': 8.29}
[2025-05-03 13:10:51,648] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0717, 'learning_rate': 0.00015585960809479696, 'epoch': 8.3}
[2025-05-03 13:11:08,690] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.091, 'learning_rate': 0.00015572521727932935, 'epoch': 8.31}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 13:11:25,604] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1022, 'learning_rate': 0.00015559068033097582, 'epoch': 8.32}
[2025-05-03 13:11:42,406] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0733, 'learning_rate': 0.00015545599760254382, 'epoch': 8.34}
{'loss': 0.0996, 'learning_rate': 0.00015532116944722308, 'epoch': 8.35}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.1043, 'learning_rate': 0.00015518619621858476, 'epoch': 8.36}
{'loss': 0.0891, 'learning_rate': 0.00015505107827058036, 'epoch': 8.38}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0981, 'learning_rate': 0.000154915815957541, 'epoch': 8.39}
[2025-05-03 13:13:06,705] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0958, 'learning_rate': 0.0001547804096341763, 'epoch': 8.4}
{'loss': 0.0822, 'learning_rate': 0.0001546448596555736, 'epoch': 8.41}
{'loss': 0.0881, 'learning_rate': 0.00015450916637719684, 'epoch': 8.43}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.1124, 'learning_rate': 0.00015437333015488587, 'epoch': 8.44}
[2025-05-03 13:14:14,105] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0799, 'learning_rate': 0.00015423735134485536, 'epoch': 8.45}
{'loss': 0.0913, 'learning_rate': 0.00015410123030369386, 'epoch': 8.46}
{'loss': 0.0844, 'learning_rate': 0.00015396496738836292, 'epoch': 8.47}
{'loss': 0.0779, 'learning_rate': 0.0001538285629561962, 'epoch': 8.49}
{'loss': 0.0887, 'learning_rate': 0.0001536920173648984, 'epoch': 8.5}
{'loss': 0.0941, 'learning_rate': 0.00015355533097254436, 'epoch': 8.51}
{'loss': 0.0795, 'learning_rate': 0.0001534185041375783, 'epoch': 8.53}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0835, 'learning_rate': 0.0001532815372188126, 'epoch': 8.54}
{'loss': 0.1016, 'learning_rate': 0.00015314443057542703, 'epoch': 8.55}
{'loss': 0.068, 'learning_rate': 0.00015300718456696778, 'epoch': 8.56}
{'loss': 0.089, 'learning_rate': 0.00015286979955334652, 'epoch': 8.57}
{'loss': 0.0817, 'learning_rate': 0.00015273227589483946, 'epoch': 8.59}
{'loss': 0.1015, 'learning_rate': 0.00015259461395208628, 'epoch': 8.6}
{'loss': 0.0952, 'learning_rate': 0.00015245681408608945, 'epoch': 8.61}
{'loss': 0.0922, 'learning_rate': 0.000152318876658213, 'epoch': 8.62}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0856, 'learning_rate': 0.00015218080203018182, 'epoch': 8.64}
{'loss': 0.1026, 'learning_rate': 0.00015204259056408046, 'epoch': 8.65}
{'loss': 0.0983, 'learning_rate': 0.0001519042426223524, 'epoch': 8.66}
{'loss': 0.0911, 'learning_rate': 0.00015176575856779904, 'epoch': 8.68}
{'loss': 0.0995, 'learning_rate': 0.00015162713876357858, 'epoch': 8.69}
{'loss': 0.0977, 'learning_rate': 0.00015148838357320537, 'epoch': 8.7}
{'loss': 0.0872, 'learning_rate': 0.00015134949336054865, 'epoch': 8.71}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0888, 'learning_rate': 0.0001512104684898319, 'epoch': 8.72}
{'loss': 0.0886, 'learning_rate': 0.0001510713093256315, 'epoch': 8.74}
{'loss': 0.0875, 'learning_rate': 0.00015093201623287631, 'epoch': 8.75}
{'loss': 0.0925, 'learning_rate': 0.0001507925895768461, 'epoch': 8.76}
{'loss': 0.0977, 'learning_rate': 0.00015065302972317108, 'epoch': 8.78}
{'loss': 0.0869, 'learning_rate': 0.00015051333703783068, 'epoch': 8.79}
{'loss': 0.0873, 'learning_rate': 0.00015037351188715265, 'epoch': 8.8}
{'loss': 0.0896, 'learning_rate': 0.0001502335546378122, 'epoch': 8.81}
{'loss': 0.0974, 'learning_rate': 0.00015009346565683087, 'epoch': 8.82}
{'loss': 0.098, 'learning_rate': 0.0001499532453115757, 'epoch': 8.84}
{'loss': 0.0879, 'learning_rate': 0.00014981289396975817, 'epoch': 8.85}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0806, 'learning_rate': 0.00014967241199943332, 'epoch': 8.86}
{'loss': 0.1038, 'learning_rate': 0.00014953179976899878, 'epoch': 8.88}
{'loss': 0.1087, 'learning_rate': 0.00014939105764719368, 'epoch': 8.89}
{'loss': 0.0989, 'learning_rate': 0.00014925018600309785, 'epoch': 8.9}
{'loss': 0.0967, 'learning_rate': 0.00014910918520613073, 'epoch': 8.91}
{'loss': 0.1065, 'learning_rate': 0.0001489680556260505, 'epoch': 8.93}
{'loss': 0.1223, 'learning_rate': 0.00014882679763295306, 'epoch': 8.94}
[2025-05-03 13:25:29,884] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0883, 'learning_rate': 0.00014868541159727096, 'epoch': 8.95}
[2025-05-03 13:25:46,715] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0987, 'learning_rate': 0.00014854389788977266, 'epoch': 8.96}
[2025-05-03 13:26:03,606] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0912, 'learning_rate': 0.0001484022568815613, 'epoch': 8.97}
{'loss': 0.0741, 'learning_rate': 0.00014826048894407397, 'epoch': 8.99}
{'loss': 0.0964, 'learning_rate': 0.00014811859444908052, 'epoch': 9.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 13:26:57,338] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0552, 'learning_rate': 0.00014797657376868273, 'epoch': 9.01}
[2025-05-03 13:27:14,438] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0485, 'learning_rate': 0.00014783442727531328, 'epoch': 9.03}
[2025-05-03 13:27:31,437] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0439, 'learning_rate': 0.00014769215534173475, 'epoch': 9.04}
[2025-05-03 13:27:48,416] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0441, 'learning_rate': 0.00014754975834103877, 'epoch': 9.05}
[2025-05-03 13:28:06,143] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0543, 'learning_rate': 0.00014740723664664483, 'epoch': 9.06}
[2025-05-03 13:28:22,981] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0468, 'learning_rate': 0.00014726459063229945, 'epoch': 9.07}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 13:28:39,909] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0627, 'learning_rate': 0.00014712182067207517, 'epoch': 9.09}
[2025-05-03 13:28:56,802] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0434, 'learning_rate': 0.00014697892714036958, 'epoch': 9.1}
[2025-05-03 13:29:13,941] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0435, 'learning_rate': 0.0001468359104119043, 'epoch': 9.11}
[2025-05-03 13:29:30,768] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.034, 'learning_rate': 0.00014669277086172406, 'epoch': 9.12}
[2025-05-03 13:29:47,777] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0596, 'learning_rate': 0.00014654950886519564, 'epoch': 9.14}
[2025-05-03 13:30:04,647] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.052, 'learning_rate': 0.00014640612479800686, 'epoch': 9.15}
[2025-05-03 13:30:22,360] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0882, 'learning_rate': 0.00014626261903616578, 'epoch': 9.16}
[2025-05-03 13:30:39,131] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0374, 'learning_rate': 0.00014611899195599953, 'epoch': 9.18}
[2025-05-03 13:30:55,957] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0498, 'learning_rate': 0.00014597524393415335, 'epoch': 9.19}
[2025-05-03 13:31:12,731] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.048, 'learning_rate': 0.00014583137534758967, 'epoch': 9.2}
[2025-05-03 13:31:29,682] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0557, 'learning_rate': 0.00014568738657358714, 'epoch': 9.21}
[2025-05-03 13:31:46,624] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0533, 'learning_rate': 0.0001455432779897395, 'epoch': 9.22}
[2025-05-03 13:32:03,468] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0502, 'learning_rate': 0.00014539904997395468, 'epoch': 9.24}
[2025-05-03 13:32:20,451] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0524, 'learning_rate': 0.00014525470290445392, 'epoch': 9.25}
[2025-05-03 13:32:37,334] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0463, 'learning_rate': 0.00014511023715977047, 'epoch': 9.26}
[2025-05-03 13:32:54,171] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0629, 'learning_rate': 0.00014496565311874902, 'epoch': 9.28}
{'loss': 0.0608, 'learning_rate': 0.00014482095116054422, 'epoch': 9.29}
{'loss': 0.0641, 'learning_rate': 0.00014467613166462023, 'epoch': 9.3}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0476, 'learning_rate': 0.00014453119501074924, 'epoch': 9.31}
{'loss': 0.0678, 'learning_rate': 0.0001443861415790107, 'epoch': 9.32}
{'loss': 0.0491, 'learning_rate': 0.00014424097174979038, 'epoch': 9.34}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0553, 'learning_rate': 0.00014409568590377918, 'epoch': 9.35}
{'loss': 0.0485, 'learning_rate': 0.0001439502844219723, 'epoch': 9.36}
{'loss': 0.0531, 'learning_rate': 0.00014380476768566824, 'epoch': 9.38}
{'loss': 0.0518, 'learning_rate': 0.00014365913607646761, 'epoch': 9.39}
{'loss': 0.0606, 'learning_rate': 0.00014351338997627234, 'epoch': 9.4}
{'loss': 0.0586, 'learning_rate': 0.0001433675297672846, 'epoch': 9.41}
{'loss': 0.0564, 'learning_rate': 0.00014322155583200576, 'epoch': 9.43}
{'loss': 0.0603, 'learning_rate': 0.00014307546855323549, 'epoch': 9.44}
{'loss': 0.0456, 'learning_rate': 0.00014292926831407061, 'epoch': 9.45}
{'loss': 0.0514, 'learning_rate': 0.0001427829554979042, 'epoch': 9.46}
{'loss': 0.0571, 'learning_rate': 0.0001426365304884246, 'epoch': 9.47}
{'loss': 0.0747, 'learning_rate': 0.0001424899936696143, 'epoch': 9.49}
{'loss': 0.0614, 'learning_rate': 0.00014234334542574906, 'epoch': 9.5}
{'loss': 0.0569, 'learning_rate': 0.00014219658614139674, 'epoch': 9.51}
{'loss': 0.0511, 'learning_rate': 0.00014204971620141647, 'epoch': 9.53}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0484, 'learning_rate': 0.00014190273599095762, 'epoch': 9.54}
{'loss': 0.0575, 'learning_rate': 0.00014175564589545854, 'epoch': 9.55}
{'loss': 0.0384, 'learning_rate': 0.00014160844630064595, 'epoch': 9.56}
{'loss': 0.0588, 'learning_rate': 0.00014146113759253362, 'epoch': 9.57}
{'loss': 0.04, 'learning_rate': 0.00014131372015742142, 'epoch': 9.59}
{'loss': 0.0606, 'learning_rate': 0.0001411661943818944, 'epoch': 9.6}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0506, 'learning_rate': 0.00014101856065282172, 'epoch': 9.61}
{'loss': 0.0558, 'learning_rate': 0.00014087081935735564, 'epoch': 9.62}
{'loss': 0.0627, 'learning_rate': 0.00014072297088293042, 'epoch': 9.64}
{'loss': 0.0734, 'learning_rate': 0.00014057501561726157, 'epoch': 9.65}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0423, 'learning_rate': 0.00014042695394834436, 'epoch': 9.66}
{'loss': 0.0513, 'learning_rate': 0.0001402787862644534, 'epoch': 9.68}
{'loss': 0.0614, 'learning_rate': 0.00014013051295414108, 'epoch': 9.69}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0548, 'learning_rate': 0.0001399821344062369, 'epoch': 9.7}
{'loss': 0.0515, 'learning_rate': 0.00013983365100984633, 'epoch': 9.71}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0492, 'learning_rate': 0.00013968506315434974, 'epoch': 9.72}
[2025-05-03 13:43:17,457] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0559, 'learning_rate': 0.00013953637122940147, 'epoch': 9.74}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 13:43:34,407] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0616, 'learning_rate': 0.00013938757562492873, 'epoch': 9.75}
[2025-05-03 13:43:51,175] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0492, 'learning_rate': 0.00013923867673113066, 'epoch': 9.76}
[2025-05-03 13:44:07,996] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0574, 'learning_rate': 0.0001390896749384773, 'epoch': 9.78}
[2025-05-03 13:44:24,985] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0567, 'learning_rate': 0.0001389405706377084, 'epoch': 9.79}
[2025-05-03 13:44:41,985] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0516, 'learning_rate': 0.00013879136421983266, 'epoch': 9.8}
[2025-05-03 13:44:58,931] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0672, 'learning_rate': 0.00013864205607612648, 'epoch': 9.81}
[2025-05-03 13:45:15,859] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0538, 'learning_rate': 0.00013849264659813312, 'epoch': 9.82}
[2025-05-03 13:45:32,974] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0619, 'learning_rate': 0.00013834313617766146, 'epoch': 9.84}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
[2025-05-03 13:45:49,918] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0491, 'learning_rate': 0.0001381935252067852, 'epoch': 9.85}
[2025-05-03 13:46:06,861] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0598, 'learning_rate': 0.0001380438140778416, 'epoch': 9.86}
[2025-05-03 13:46:23,902] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0485, 'learning_rate': 0.00013789400318343068, 'epoch': 9.88}
[2025-05-03 13:46:40,896] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0482, 'learning_rate': 0.00013774409291641407, 'epoch': 9.89}
[2025-05-03 13:46:58,608] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0668, 'learning_rate': 0.0001375940836699139, 'epoch': 9.9}
[2025-05-03 13:47:15,435] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0411, 'learning_rate': 0.00013744397583731203, 'epoch': 9.91}
[2025-05-03 13:47:32,429] [WARNING] [stage3.py:2139:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0373, 'learning_rate': 0.0001372937698122487, 'epoch': 9.93}
[2025-05-03 13:47:49,232] [WARNING] [stage3.py:2139:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0608, 'learning_rate': 0.00013714346598862166, 'epoch': 9.94}
{'loss': 0.0612, 'learning_rate': 0.0001369930647605852, 'epoch': 9.95}
{'loss': 0.0507, 'learning_rate': 0.00013684256652254906, 'epoch': 9.96}
{'loss': 0.053, 'learning_rate': 0.00013669197166917723, 'epoch': 9.97}
{'loss': 0.0541, 'learning_rate': 0.0001365412805953872, 'epoch': 9.99}
{'loss': 0.0467, 'learning_rate': 0.00013639049369634876, 'epoch': 10.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0234, 'learning_rate': 0.00013623961136748295, 'epoch': 10.01}
{'loss': 0.0183, 'learning_rate': 0.00013608863400446113, 'epoch': 10.03}
{'loss': 0.027, 'learning_rate': 0.00013593756200320372, 'epoch': 10.04}
{'loss': 0.038, 'learning_rate': 0.00013578639575987958, 'epoch': 10.05}
{'loss': 0.0427, 'learning_rate': 0.0001356351356709045, 'epoch': 10.06}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0294, 'learning_rate': 0.0001354837821329404, 'epoch': 10.07}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0233, 'learning_rate': 0.00013533233554289433, 'epoch': 10.09}
{'loss': 0.0222, 'learning_rate': 0.00013518079629791724, 'epoch': 10.1}
{'loss': 0.0253, 'learning_rate': 0.00013502916479540326, 'epoch': 10.11}
{'loss': 0.0247, 'learning_rate': 0.00013487744143298822, 'epoch': 10.12}
{'loss': 0.0261, 'learning_rate': 0.00013472562660854902, 'epoch': 10.14}
{'loss': 0.0302, 'learning_rate': 0.0001345737207202023, 'epoch': 10.15}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.022, 'learning_rate': 0.00013442172416630355, 'epoch': 10.16}
{'loss': 0.0356, 'learning_rate': 0.000134269637345446, 'epoch': 10.18}
{'loss': 0.0307, 'learning_rate': 0.0001341174606564596, 'epoch': 10.19}
{'loss': 0.0284, 'learning_rate': 0.00013396519449841005, 'epoch': 10.2}
{'loss': 0.0353, 'learning_rate': 0.00013381283927059752, 'epoch': 10.21}
{'loss': 0.0376, 'learning_rate': 0.0001336603953725559, 'epoch': 10.22}
{'loss': 0.0253, 'learning_rate': 0.00013350786320405144, 'epoch': 10.24}
{'loss': 0.0359, 'learning_rate': 0.00013335524316508208, 'epoch': 10.25}
{'loss': 0.0344, 'learning_rate': 0.000133202535655876, 'epoch': 10.26}
{'loss': 0.0262, 'learning_rate': 0.00013304974107689087, 'epoch': 10.28}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0361, 'learning_rate': 0.0001328968598288127, 'epoch': 10.29}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0189, 'learning_rate': 0.00013274389231255466, 'epoch': 10.3}
{'loss': 0.0226, 'learning_rate': 0.00013259083892925633, 'epoch': 10.31}
{'loss': 0.0316, 'learning_rate': 0.00013243770008028224, 'epoch': 10.32}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0345, 'learning_rate': 0.00013228447616722128, 'epoch': 10.34}
{'loss': 0.0464, 'learning_rate': 0.00013213116759188523, 'epoch': 10.35}
{'loss': 0.0386, 'learning_rate': 0.000131977774756308, 'epoch': 10.36}
{'loss': 0.0309, 'learning_rate': 0.0001318242980627444, 'epoch': 10.38}
{'loss': 0.0356, 'learning_rate': 0.00013167073791366915, 'epoch': 10.39}
{'loss': 0.0204, 'learning_rate': 0.00013151709471177588, 'epoch': 10.4}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0307, 'learning_rate': 0.0001313633688599759, 'epoch': 10.41}
{'loss': 0.0261, 'learning_rate': 0.00013120956076139746, 'epoch': 10.43}
{'loss': 0.0347, 'learning_rate': 0.00013105567081938424, 'epoch': 10.44}
{'loss': 0.0367, 'learning_rate': 0.00013090169943749476, 'epoch': 10.45}
{'loss': 0.0416, 'learning_rate': 0.00013074764701950095, 'epoch': 10.46}
{'loss': 0.0242, 'learning_rate': 0.0001305935139693874, 'epoch': 10.47}
{'loss': 0.0337, 'learning_rate': 0.00013043930069134998, 'epoch': 10.49}
{'loss': 0.0228, 'learning_rate': 0.00013028500758979506, 'epoch': 10.5}
{'loss': 0.0224, 'learning_rate': 0.00013013063506933837, 'epoch': 10.51}
{'loss': 0.0288, 'learning_rate': 0.00012997618353480377, 'epoch': 10.53}
{'loss': 0.029, 'learning_rate': 0.00012982165339122246, 'epoch': 10.54}
{'loss': 0.043, 'learning_rate': 0.00012966704504383168, 'epoch': 10.55}
{'loss': 0.0369, 'learning_rate': 0.00012951235889807386, 'epoch': 10.56}
{'loss': 0.0222, 'learning_rate': 0.00012935759535959528, 'epoch': 10.57}
{'loss': 0.0286, 'learning_rate': 0.00012920275483424538, 'epoch': 10.59}
{'loss': 0.0455, 'learning_rate': 0.00012904783772807533, 'epoch': 10.6}
{'loss': 0.0185, 'learning_rate': 0.00012889284444733722, 'epoch': 10.61}
{'loss': 0.0251, 'learning_rate': 0.00012873777539848283, 'epoch': 10.62}
{'loss': 0.0289, 'learning_rate': 0.00012858263098816265, 'epoch': 10.64}
{'loss': 0.0248, 'learning_rate': 0.00012842741162322487, 'epoch': 10.65}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0317, 'learning_rate': 0.0001282721177107141, 'epoch': 10.66}
{'loss': 0.0265, 'learning_rate': 0.00012811674965787056, 'epoch': 10.68}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0388, 'learning_rate': 0.0001279613078721289, 'epoch': 10.69}
{'loss': 0.0217, 'learning_rate': 0.00012780579276111702, 'epoch': 10.7}
{'loss': 0.0424, 'learning_rate': 0.00012765020473265519, 'epoch': 10.71}
{'loss': 0.0334, 'learning_rate': 0.00012749454419475487, 'epoch': 10.72}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0363, 'learning_rate': 0.0001273388115556177, 'epoch': 10.74}
{'loss': 0.0289, 'learning_rate': 0.0001271830072236343, 'epoch': 10.75}
{'loss': 0.0383, 'learning_rate': 0.00012702713160738345, 'epoch': 10.76}
{'loss': 0.0311, 'learning_rate': 0.00012687118511563075, 'epoch': 10.78}
{'loss': 0.0272, 'learning_rate': 0.00012671516815732767, 'epoch': 10.79}
{'loss': 0.0428, 'learning_rate': 0.0001265590811416105, 'epoch': 10.8}
{'loss': 0.0256, 'learning_rate': 0.0001264029244777993, 'epoch': 10.81}
{'loss': 0.0304, 'learning_rate': 0.0001262466985753967, 'epoch': 10.82}
{'loss': 0.0273, 'learning_rate': 0.00012609040384408684, 'epoch': 10.84}
{'loss': 0.0349, 'learning_rate': 0.0001259340406937345, 'epoch': 10.85}
{'loss': 0.0358, 'learning_rate': 0.00012577760953438383, 'epoch': 10.86}
{'loss': 0.033, 'learning_rate': 0.00012562111077625722, 'epoch': 10.88}
{'loss': 0.0384, 'learning_rate': 0.00012546454482975454, 'epoch': 10.89}
{'loss': 0.0228, 'learning_rate': 0.00012530791210545162, 'epoch': 10.9}
{'loss': 0.0325, 'learning_rate': 0.0001251512130140996, 'epoch': 10.91}
{'loss': 0.0356, 'learning_rate': 0.00012499444796662353, 'epoch': 10.93}
{'loss': 0.0286, 'learning_rate': 0.0001248376173741215, 'epoch': 10.94}
{'loss': 0.0241, 'learning_rate': 0.0001246807216478634, 'epoch': 10.95}
{'loss': 0.0226, 'learning_rate': 0.00012452376119929007, 'epoch': 10.96}
{'loss': 0.0249, 'learning_rate': 0.00012436673644001197, 'epoch': 10.97}
{'loss': 0.0348, 'learning_rate': 0.00012420964778180814, 'epoch': 10.99}
{'loss': 0.0322, 'learning_rate': 0.00012405249563662537, 'epoch': 11.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0136, 'learning_rate': 0.0001238952804165768, 'epoch': 11.01}
{'loss': 0.0157, 'learning_rate': 0.00012373800253394102, 'epoch': 11.03}
{'loss': 0.0164, 'learning_rate': 0.00012358066240116092, 'epoch': 11.04}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0246, 'learning_rate': 0.00012342326043084266, 'epoch': 11.05}
{'loss': 0.015, 'learning_rate': 0.00012326579703575462, 'epoch': 11.06}
{'loss': 0.0171, 'learning_rate': 0.00012310827262882615, 'epoch': 11.07}
{'loss': 0.0187, 'learning_rate': 0.0001229506876231466, 'epoch': 11.09}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0106, 'learning_rate': 0.00012279304243196436, 'epoch': 11.1}
{'loss': 0.0139, 'learning_rate': 0.00012263533746868552, 'epoch': 11.11}
{'loss': 0.0145, 'learning_rate': 0.00012247757314687297, 'epoch': 11.12}
{'loss': 0.0156, 'learning_rate': 0.0001223197498802452, 'epoch': 11.14}
{'loss': 0.0103, 'learning_rate': 0.00012216186808267546, 'epoch': 11.15}
{'loss': 0.0123, 'learning_rate': 0.00012200392816819022, 'epoch': 11.16}
{'loss': 0.0171, 'learning_rate': 0.00012184593055096854, 'epoch': 11.18}
{'loss': 0.0142, 'learning_rate': 0.00012168787564534078, 'epoch': 11.19}
{'loss': 0.0114, 'learning_rate': 0.0001215297638657875, 'epoch': 11.2}
{'loss': 0.0128, 'learning_rate': 0.00012137159562693838, 'epoch': 11.21}
{'loss': 0.0239, 'learning_rate': 0.0001212133713435712, 'epoch': 11.22}
{'loss': 0.0272, 'learning_rate': 0.00012105509143061071, 'epoch': 11.24}
{'loss': 0.0355, 'learning_rate': 0.00012089675630312754, 'epoch': 11.25}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0169, 'learning_rate': 0.00012073836637633705, 'epoch': 11.26}
{'loss': 0.0176, 'learning_rate': 0.00012057992206559837, 'epoch': 11.28}
{'loss': 0.013, 'learning_rate': 0.0001204214237864133, 'epoch': 11.29}
{'loss': 0.0158, 'learning_rate': 0.00012026287195442503, 'epoch': 11.3}
{'loss': 0.0135, 'learning_rate': 0.00012010426698541728, 'epoch': 11.31}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0179, 'learning_rate': 0.00011994560929531309, 'epoch': 11.32}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0168, 'learning_rate': 0.00011978689930017379, 'epoch': 11.34}
{'loss': 0.0117, 'learning_rate': 0.00011962813741619777, 'epoch': 11.35}
{'loss': 0.0158, 'learning_rate': 0.0001194693240597196, 'epoch': 11.36}
{'loss': 0.0227, 'learning_rate': 0.00011931045964720881, 'epoch': 11.38}
{'loss': 0.0242, 'learning_rate': 0.00011915154459526875, 'epoch': 11.39}
{'loss': 0.0113, 'learning_rate': 0.0001189925793206357, 'epoch': 11.4}
{'loss': 0.0242, 'learning_rate': 0.00011883356424017748, 'epoch': 11.41}
{'loss': 0.0149, 'learning_rate': 0.00011867449977089265, 'epoch': 11.43}
{'loss': 0.0144, 'learning_rate': 0.00011851538632990921, 'epoch': 11.44}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0116, 'learning_rate': 0.00011835622433448361, 'epoch': 11.45}
{'loss': 0.0118, 'learning_rate': 0.00011819701420199969, 'epoch': 11.46}
{'loss': 0.0215, 'learning_rate': 0.00011803775634996734, 'epoch': 11.47}
{'loss': 0.0128, 'learning_rate': 0.00011787845119602183, 'epoch': 11.49}
{'loss': 0.0192, 'learning_rate': 0.0001177190991579223, 'epoch': 11.5}
{'loss': 0.0092, 'learning_rate': 0.00011755970065355086, 'epoch': 11.51}
{'loss': 0.0149, 'learning_rate': 0.00011740025610091159, 'epoch': 11.53}
{'loss': 0.0211, 'learning_rate': 0.00011724076591812918, 'epoch': 11.54}
{'loss': 0.0172, 'learning_rate': 0.00011708123052344804, 'epoch': 11.55}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0157, 'learning_rate': 0.00011692165033523117, 'epoch': 11.56}
{'loss': 0.0108, 'learning_rate': 0.00011676202577195901, 'epoch': 11.57}
{'loss': 0.018, 'learning_rate': 0.00011660235725222835, 'epoch': 11.59}
{'loss': 0.0176, 'learning_rate': 0.0001164426451947513, 'epoch': 11.6}
{'loss': 0.0247, 'learning_rate': 0.00011628289001835404, 'epoch': 11.61}
{'loss': 0.0196, 'learning_rate': 0.00011612309214197599, 'epoch': 11.62}
{'loss': 0.0107, 'learning_rate': 0.0001159632519846684, 'epoch': 11.64}
{'loss': 0.0113, 'learning_rate': 0.00011580336996559343, 'epoch': 11.65}
{'loss': 0.0111, 'learning_rate': 0.0001156434465040231, 'epoch': 11.66}
{'loss': 0.0165, 'learning_rate': 0.00011548348201933798, 'epoch': 11.68}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0083, 'learning_rate': 0.00011532347693102632, 'epoch': 11.69}
{'loss': 0.0184, 'learning_rate': 0.00011516343165868279, 'epoch': 11.7}
{'loss': 0.0138, 'learning_rate': 0.00011500334662200749, 'epoch': 11.71}
{'loss': 0.025, 'learning_rate': 0.00011484322224080472, 'epoch': 11.72}
{'loss': 0.018, 'learning_rate': 0.00011468305893498203, 'epoch': 11.74}
{'loss': 0.0223, 'learning_rate': 0.00011452285712454904, 'epoch': 11.75}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.012, 'learning_rate': 0.00011436261722961628, 'epoch': 11.76}
{'loss': 0.0254, 'learning_rate': 0.00011420233967039422, 'epoch': 11.78}
{'loss': 0.0188, 'learning_rate': 0.00011404202486719205, 'epoch': 11.79}
{'loss': 0.0142, 'learning_rate': 0.00011388167324041669, 'epoch': 11.8}
{'loss': 0.0153, 'learning_rate': 0.00011372128521057155, 'epoch': 11.81}
{'loss': 0.0204, 'learning_rate': 0.00011356086119825553, 'epoch': 11.82}
{'loss': 0.0224, 'learning_rate': 0.00011340040162416197, 'epoch': 11.84}
{'loss': 0.0198, 'learning_rate': 0.00011323990690907733, 'epoch': 11.85}
{'loss': 0.0223, 'learning_rate': 0.00011307937747388034, 'epoch': 11.86}
{'loss': 0.0197, 'learning_rate': 0.00011291881373954065, 'epoch': 11.88}
{'loss': 0.0233, 'learning_rate': 0.00011275821612711803, 'epoch': 11.89}
{'loss': 0.024, 'learning_rate': 0.00011259758505776092, 'epoch': 11.9}
{'loss': 0.0208, 'learning_rate': 0.00011243692095270564, 'epoch': 11.91}
{'loss': 0.0129, 'learning_rate': 0.00011227622423327502, 'epoch': 11.93}
{'loss': 0.0195, 'learning_rate': 0.00011211549532087749, 'epoch': 11.94}
{'loss': 0.0209, 'learning_rate': 0.0001119547346370059, 'epoch': 11.95}
{'loss': 0.021, 'learning_rate': 0.0001117939426032364, 'epoch': 11.96}
{'loss': 0.0264, 'learning_rate': 0.00011163311964122734, 'epoch': 11.97}
{'loss': 0.0176, 'learning_rate': 0.0001114722661727182, 'epoch': 11.99}
{'loss': 0.0217, 'learning_rate': 0.00011131138261952845, 'epoch': 12.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0098, 'learning_rate': 0.00011115046940355642, 'epoch': 12.01}
{'loss': 0.01, 'learning_rate': 0.00011098952694677829, 'epoch': 12.03}
{'loss': 0.0068, 'learning_rate': 0.00011082855567124692, 'epoch': 12.04}
{'loss': 0.013, 'learning_rate': 0.00011066755599909064, 'epoch': 12.05}
{'loss': 0.0107, 'learning_rate': 0.0001105065283525124, 'epoch': 12.06}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0095, 'learning_rate': 0.00011034547315378838, 'epoch': 12.07}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0075, 'learning_rate': 0.00011018439082526707, 'epoch': 12.09}
{'loss': 0.0116, 'learning_rate': 0.00011002328178936811, 'epoch': 12.1}
{'loss': 0.0092, 'learning_rate': 0.00010986214646858115, 'epoch': 12.11}
{'loss': 0.0138, 'learning_rate': 0.00010970098528546481, 'epoch': 12.12}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0122, 'learning_rate': 0.00010953979866264548, 'epoch': 12.14}
{'loss': 0.0036, 'learning_rate': 0.00010937858702281631, 'epoch': 12.15}
{'loss': 0.0049, 'learning_rate': 0.00010921735078873598, 'epoch': 12.16}
{'loss': 0.0095, 'learning_rate': 0.00010905609038322779, 'epoch': 12.18}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0079, 'learning_rate': 0.0001088948062291783, 'epoch': 12.19}
{'loss': 0.0144, 'learning_rate': 0.0001087334987495364, 'epoch': 12.2}
{'loss': 0.006, 'learning_rate': 0.00010857216836731222, 'epoch': 12.21}
{'loss': 0.0131, 'learning_rate': 0.00010841081550557578, 'epoch': 12.22}
{'loss': 0.0073, 'learning_rate': 0.00010824944058745623, 'epoch': 12.24}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0046, 'learning_rate': 0.00010808804403614043, 'epoch': 12.25}
{'loss': 0.0044, 'learning_rate': 0.00010792662627487207, 'epoch': 12.26}
{'loss': 0.004, 'learning_rate': 0.00010776518772695034, 'epoch': 12.28}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0124, 'learning_rate': 0.00010760372881572905, 'epoch': 12.29}
{'loss': 0.0102, 'learning_rate': 0.0001074422499646154, 'epoch': 12.3}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0088, 'learning_rate': 0.0001072807515970688, 'epoch': 12.31}
{'loss': 0.0072, 'learning_rate': 0.00010711923413659995, 'epoch': 12.32}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0079, 'learning_rate': 0.00010695769800676949, 'epoch': 12.34}
{'loss': 0.0086, 'learning_rate': 0.00010679614363118717, 'epoch': 12.35}
{'loss': 0.0064, 'learning_rate': 0.00010663457143351044, 'epoch': 12.36}
{'loss': 0.0165, 'learning_rate': 0.00010647298183744359, 'epoch': 12.38}
{'loss': 0.0172, 'learning_rate': 0.00010631137526673647, 'epoch': 12.39}
{'loss': 0.0093, 'learning_rate': 0.0001061497521451835, 'epoch': 12.4}
{'loss': 0.0113, 'learning_rate': 0.00010598811289662243, 'epoch': 12.41}
{'loss': 0.0051, 'learning_rate': 0.00010582645794493337, 'epoch': 12.43}
{'loss': 0.0147, 'learning_rate': 0.00010566478771403763, 'epoch': 12.44}
{'loss': 0.0095, 'learning_rate': 0.00010550310262789649, 'epoch': 12.45}
{'loss': 0.0152, 'learning_rate': 0.00010534140311051026, 'epoch': 12.46}
{'loss': 0.0134, 'learning_rate': 0.00010517968958591705, 'epoch': 12.47}
{'loss': 0.0058, 'learning_rate': 0.00010501796247819175, 'epoch': 12.49}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0102, 'learning_rate': 0.00010485622221144484, 'epoch': 12.5}
{'loss': 0.0058, 'learning_rate': 0.00010469446920982129, 'epoch': 12.51}
{'loss': 0.0039, 'learning_rate': 0.00010453270389749957, 'epoch': 12.53}
{'loss': 0.0038, 'learning_rate': 0.00010437092669869024, 'epoch': 12.54}
{'loss': 0.0098, 'learning_rate': 0.00010420913803763521, 'epoch': 12.55}
{'loss': 0.014, 'learning_rate': 0.00010404733833860639, 'epoch': 12.56}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.01, 'learning_rate': 0.00010388552802590462, 'epoch': 12.57}
{'loss': 0.0052, 'learning_rate': 0.00010372370752385853, 'epoch': 12.59}
{'loss': 0.0134, 'learning_rate': 0.00010356187725682359, 'epoch': 12.6}
{'loss': 0.0086, 'learning_rate': 0.00010340003764918078, 'epoch': 12.61}
{'loss': 0.0097, 'learning_rate': 0.00010323818912533561, 'epoch': 12.62}
{'loss': 0.0062, 'learning_rate': 0.00010307633210971697, 'epoch': 12.64}
{'loss': 0.007, 'learning_rate': 0.00010291446702677599, 'epoch': 12.65}
{'loss': 0.0174, 'learning_rate': 0.000102752594300985, 'epoch': 12.66}
{'loss': 0.0045, 'learning_rate': 0.00010259071435683636, 'epoch': 12.68}
{'loss': 0.0079, 'learning_rate': 0.00010242882761884131, 'epoch': 12.69}
{'loss': 0.0133, 'learning_rate': 0.000102266934511529, 'epoch': 12.7}
{'loss': 0.0297, 'learning_rate': 0.00010210503545944521, 'epoch': 12.71}
{'loss': 0.009, 'learning_rate': 0.00010194313088715135, 'epoch': 12.72}
{'loss': 0.0085, 'learning_rate': 0.00010178122121922324, 'epoch': 12.74}
{'loss': 0.0068, 'learning_rate': 0.00010161930688025017, 'epoch': 12.75}
{'loss': 0.013, 'learning_rate': 0.00010145738829483353, 'epoch': 12.76}
{'loss': 0.0087, 'learning_rate': 0.00010129546588758605, 'epoch': 12.78}
{'loss': 0.0167, 'learning_rate': 0.00010113354008313025, 'epoch': 12.79}
{'loss': 0.0161, 'learning_rate': 0.00010097161130609773, 'epoch': 12.8}
{'loss': 0.0125, 'learning_rate': 0.00010080967998112787, 'epoch': 12.81}
{'loss': 0.0052, 'learning_rate': 0.00010064774653286661, 'epoch': 12.82}
{'loss': 0.0079, 'learning_rate': 0.00010048581138596563, 'epoch': 12.84}
{'loss': 0.023, 'learning_rate': 0.00010032387496508089, 'epoch': 12.85}
{'loss': 0.009, 'learning_rate': 0.00010016193769487181, 'epoch': 12.86}
{'loss': 0.0074, 'learning_rate': 0.0001, 'epoch': 12.88}
{'loss': 0.0079, 'learning_rate': 9.98380623051282e-05, 'epoch': 12.89}
{'loss': 0.0048, 'learning_rate': 9.967612503491914e-05, 'epoch': 12.9}
{'loss': 0.0048, 'learning_rate': 9.95141886140344e-05, 'epoch': 12.91}
{'loss': 0.0092, 'learning_rate': 9.935225346713341e-05, 'epoch': 12.93}
{'loss': 0.0125, 'learning_rate': 9.919032001887215e-05, 'epoch': 12.94}
{'loss': 0.0064, 'learning_rate': 9.902838869390229e-05, 'epoch': 12.95}
{'loss': 0.0087, 'learning_rate': 9.886645991686976e-05, 'epoch': 12.96}
{'loss': 0.0054, 'learning_rate': 9.870453411241399e-05, 'epoch': 12.97}
{'loss': 0.0054, 'learning_rate': 9.854261170516648e-05, 'epoch': 12.99}
{'loss': 0.0071, 'learning_rate': 9.838069311974986e-05, 'epoch': 13.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0045, 'learning_rate': 9.821877878077678e-05, 'epoch': 13.01}
{'loss': 0.0042, 'learning_rate': 9.805686911284868e-05, 'epoch': 13.03}
{'loss': 0.0034, 'learning_rate': 9.789496454055482e-05, 'epoch': 13.04}
{'loss': 0.0012, 'learning_rate': 9.7733065488471e-05, 'epoch': 13.05}
{'loss': 0.0048, 'learning_rate': 9.757117238115871e-05, 'epoch': 13.06}
{'loss': 0.0043, 'learning_rate': 9.740928564316368e-05, 'epoch': 13.07}
{'loss': 0.0047, 'learning_rate': 9.724740569901503e-05, 'epoch': 13.09}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0058, 'learning_rate': 9.708553297322406e-05, 'epoch': 13.1}
{'loss': 0.0062, 'learning_rate': 9.692366789028307e-05, 'epoch': 13.11}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0021, 'learning_rate': 9.676181087466444e-05, 'epoch': 13.12}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.006, 'learning_rate': 9.659996235081926e-05, 'epoch': 13.14}
{'loss': 0.0025, 'learning_rate': 9.643812274317644e-05, 'epoch': 13.15}
{'loss': 0.0145, 'learning_rate': 9.627629247614152e-05, 'epoch': 13.16}
{'loss': 0.0038, 'learning_rate': 9.611447197409543e-05, 'epoch': 13.18}
{'loss': 0.0041, 'learning_rate': 9.595266166139366e-05, 'epoch': 13.19}
{'loss': 0.0019, 'learning_rate': 9.579086196236482e-05, 'epoch': 13.2}
{'loss': 0.003, 'learning_rate': 9.562907330130981e-05, 'epoch': 13.21}
{'loss': 0.0018, 'learning_rate': 9.54672961025005e-05, 'epoch': 13.22}
{'loss': 0.0014, 'learning_rate': 9.530553079017872e-05, 'epoch': 13.24}
{'loss': 0.0094, 'learning_rate': 9.514377778855521e-05, 'epoch': 13.25}
{'loss': 0.0011, 'learning_rate': 9.498203752180826e-05, 'epoch': 13.26}
{'loss': 0.0069, 'learning_rate': 9.482031041408296e-05, 'epoch': 13.28}
{'loss': 0.002, 'learning_rate': 9.465859688948977e-05, 'epoch': 13.29}
{'loss': 0.0032, 'learning_rate': 9.449689737210352e-05, 'epoch': 13.3}
{'loss': 0.0084, 'learning_rate': 9.433521228596237e-05, 'epoch': 13.31}
{'loss': 0.0065, 'learning_rate': 9.417354205506663e-05, 'epoch': 13.32}
{'loss': 0.0093, 'learning_rate': 9.401188710337758e-05, 'epoch': 13.34}
{'loss': 0.0032, 'learning_rate': 9.385024785481654e-05, 'epoch': 13.35}
{'loss': 0.0078, 'learning_rate': 9.368862473326354e-05, 'epoch': 13.36}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0032, 'learning_rate': 9.352701816255643e-05, 'epoch': 13.38}
{'loss': 0.008, 'learning_rate': 9.336542856648956e-05, 'epoch': 13.39}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0016, 'learning_rate': 9.320385636881283e-05, 'epoch': 13.4}
{'loss': 0.0041, 'learning_rate': 9.304230199323049e-05, 'epoch': 13.41}
{'loss': 0.0084, 'learning_rate': 9.288076586340006e-05, 'epoch': 13.43}
{'loss': 0.0023, 'learning_rate': 9.27192484029312e-05, 'epoch': 13.44}
{'loss': 0.002, 'learning_rate': 9.255775003538462e-05, 'epoch': 13.45}
{'loss': 0.0044, 'learning_rate': 9.239627118427096e-05, 'epoch': 13.46}
{'loss': 0.0027, 'learning_rate': 9.223481227304968e-05, 'epoch': 13.47}
{'loss': 0.0112, 'learning_rate': 9.207337372512796e-05, 'epoch': 13.49}
{'loss': 0.0015, 'learning_rate': 9.19119559638596e-05, 'epoch': 13.5}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0022, 'learning_rate': 9.175055941254379e-05, 'epoch': 13.51}
{'loss': 0.0022, 'learning_rate': 9.158918449442423e-05, 'epoch': 13.53}
{'loss': 0.0103, 'learning_rate': 9.142783163268781e-05, 'epoch': 13.54}
{'loss': 0.0028, 'learning_rate': 9.126650125046361e-05, 'epoch': 13.55}
{'loss': 0.0022, 'learning_rate': 9.110519377082172e-05, 'epoch': 13.56}
{'loss': 0.0078, 'learning_rate': 9.094390961677223e-05, 'epoch': 13.57}
{'loss': 0.0065, 'learning_rate': 9.078264921126404e-05, 'epoch': 13.59}
{'loss': 0.0017, 'learning_rate': 9.062141297718371e-05, 'epoch': 13.6}
{'loss': 0.0101, 'learning_rate': 9.046020133735454e-05, 'epoch': 13.61}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0041, 'learning_rate': 9.02990147145352e-05, 'epoch': 13.62}
{'loss': 0.002, 'learning_rate': 9.013785353141886e-05, 'epoch': 13.64}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.003, 'learning_rate': 8.997671821063191e-05, 'epoch': 13.65}
{'loss': 0.003, 'learning_rate': 8.981560917473293e-05, 'epoch': 13.66}
{'loss': 0.0091, 'learning_rate': 8.965452684621164e-05, 'epoch': 13.68}
{'loss': 0.0047, 'learning_rate': 8.949347164748762e-05, 'epoch': 13.69}
{'loss': 0.0033, 'learning_rate': 8.933244400090937e-05, 'epoch': 13.7}
{'loss': 0.0027, 'learning_rate': 8.91714443287531e-05, 'epoch': 13.71}
{'loss': 0.0103, 'learning_rate': 8.901047305322172e-05, 'epoch': 13.72}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0014, 'learning_rate': 8.88495305964436e-05, 'epoch': 13.74}
{'loss': 0.0021, 'learning_rate': 8.868861738047158e-05, 'epoch': 13.75}
{'loss': 0.0029, 'learning_rate': 8.852773382728183e-05, 'epoch': 13.76}
{'loss': 0.0015, 'learning_rate': 8.836688035877267e-05, 'epoch': 13.78}
{'loss': 0.004, 'learning_rate': 8.820605739676363e-05, 'epoch': 13.79}
{'loss': 0.0027, 'learning_rate': 8.804526536299413e-05, 'epoch': 13.8}
{'loss': 0.0032, 'learning_rate': 8.788450467912255e-05, 'epoch': 13.81}
{'loss': 0.0033, 'learning_rate': 8.772377576672502e-05, 'epoch': 13.82}
{'loss': 0.0027, 'learning_rate': 8.756307904729439e-05, 'epoch': 13.84}
{'loss': 0.0067, 'learning_rate': 8.740241494223911e-05, 'epoch': 13.85}
{'loss': 0.0026, 'learning_rate': 8.724178387288201e-05, 'epoch': 13.86}
{'loss': 0.0036, 'learning_rate': 8.70811862604594e-05, 'epoch': 13.88}
{'loss': 0.0021, 'learning_rate': 8.692062252611973e-05, 'epoch': 13.89}
{'loss': 0.0052, 'learning_rate': 8.676009309092272e-05, 'epoch': 13.9}
{'loss': 0.0025, 'learning_rate': 8.659959837583807e-05, 'epoch': 13.91}
{'loss': 0.0024, 'learning_rate': 8.643913880174448e-05, 'epoch': 13.93}
{'loss': 0.0076, 'learning_rate': 8.627871478942851e-05, 'epoch': 13.94}
{'loss': 0.0042, 'learning_rate': 8.611832675958336e-05, 'epoch': 13.95}
{'loss': 0.0074, 'learning_rate': 8.595797513280799e-05, 'epoch': 13.96}
{'loss': 0.0027, 'learning_rate': 8.579766032960582e-05, 'epoch': 13.97}
{'loss': 0.0096, 'learning_rate': 8.563738277038377e-05, 'epoch': 13.99}
{'loss': 0.0014, 'learning_rate': 8.5477142875451e-05, 'epoch': 14.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0006, 'learning_rate': 8.531694106501797e-05, 'epoch': 14.01}
{'loss': 0.0011, 'learning_rate': 8.515677775919527e-05, 'epoch': 14.03}
{'loss': 0.0011, 'learning_rate': 8.499665337799254e-05, 'epoch': 14.04}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0028, 'learning_rate': 8.48365683413172e-05, 'epoch': 14.05}
{'loss': 0.001, 'learning_rate': 8.467652306897369e-05, 'epoch': 14.06}
{'loss': 0.0021, 'learning_rate': 8.451651798066203e-05, 'epoch': 14.07}
{'loss': 0.0011, 'learning_rate': 8.435655349597689e-05, 'epoch': 14.09}
{'loss': 0.0005, 'learning_rate': 8.419663003440657e-05, 'epoch': 14.1}
{'loss': 0.0019, 'learning_rate': 8.403674801533161e-05, 'epoch': 14.11}
{'loss': 0.003, 'learning_rate': 8.387690785802402e-05, 'epoch': 14.12}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0051, 'learning_rate': 8.371710998164594e-05, 'epoch': 14.14}
{'loss': 0.001, 'learning_rate': 8.355735480524874e-05, 'epoch': 14.15}
{'loss': 0.0014, 'learning_rate': 8.339764274777165e-05, 'epoch': 14.16}
{'loss': 0.0011, 'learning_rate': 8.323797422804099e-05, 'epoch': 14.18}
{'loss': 0.0014, 'learning_rate': 8.307834966476884e-05, 'epoch': 14.19}
{'loss': 0.0026, 'learning_rate': 8.291876947655196e-05, 'epoch': 14.2}
{'loss': 0.0011, 'learning_rate': 8.275923408187086e-05, 'epoch': 14.21}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0019, 'learning_rate': 8.259974389908842e-05, 'epoch': 14.22}
{'loss': 0.0051, 'learning_rate': 8.244029934644916e-05, 'epoch': 14.24}
{'loss': 0.0014, 'learning_rate': 8.228090084207774e-05, 'epoch': 14.25}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0016, 'learning_rate': 8.212154880397818e-05, 'epoch': 14.26}
{'loss': 0.0096, 'learning_rate': 8.196224365003267e-05, 'epoch': 14.28}
{'loss': 0.0007, 'learning_rate': 8.180298579800035e-05, 'epoch': 14.29}
{'loss': 0.0024, 'learning_rate': 8.16437756655164e-05, 'epoch': 14.3}
{'loss': 0.0013, 'learning_rate': 8.14846136700908e-05, 'epoch': 14.31}
{'loss': 0.0006, 'learning_rate': 8.132550022910737e-05, 'epoch': 14.32}
{'loss': 0.0056, 'learning_rate': 8.116643575982253e-05, 'epoch': 14.34}
{'loss': 0.0006, 'learning_rate': 8.100742067936431e-05, 'epoch': 14.35}
{'loss': 0.0007, 'learning_rate': 8.084845540473126e-05, 'epoch': 14.36}
{'loss': 0.0016, 'learning_rate': 8.068954035279121e-05, 'epoch': 14.38}
{'loss': 0.0006, 'learning_rate': 8.053067594028044e-05, 'epoch': 14.39}
{'loss': 0.0028, 'learning_rate': 8.037186258380226e-05, 'epoch': 14.4}
{'loss': 0.0085, 'learning_rate': 8.021310069982624e-05, 'epoch': 14.41}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0016, 'learning_rate': 8.005439070468692e-05, 'epoch': 14.43}
{'loss': 0.0013, 'learning_rate': 7.989573301458273e-05, 'epoch': 14.44}
{'loss': 0.0007, 'learning_rate': 7.973712804557501e-05, 'epoch': 14.45}
{'loss': 0.0025, 'learning_rate': 7.957857621358674e-05, 'epoch': 14.46}
{'loss': 0.003, 'learning_rate': 7.942007793440164e-05, 'epoch': 14.47}
{'loss': 0.0012, 'learning_rate': 7.926163362366299e-05, 'epoch': 14.49}
{'loss': 0.0034, 'learning_rate': 7.91032436968725e-05, 'epoch': 14.5}
{'loss': 0.0012, 'learning_rate': 7.894490856938932e-05, 'epoch': 14.51}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0007, 'learning_rate': 7.878662865642881e-05, 'epoch': 14.53}
{'loss': 0.0009, 'learning_rate': 7.862840437306165e-05, 'epoch': 14.54}
{'loss': 0.002, 'learning_rate': 7.847023613421251e-05, 'epoch': 14.55}
{'loss': 0.0043, 'learning_rate': 7.831212435465924e-05, 'epoch': 14.56}
{'loss': 0.0014, 'learning_rate': 7.815406944903147e-05, 'epoch': 14.57}
{'loss': 0.0026, 'learning_rate': 7.799607183180982e-05, 'epoch': 14.59}
{'loss': 0.0042, 'learning_rate': 7.78381319173246e-05, 'epoch': 14.6}
{'loss': 0.0023, 'learning_rate': 7.768025011975481e-05, 'epoch': 14.61}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0065, 'learning_rate': 7.75224268531271e-05, 'epoch': 14.62}
{'loss': 0.007, 'learning_rate': 7.736466253131452e-05, 'epoch': 14.64}
{'loss': 0.0007, 'learning_rate': 7.72069575680357e-05, 'epoch': 14.65}
{'loss': 0.0016, 'learning_rate': 7.704931237685342e-05, 'epoch': 14.66}
{'loss': 0.0064, 'learning_rate': 7.689172737117389e-05, 'epoch': 14.68}
{'loss': 0.0005, 'learning_rate': 7.673420296424541e-05, 'epoch': 14.69}
{'loss': 0.0028, 'learning_rate': 7.657673956915735e-05, 'epoch': 14.7}
{'loss': 0.0008, 'learning_rate': 7.641933759883913e-05, 'epoch': 14.71}
{'loss': 0.0007, 'learning_rate': 7.626199746605903e-05, 'epoch': 14.72}
{'loss': 0.0027, 'learning_rate': 7.610471958342326e-05, 'epoch': 14.74}
{'loss': 0.0013, 'learning_rate': 7.594750436337467e-05, 'epoch': 14.75}
{'loss': 0.0009, 'learning_rate': 7.579035221819187e-05, 'epoch': 14.76}
{'loss': 0.0008, 'learning_rate': 7.563326355998803e-05, 'epoch': 14.78}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0013, 'learning_rate': 7.547623880070993e-05, 'epoch': 14.79}
{'loss': 0.0028, 'learning_rate': 7.531927835213656e-05, 'epoch': 14.8}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0034, 'learning_rate': 7.516238262587851e-05, 'epoch': 14.81}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0015, 'learning_rate': 7.500555203337647e-05, 'epoch': 14.82}
{'loss': 0.0016, 'learning_rate': 7.48487869859004e-05, 'epoch': 14.84}
{'loss': 0.0028, 'learning_rate': 7.469208789454838e-05, 'epoch': 14.85}
{'loss': 0.0025, 'learning_rate': 7.453545517024547e-05, 'epoch': 14.86}
{'loss': 0.0005, 'learning_rate': 7.437888922374276e-05, 'epoch': 14.88}
{'loss': 0.0019, 'learning_rate': 7.422239046561619e-05, 'epoch': 14.89}
{'loss': 0.0026, 'learning_rate': 7.40659593062655e-05, 'epoch': 14.9}
{'loss': 0.0024, 'learning_rate': 7.390959615591316e-05, 'epoch': 14.91}
{'loss': 0.0008, 'learning_rate': 7.37533014246033e-05, 'epoch': 14.93}
{'loss': 0.0006, 'learning_rate': 7.35970755222007e-05, 'epoch': 14.94}
{'loss': 0.0053, 'learning_rate': 7.344091885838948e-05, 'epoch': 14.95}
{'loss': 0.0034, 'learning_rate': 7.328483184267235e-05, 'epoch': 14.96}
{'loss': 0.0057, 'learning_rate': 7.312881488436927e-05, 'epoch': 14.97}
{'loss': 0.0006, 'learning_rate': 7.297286839261658e-05, 'epoch': 14.99}
{'loss': 0.0065, 'learning_rate': 7.281699277636572e-05, 'epoch': 15.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0029, 'learning_rate': 7.266118844438235e-05, 'epoch': 15.01}
{'loss': 0.0016, 'learning_rate': 7.250545580524515e-05, 'epoch': 15.03}
{'loss': 0.0005, 'learning_rate': 7.234979526734482e-05, 'epoch': 15.04}
{'loss': 0.0011, 'learning_rate': 7.2194207238883e-05, 'epoch': 15.05}
{'loss': 0.0009, 'learning_rate': 7.20386921278711e-05, 'epoch': 15.06}
{'loss': 0.0034, 'learning_rate': 7.188325034212943e-05, 'epoch': 15.07}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.003, 'learning_rate': 7.172788228928591e-05, 'epoch': 15.09}
{'loss': 0.0007, 'learning_rate': 7.157258837677514e-05, 'epoch': 15.1}
{'loss': 0.001, 'learning_rate': 7.141736901183736e-05, 'epoch': 15.11}
{'loss': 0.0004, 'learning_rate': 7.126222460151719e-05, 'epoch': 15.12}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0043, 'learning_rate': 7.110715555266281e-05, 'epoch': 15.14}
{'loss': 0.0005, 'learning_rate': 7.095216227192467e-05, 'epoch': 15.15}
{'loss': 0.0006, 'learning_rate': 7.079724516575466e-05, 'epoch': 15.16}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0005, 'learning_rate': 7.064240464040473e-05, 'epoch': 15.18}
{'loss': 0.0023, 'learning_rate': 7.048764110192618e-05, 'epoch': 15.19}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0004, 'learning_rate': 7.033295495616834e-05, 'epoch': 15.2}
{'loss': 0.0029, 'learning_rate': 7.017834660877756e-05, 'epoch': 15.21}
{'loss': 0.0006, 'learning_rate': 7.002381646519625e-05, 'epoch': 15.22}
{'loss': 0.0005, 'learning_rate': 6.986936493066165e-05, 'epoch': 15.24}
{'loss': 0.0006, 'learning_rate': 6.971499241020495e-05, 'epoch': 15.25}
{'loss': 0.0004, 'learning_rate': 6.956069930865004e-05, 'epoch': 15.26}
{'loss': 0.0008, 'learning_rate': 6.940648603061263e-05, 'epoch': 15.28}
{'loss': 0.0017, 'learning_rate': 6.925235298049906e-05, 'epoch': 15.29}
{'loss': 0.0095, 'learning_rate': 6.909830056250527e-05, 'epoch': 15.3}
{'loss': 0.0007, 'learning_rate': 6.894432918061579e-05, 'epoch': 15.31}
{'loss': 0.0004, 'learning_rate': 6.879043923860257e-05, 'epoch': 15.32}
{'loss': 0.0019, 'learning_rate': 6.863663114002411e-05, 'epoch': 15.34}
{'loss': 0.0008, 'learning_rate': 6.848290528822416e-05, 'epoch': 15.35}
{'loss': 0.0049, 'learning_rate': 6.832926208633086e-05, 'epoch': 15.36}
{'loss': 0.0004, 'learning_rate': 6.817570193725564e-05, 'epoch': 15.38}
{'loss': 0.0007, 'learning_rate': 6.802222524369202e-05, 'epoch': 15.39}
{'loss': 0.0011, 'learning_rate': 6.786883240811479e-05, 'epoch': 15.4}
{'loss': 0.0006, 'learning_rate': 6.771552383277875e-05, 'epoch': 15.41}
{'loss': 0.0055, 'learning_rate': 6.756229991971779e-05, 'epoch': 15.43}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0004, 'learning_rate': 6.740916107074372e-05, 'epoch': 15.44}
{'loss': 0.0005, 'learning_rate': 6.725610768744534e-05, 'epoch': 15.45}
{'loss': 0.0011, 'learning_rate': 6.710314017118734e-05, 'epoch': 15.46}
{'loss': 0.0008, 'learning_rate': 6.695025892310914e-05, 'epoch': 15.47}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0023, 'learning_rate': 6.679746434412404e-05, 'epoch': 15.49}
{'loss': 0.0006, 'learning_rate': 6.664475683491796e-05, 'epoch': 15.5}
{'loss': 0.0008, 'learning_rate': 6.649213679594859e-05, 'epoch': 15.51}
{'loss': 0.0006, 'learning_rate': 6.633960462744416e-05, 'epoch': 15.53}
{'loss': 0.0004, 'learning_rate': 6.618716072940248e-05, 'epoch': 15.54}
{'loss': 0.0007, 'learning_rate': 6.603480550158995e-05, 'epoch': 15.55}
{'loss': 0.0021, 'learning_rate': 6.588253934354039e-05, 'epoch': 15.56}
{'loss': 0.0005, 'learning_rate': 6.5730362654554e-05, 'epoch': 15.57}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0005, 'learning_rate': 6.557827583369647e-05, 'epoch': 15.59}
{'loss': 0.0005, 'learning_rate': 6.542627927979771e-05, 'epoch': 15.6}
{'loss': 0.0012, 'learning_rate': 6.527437339145098e-05, 'epoch': 15.61}
{'loss': 0.0008, 'learning_rate': 6.512255856701177e-05, 'epoch': 15.62}
{'loss': 0.0064, 'learning_rate': 6.497083520459674e-05, 'epoch': 15.64}
{'loss': 0.0004, 'learning_rate': 6.481920370208274e-05, 'epoch': 15.65}
{'loss': 0.0006, 'learning_rate': 6.466766445710568e-05, 'epoch': 15.66}
{'loss': 0.0005, 'learning_rate': 6.451621786705962e-05, 'epoch': 15.68}
{'loss': 0.0008, 'learning_rate': 6.43648643290955e-05, 'epoch': 15.69}
{'loss': 0.0003, 'learning_rate': 6.42136042401204e-05, 'epoch': 15.7}
{'loss': 0.0003, 'learning_rate': 6.406243799679626e-05, 'epoch': 15.71}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0005, 'learning_rate': 6.39113659955389e-05, 'epoch': 15.72}
{'loss': 0.0005, 'learning_rate': 6.376038863251705e-05, 'epoch': 15.74}
{'loss': 0.0005, 'learning_rate': 6.360950630365126e-05, 'epoch': 15.75}
{'loss': 0.0004, 'learning_rate': 6.345871940461283e-05, 'epoch': 15.76}
{'loss': 0.0003, 'learning_rate': 6.330802833082279e-05, 'epoch': 15.78}
{'loss': 0.0017, 'learning_rate': 6.315743347745097e-05, 'epoch': 15.79}
{'loss': 0.0058, 'learning_rate': 6.300693523941482e-05, 'epoch': 15.8}
{'loss': 0.0003, 'learning_rate': 6.285653401137837e-05, 'epoch': 15.81}
{'loss': 0.0003, 'learning_rate': 6.270623018775135e-05, 'epoch': 15.82}
{'loss': 0.0024, 'learning_rate': 6.255602416268799e-05, 'epoch': 15.84}
{'loss': 0.0005, 'learning_rate': 6.24059163300861e-05, 'epoch': 15.85}
{'loss': 0.0003, 'learning_rate': 6.225590708358596e-05, 'epoch': 15.86}
{'loss': 0.0004, 'learning_rate': 6.210599681656933e-05, 'epoch': 15.88}
{'loss': 0.001, 'learning_rate': 6.195618592215843e-05, 'epoch': 15.89}
{'loss': 0.0003, 'learning_rate': 6.180647479321485e-05, 'epoch': 15.9}
{'loss': 0.0003, 'learning_rate': 6.165686382233855e-05, 'epoch': 15.91}
{'loss': 0.0003, 'learning_rate': 6.15073534018669e-05, 'epoch': 15.93}
{'loss': 0.0006, 'learning_rate': 6.135794392387353e-05, 'epoch': 15.94}
{'loss': 0.0004, 'learning_rate': 6.120863578016735e-05, 'epoch': 15.95}
{'loss': 0.0003, 'learning_rate': 6.105942936229161e-05, 'epoch': 15.96}
{'loss': 0.0023, 'learning_rate': 6.091032506152274e-05, 'epoch': 15.97}
{'loss': 0.0005, 'learning_rate': 6.076132326886934e-05, 'epoch': 15.99}
{'loss': 0.0003, 'learning_rate': 6.061242437507131e-05, 'epoch': 16.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0003, 'learning_rate': 6.046362877059857e-05, 'epoch': 16.01}
{'loss': 0.0004, 'learning_rate': 6.031493684565029e-05, 'epoch': 16.02}
{'loss': 0.0004, 'learning_rate': 6.016634899015369e-05, 'epoch': 16.04}
{'loss': 0.0003, 'learning_rate': 6.00178655937631e-05, 'epoch': 16.05}
{'loss': 0.0002, 'learning_rate': 5.986948704585895e-05, 'epoch': 16.06}
{'loss': 0.0004, 'learning_rate': 5.972121373554664e-05, 'epoch': 16.07}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0003, 'learning_rate': 5.957304605165567e-05, 'epoch': 16.09}
{'loss': 0.0003, 'learning_rate': 5.942498438273849e-05, 'epoch': 16.1}
{'loss': 0.0004, 'learning_rate': 5.9277029117069604e-05, 'epoch': 16.11}
{'loss': 0.0003, 'learning_rate': 5.9129180642644414e-05, 'epoch': 16.12}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0003, 'learning_rate': 5.8981439347178304e-05, 'epoch': 16.14}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 5.883380561810563e-05, 'epoch': 16.15}
{'loss': 0.0003, 'learning_rate': 5.8686279842578615e-05, 'epoch': 16.16}
{'loss': 0.0002, 'learning_rate': 5.8538862407466425e-05, 'epoch': 16.18}
{'loss': 0.0005, 'learning_rate': 5.839155369935407e-05, 'epoch': 16.19}
{'loss': 0.0003, 'learning_rate': 5.82443541045415e-05, 'epoch': 16.2}
{'loss': 0.0002, 'learning_rate': 5.809726400904242e-05, 'epoch': 16.21}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 5.795028379858355e-05, 'epoch': 16.23}
{'loss': 0.0003, 'learning_rate': 5.780341385860333e-05, 'epoch': 16.24}
{'loss': 0.0002, 'learning_rate': 5.765665457425102e-05, 'epoch': 16.25}
{'loss': 0.0003, 'learning_rate': 5.751000633038572e-05, 'epoch': 16.26}
{'loss': 0.0002, 'learning_rate': 5.736346951157544e-05, 'epoch': 16.27}
{'loss': 0.0003, 'learning_rate': 5.7217044502095806e-05, 'epoch': 16.29}
{'loss': 0.0003, 'learning_rate': 5.707073168592942e-05, 'epoch': 16.3}
{'loss': 0.0002, 'learning_rate': 5.6924531446764504e-05, 'epoch': 16.31}
{'loss': 0.0003, 'learning_rate': 5.677844416799424e-05, 'epoch': 16.32}
{'loss': 0.0002, 'learning_rate': 5.6632470232715426e-05, 'epoch': 16.34}
{'loss': 0.0004, 'learning_rate': 5.648661002372768e-05, 'epoch': 16.35}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0004, 'learning_rate': 5.6340863923532396e-05, 'epoch': 16.36}
{'loss': 0.0003, 'learning_rate': 5.6195232314331766e-05, 'epoch': 16.38}
{'loss': 0.0005, 'learning_rate': 5.60497155780277e-05, 'epoch': 16.39}
{'loss': 0.0012, 'learning_rate': 5.590431409622081e-05, 'epoch': 16.4}
{'loss': 0.0002, 'learning_rate': 5.575902825020962e-05, 'epoch': 16.41}
{'loss': 0.0018, 'learning_rate': 5.56138584209893e-05, 'epoch': 16.43}
{'loss': 0.0005, 'learning_rate': 5.5468804989250786e-05, 'epoch': 16.44}
{'loss': 0.0002, 'learning_rate': 5.532386833537977e-05, 'epoch': 16.45}
{'loss': 0.0019, 'learning_rate': 5.517904883945577e-05, 'epoch': 16.46}
{'loss': 0.0002, 'learning_rate': 5.503434688125104e-05, 'epoch': 16.48}
{'loss': 0.0003, 'learning_rate': 5.488976284022953e-05, 'epoch': 16.49}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 5.474529709554612e-05, 'epoch': 16.5}
{'loss': 0.0002, 'learning_rate': 5.4600950026045326e-05, 'epoch': 16.51}
{'loss': 0.0003, 'learning_rate': 5.445672201026054e-05, 'epoch': 16.52}
{'loss': 0.0007, 'learning_rate': 5.431261342641286e-05, 'epoch': 16.54}
{'loss': 0.0003, 'learning_rate': 5.416862465241033e-05, 'epoch': 16.55}
{'loss': 0.0004, 'learning_rate': 5.402475606584669e-05, 'epoch': 16.56}
{'loss': 0.0007, 'learning_rate': 5.388100804400049e-05, 'epoch': 16.57}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0005, 'learning_rate': 5.373738096383423e-05, 'epoch': 16.59}
{'loss': 0.0002, 'learning_rate': 5.3593875201993174e-05, 'epoch': 16.6}
{'loss': 0.0002, 'learning_rate': 5.3450491134804414e-05, 'epoch': 16.61}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0004, 'learning_rate': 5.3307229138275936e-05, 'epoch': 16.62}
{'loss': 0.0003, 'learning_rate': 5.31640895880957e-05, 'epoch': 16.64}
{'loss': 0.0002, 'learning_rate': 5.302107285963045e-05, 'epoch': 16.65}
{'loss': 0.0002, 'learning_rate': 5.287817932792485e-05, 'epoch': 16.66}
{'loss': 0.0006, 'learning_rate': 5.273540936770058e-05, 'epoch': 16.68}
{'loss': 0.0002, 'learning_rate': 5.259276335335521e-05, 'epoch': 16.69}
{'loss': 0.0002, 'learning_rate': 5.245024165896126e-05, 'epoch': 16.7}
{'loss': 0.0002, 'learning_rate': 5.230784465826524e-05, 'epoch': 16.71}
{'loss': 0.0002, 'learning_rate': 5.2165572724686754e-05, 'epoch': 16.73}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0008, 'learning_rate': 5.202342623131731e-05, 'epoch': 16.74}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0005, 'learning_rate': 5.1881405550919493e-05, 'epoch': 16.75}
{'loss': 0.0003, 'learning_rate': 5.1739511055926047e-05, 'epoch': 16.76}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 5.1597743118438726e-05, 'epoch': 16.77}
{'loss': 0.0002, 'learning_rate': 5.1456102110227375e-05, 'epoch': 16.79}
{'loss': 0.0004, 'learning_rate': 5.1314588402729044e-05, 'epoch': 16.8}
{'loss': 0.0002, 'learning_rate': 5.117320236704697e-05, 'epoch': 16.81}
{'loss': 0.0002, 'learning_rate': 5.103194437394952e-05, 'epoch': 16.82}
{'loss': 0.0002, 'learning_rate': 5.089081479386928e-05, 'epoch': 16.84}
{'loss': 0.0002, 'learning_rate': 5.074981399690218e-05, 'epoch': 16.85}
{'loss': 0.0002, 'learning_rate': 5.0608942352806364e-05, 'epoch': 16.86}
{'loss': 0.0002, 'learning_rate': 5.0468200231001286e-05, 'epoch': 16.88}
{'loss': 0.0002, 'learning_rate': 5.0327588000566695e-05, 'epoch': 16.89}
{'loss': 0.0002, 'learning_rate': 5.018710603024187e-05, 'epoch': 16.9}
{'loss': 0.0002, 'learning_rate': 5.004675468842436e-05, 'epoch': 16.91}
{'loss': 0.0002, 'learning_rate': 4.9906534343169144e-05, 'epoch': 16.93}
{'loss': 0.0002, 'learning_rate': 4.976644536218783e-05, 'epoch': 16.94}
{'loss': 0.0017, 'learning_rate': 4.962648811284738e-05, 'epoch': 16.95}
{'loss': 0.0007, 'learning_rate': 4.948666296216938e-05, 'epoch': 16.96}
{'loss': 0.0002, 'learning_rate': 4.934697027682894e-05, 'epoch': 16.98}
{'loss': 0.0002, 'learning_rate': 4.920741042315392e-05, 'epoch': 16.99}
{'loss': 0.0002, 'learning_rate': 4.9067983767123736e-05, 'epoch': 17.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 4.89286906743685e-05, 'epoch': 17.01}
{'loss': 0.0002, 'learning_rate': 4.8789531510168163e-05, 'epoch': 17.02}
{'loss': 0.0002, 'learning_rate': 4.865050663945139e-05, 'epoch': 17.04}
{'loss': 0.0002, 'learning_rate': 4.851161642679466e-05, 'epoch': 17.05}
{'loss': 0.0002, 'learning_rate': 4.837286123642141e-05, 'epoch': 17.06}
{'loss': 0.0002, 'learning_rate': 4.8234241432200965e-05, 'epoch': 17.07}
{'loss': 0.0002, 'learning_rate': 4.809575737764759e-05, 'epoch': 17.09}
{'loss': 0.0002, 'learning_rate': 4.795740943591955e-05, 'epoch': 17.1}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 4.7819197969818175e-05, 'epoch': 17.11}
{'loss': 0.0002, 'learning_rate': 4.768112334178699e-05, 'epoch': 17.12}
{'loss': 0.0002, 'learning_rate': 4.754318591391057e-05, 'epoch': 17.14}
{'loss': 0.0002, 'learning_rate': 4.74053860479137e-05, 'epoch': 17.15}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 4.726772410516055e-05, 'epoch': 17.16}
{'loss': 0.0002, 'learning_rate': 4.7130200446653475e-05, 'epoch': 17.18}
{'loss': 0.0002, 'learning_rate': 4.699281543303222e-05, 'epoch': 17.19}
{'loss': 0.0002, 'learning_rate': 4.6855569424572955e-05, 'epoch': 17.2}
{'loss': 0.0002, 'learning_rate': 4.67184627811874e-05, 'epoch': 17.21}
{'loss': 0.0002, 'learning_rate': 4.65814958624217e-05, 'epoch': 17.23}
{'loss': 0.0002, 'learning_rate': 4.644466902745561e-05, 'epoch': 17.24}
{'loss': 0.0001, 'learning_rate': 4.630798263510162e-05, 'epoch': 17.25}
{'loss': 0.0002, 'learning_rate': 4.617143704380381e-05, 'epoch': 17.26}
{'loss': 0.0002, 'learning_rate': 4.6035032611637094e-05, 'epoch': 17.27}
{'loss': 0.0001, 'learning_rate': 4.5898769696306155e-05, 'epoch': 17.29}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 4.5762648655144666e-05, 'epoch': 17.3}
{'loss': 0.0002, 'learning_rate': 4.562666984511416e-05, 'epoch': 17.31}
{'loss': 0.0002, 'learning_rate': 4.549083362280317e-05, 'epoch': 17.32}
{'loss': 0.0001, 'learning_rate': 4.535514034442644e-05, 'epoch': 17.34}
{'loss': 0.0002, 'learning_rate': 4.5219590365823714e-05, 'epoch': 17.35}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 4.508418404245903e-05, 'epoch': 17.36}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 4.494892172941965e-05, 'epoch': 17.38}
{'loss': 0.0002, 'learning_rate': 4.481380378141527e-05, 'epoch': 17.39}
{'loss': 0.0002, 'learning_rate': 4.467883055277695e-05, 'epoch': 17.4}
{'loss': 0.0002, 'learning_rate': 4.454400239745619e-05, 'epoch': 17.41}
{'loss': 0.0002, 'learning_rate': 4.440931966902418e-05, 'epoch': 17.43}
{'loss': 0.0002, 'learning_rate': 4.427478272067066e-05, 'epoch': 17.44}
{'loss': 0.0002, 'learning_rate': 4.414039190520308e-05, 'epoch': 17.45}
{'loss': 0.0002, 'learning_rate': 4.400614757504564e-05, 'epoch': 17.46}
{'loss': 0.0001, 'learning_rate': 4.387205008223854e-05, 'epoch': 17.48}
{'loss': 0.0011, 'learning_rate': 4.373809977843676e-05, 'epoch': 17.49}
{'loss': 0.0002, 'learning_rate': 4.360429701490934e-05, 'epoch': 17.5}
{'loss': 0.0002, 'learning_rate': 4.34706421425385e-05, 'epoch': 17.51}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 4.333713551181852e-05, 'epoch': 17.52}
{'loss': 0.0002, 'learning_rate': 4.320377747285497e-05, 'epoch': 17.54}
{'loss': 0.0002, 'learning_rate': 4.307056837536373e-05, 'epoch': 17.55}
{'loss': 0.0002, 'learning_rate': 4.2937508568670194e-05, 'epoch': 17.56}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 4.2804598401708175e-05, 'epoch': 17.57}
{'loss': 0.0002, 'learning_rate': 4.2671838223019036e-05, 'epoch': 17.59}
{'loss': 0.0002, 'learning_rate': 4.253922838075095e-05, 'epoch': 17.6}
{'loss': 0.0002, 'learning_rate': 4.240676922265774e-05, 'epoch': 17.61}
{'loss': 0.0002, 'learning_rate': 4.227446109609809e-05, 'epoch': 17.62}
{'loss': 0.0001, 'learning_rate': 4.21423043480346e-05, 'epoch': 17.64}
{'loss': 0.0002, 'learning_rate': 4.2010299325033034e-05, 'epoch': 17.65}
{'loss': 0.0002, 'learning_rate': 4.18784463732611e-05, 'epoch': 17.66}
{'loss': 0.0001, 'learning_rate': 4.17467458384878e-05, 'epoch': 17.68}
{'loss': 0.0001, 'learning_rate': 4.161519806608247e-05, 'epoch': 17.69}
{'loss': 0.0002, 'learning_rate': 4.1483803401013796e-05, 'epoch': 17.7}
{'loss': 0.0002, 'learning_rate': 4.1352562187848954e-05, 'epoch': 17.71}
{'loss': 0.0002, 'learning_rate': 4.12214747707527e-05, 'epoch': 17.73}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 4.109054149348655e-05, 'epoch': 17.74}
{'loss': 0.0002, 'learning_rate': 4.0959762699407766e-05, 'epoch': 17.75}
{'loss': 0.0002, 'learning_rate': 4.0829138731468416e-05, 'epoch': 17.76}
{'loss': 0.0001, 'learning_rate': 4.0698669932214727e-05, 'epoch': 17.77}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 4.0568356643785856e-05, 'epoch': 17.79}
{'loss': 0.0001, 'learning_rate': 4.043819920791322e-05, 'epoch': 17.8}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 4.030819796591949e-05, 'epoch': 17.81}
{'loss': 0.0002, 'learning_rate': 4.0178353258717804e-05, 'epoch': 17.82}
{'loss': 0.0002, 'learning_rate': 4.0048665426810794e-05, 'epoch': 17.84}
{'loss': 0.0002, 'learning_rate': 3.991913481028965e-05, 'epoch': 17.85}
{'loss': 0.0002, 'learning_rate': 3.978976174883329e-05, 'epoch': 17.86}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 3.966054658170754e-05, 'epoch': 17.88}
{'loss': 0.0002, 'learning_rate': 3.953148964776408e-05, 'epoch': 17.89}
{'loss': 0.0001, 'learning_rate': 3.940259128543967e-05, 'epoch': 17.9}
{'loss': 0.0001, 'learning_rate': 3.9273851832755214e-05, 'epoch': 17.91}
{'loss': 0.0024, 'learning_rate': 3.9145271627314986e-05, 'epoch': 17.93}
{'loss': 0.0001, 'learning_rate': 3.9016851006305545e-05, 'epoch': 17.94}
{'loss': 0.0007, 'learning_rate': 3.8888590306494974e-05, 'epoch': 17.95}
{'loss': 0.0002, 'learning_rate': 3.8760489864232066e-05, 'epoch': 17.96}
{'loss': 0.0001, 'learning_rate': 3.8632550015445256e-05, 'epoch': 17.98}
{'loss': 0.0002, 'learning_rate': 3.8504771095641904e-05, 'epoch': 17.99}
{'loss': 0.0002, 'learning_rate': 3.8377153439907266e-05, 'epoch': 18.0}
{'loss': 0.0001, 'learning_rate': 3.824969738290386e-05, 'epoch': 18.01}
{'loss': 0.0001, 'learning_rate': 3.81224032588703e-05, 'epoch': 18.02}
{'loss': 0.0002, 'learning_rate': 3.799527140162055e-05, 'epoch': 18.04}
{'loss': 0.0001, 'learning_rate': 3.786830214454315e-05, 'epoch': 18.05}
{'loss': 0.0013, 'learning_rate': 3.774149582060012e-05, 'epoch': 18.06}
{'loss': 0.0001, 'learning_rate': 3.7614852762326305e-05, 'epoch': 18.07}
{'loss': 0.0001, 'learning_rate': 3.7488373301828296e-05, 'epoch': 18.09}
{'loss': 0.0002, 'learning_rate': 3.736205777078381e-05, 'epoch': 18.1}
{'loss': 0.0001, 'learning_rate': 3.7235906500440574e-05, 'epoch': 18.11}
{'loss': 0.0002, 'learning_rate': 3.710991982161555e-05, 'epoch': 18.12}
{'loss': 0.0001, 'learning_rate': 3.698409806469417e-05, 'epoch': 18.14}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.6858441559629306e-05, 'epoch': 18.15}
{'loss': 0.0001, 'learning_rate': 3.673295063594049e-05, 'epoch': 18.16}
{'loss': 0.0001, 'learning_rate': 3.6607625622713e-05, 'epoch': 18.18}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 3.648246684859716e-05, 'epoch': 18.19}
{'loss': 0.0001, 'learning_rate': 3.63574746418072e-05, 'epoch': 18.2}
{'loss': 0.0001, 'learning_rate': 3.6232649330120605e-05, 'epoch': 18.21}
{'loss': 0.0002, 'learning_rate': 3.610799124087725e-05, 'epoch': 18.23}
{'loss': 0.0001, 'learning_rate': 3.5983500700978425e-05, 'epoch': 18.24}
{'loss': 0.0002, 'learning_rate': 3.585917803688603e-05, 'epoch': 18.25}
{'loss': 0.0002, 'learning_rate': 3.573502357462176e-05, 'epoch': 18.26}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.5611037639766265e-05, 'epoch': 18.27}
{'loss': 0.0001, 'learning_rate': 3.5487220557458176e-05, 'epoch': 18.29}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.5363572652393326e-05, 'epoch': 18.3}
{'loss': 0.0001, 'learning_rate': 3.5240094248824e-05, 'epoch': 18.31}
{'loss': 0.0001, 'learning_rate': 3.511678567055786e-05, 'epoch': 18.32}
{'loss': 0.0001, 'learning_rate': 3.4993647240957304e-05, 'epoch': 18.34}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.487067928293848e-05, 'epoch': 18.35}
{'loss': 0.0002, 'learning_rate': 3.4747882118970565e-05, 'epoch': 18.36}
{'loss': 0.0002, 'learning_rate': 3.4625256071074773e-05, 'epoch': 18.38}
{'loss': 0.0002, 'learning_rate': 3.4502801460823607e-05, 'epoch': 18.39}
{'loss': 0.0004, 'learning_rate': 3.4380518609340076e-05, 'epoch': 18.4}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 3.425840783729664e-05, 'epoch': 18.41}
{'loss': 0.0002, 'learning_rate': 3.4136469464914575e-05, 'epoch': 18.43}
{'loss': 0.0002, 'learning_rate': 3.4014703811963025e-05, 'epoch': 18.44}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.389311119775828e-05, 'epoch': 18.45}
{'loss': 0.0001, 'learning_rate': 3.377169194116275e-05, 'epoch': 18.46}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.3650446360584275e-05, 'epoch': 18.48}
{'loss': 0.0002, 'learning_rate': 3.35293747739753e-05, 'epoch': 18.49}
{'loss': 0.0005, 'learning_rate': 3.340847749883191e-05, 'epoch': 18.5}
{'loss': 0.0001, 'learning_rate': 3.3287754852193144e-05, 'epoch': 18.51}
{'loss': 0.0001, 'learning_rate': 3.316720715064e-05, 'epoch': 18.52}
{'loss': 0.0001, 'learning_rate': 3.304683471029485e-05, 'epoch': 18.54}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.292663784682036e-05, 'epoch': 18.55}
{'loss': 0.0001, 'learning_rate': 3.280661687541876e-05, 'epoch': 18.56}
{'loss': 0.0001, 'learning_rate': 3.268677211083109e-05, 'epoch': 18.57}
{'loss': 0.0001, 'learning_rate': 3.256710386733629e-05, 'epoch': 18.59}
{'loss': 0.0001, 'learning_rate': 3.2447612458750365e-05, 'epoch': 18.6}
{'loss': 0.0001, 'learning_rate': 3.232829819842555e-05, 'epoch': 18.61}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 3.2209161399249674e-05, 'epoch': 18.62}
{'loss': 0.0001, 'learning_rate': 3.209020237364505e-05, 'epoch': 18.64}
{'loss': 0.0001, 'learning_rate': 3.197142143356787e-05, 'epoch': 18.65}
{'loss': 0.0001, 'learning_rate': 3.185281889050725e-05, 'epoch': 18.66}
{'loss': 0.0001, 'learning_rate': 3.173439505548462e-05, 'epoch': 18.68}
{'loss': 0.0001, 'learning_rate': 3.161615023905265e-05, 'epoch': 18.69}
{'loss': 0.0001, 'learning_rate': 3.149808475129452e-05, 'epoch': 18.7}
{'loss': 0.0001, 'learning_rate': 3.138019890182331e-05, 'epoch': 18.71}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.126249299978086e-05, 'epoch': 18.73}
{'loss': 0.0001, 'learning_rate': 3.1144967353837196e-05, 'epoch': 18.74}
{'loss': 0.0002, 'learning_rate': 3.102762227218957e-05, 'epoch': 18.75}
{'loss': 0.0001, 'learning_rate': 3.091045806256187e-05, 'epoch': 18.76}
{'loss': 0.0001, 'learning_rate': 3.079347503220351e-05, 'epoch': 18.77}
{'loss': 0.0001, 'learning_rate': 3.067667348788885e-05, 'epoch': 18.79}
{'loss': 0.0001, 'learning_rate': 3.056005373591637e-05, 'epoch': 18.8}
{'loss': 0.0001, 'learning_rate': 3.044361608210775e-05, 'epoch': 18.81}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.032736083180716e-05, 'epoch': 18.82}
{'loss': 0.0002, 'learning_rate': 3.02112882898804e-05, 'epoch': 18.84}
{'loss': 0.0001, 'learning_rate': 3.0095398760714267e-05, 'epoch': 18.85}
{'loss': 0.0001, 'learning_rate': 2.9979692548215477e-05, 'epoch': 18.86}
{'loss': 0.0001, 'learning_rate': 2.9864169955810084e-05, 'epoch': 18.88}
{'loss': 0.0001, 'learning_rate': 2.9748831286442657e-05, 'epoch': 18.89}
{'loss': 0.0002, 'learning_rate': 2.9633676842575387e-05, 'epoch': 18.9}
{'loss': 0.0001, 'learning_rate': 2.951870692618739e-05, 'epoch': 18.91}
{'loss': 0.0001, 'learning_rate': 2.940392183877382e-05, 'epoch': 18.93}
{'loss': 0.0001, 'learning_rate': 2.9289321881345254e-05, 'epoch': 18.94}
{'loss': 0.0002, 'learning_rate': 2.9174907354426696e-05, 'epoch': 18.95}
{'loss': 0.0001, 'learning_rate': 2.9060678558056874e-05, 'epoch': 18.96}
{'loss': 0.0001, 'learning_rate': 2.8946635791787545e-05, 'epoch': 18.98}
{'loss': 0.0001, 'learning_rate': 2.8832779354682536e-05, 'epoch': 18.99}
{'loss': 0.0002, 'learning_rate': 2.8719109545317103e-05, 'epoch': 19.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.8605626661776996e-05, 'epoch': 19.01}
{'loss': 0.0001, 'learning_rate': 2.8492331001657945e-05, 'epoch': 19.02}
{'loss': 0.0001, 'learning_rate': 2.8379222862064568e-05, 'epoch': 19.04}
{'loss': 0.0001, 'learning_rate': 2.8266302539609745e-05, 'epoch': 19.05}
{'loss': 0.0001, 'learning_rate': 2.8153570330413925e-05, 'epoch': 19.06}
{'loss': 0.0001, 'learning_rate': 2.804102653010414e-05, 'epoch': 19.07}
{'loss': 0.0001, 'learning_rate': 2.7928671433813392e-05, 'epoch': 19.09}
{'loss': 0.0001, 'learning_rate': 2.7816505336179798e-05, 'epoch': 19.1}
{'loss': 0.0001, 'learning_rate': 2.770452853134593e-05, 'epoch': 19.11}
{'loss': 0.0001, 'learning_rate': 2.759274131295787e-05, 'epoch': 19.12}
{'loss': 0.0001, 'learning_rate': 2.7481143974164547e-05, 'epoch': 19.14}
{'loss': 0.0001, 'learning_rate': 2.736973680761702e-05, 'epoch': 19.15}
{'loss': 0.0001, 'learning_rate': 2.7258520105467567e-05, 'epoch': 19.16}
{'loss': 0.0001, 'learning_rate': 2.7147494159369036e-05, 'epoch': 19.18}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.7036659260473974e-05, 'epoch': 19.19}
{'loss': 0.0001, 'learning_rate': 2.6926015699434072e-05, 'epoch': 19.2}
{'loss': 0.0001, 'learning_rate': 2.681556376639912e-05, 'epoch': 19.21}
{'loss': 0.0001, 'learning_rate': 2.6705303751016408e-05, 'epoch': 19.23}
{'loss': 0.0001, 'learning_rate': 2.659523594243004e-05, 'epoch': 19.24}
{'loss': 0.0002, 'learning_rate': 2.6485360629279987e-05, 'epoch': 19.25}
{'loss': 0.0001, 'learning_rate': 2.6375678099701428e-05, 'epoch': 19.26}
{'loss': 0.0001, 'learning_rate': 2.6266188641323996e-05, 'epoch': 19.27}
{'loss': 0.0002, 'learning_rate': 2.6156892541271084e-05, 'epoch': 19.29}
{'loss': 0.0001, 'learning_rate': 2.6047790086158952e-05, 'epoch': 19.3}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0002, 'learning_rate': 2.593888156209603e-05, 'epoch': 19.31}
{'loss': 0.0006, 'learning_rate': 2.5830167254682257e-05, 'epoch': 19.32}
{'loss': 0.0002, 'learning_rate': 2.572164744900827e-05, 'epoch': 19.34}
{'loss': 0.0001, 'learning_rate': 2.5613322429654574e-05, 'epoch': 19.35}
{'loss': 0.0001, 'learning_rate': 2.5505192480690865e-05, 'epoch': 19.36}
{'loss': 0.0001, 'learning_rate': 2.5397257885675397e-05, 'epoch': 19.38}
{'loss': 0.0001, 'learning_rate': 2.5289518927654023e-05, 'epoch': 19.39}
{'loss': 0.0001, 'learning_rate': 2.5181975889159615e-05, 'epoch': 19.4}
{'loss': 0.0001, 'learning_rate': 2.5074629052211217e-05, 'epoch': 19.41}
{'loss': 0.0001, 'learning_rate': 2.496747869831345e-05, 'epoch': 19.43}
{'loss': 0.0001, 'learning_rate': 2.48605251084556e-05, 'epoch': 19.44}
{'loss': 0.0001, 'learning_rate': 2.475376856311097e-05, 'epoch': 19.45}
{'loss': 0.0001, 'learning_rate': 2.464720934223619e-05, 'epoch': 19.46}
{'loss': 0.0001, 'learning_rate': 2.4540847725270378e-05, 'epoch': 19.48}
{'loss': 0.0001, 'learning_rate': 2.4434683991134477e-05, 'epoch': 19.49}
{'loss': 0.0001, 'learning_rate': 2.432871841823047e-05, 'epoch': 19.5}
{'loss': 0.0001, 'learning_rate': 2.4222951284440776e-05, 'epoch': 19.51}
{'loss': 0.0001, 'learning_rate': 2.411738286712735e-05, 'epoch': 19.52}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.401201344313102e-05, 'epoch': 19.54}
{'loss': 0.0001, 'learning_rate': 2.3906843288770886e-05, 'epoch': 19.55}
{'loss': 0.0001, 'learning_rate': 2.3801872679843385e-05, 'epoch': 19.56}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.3697101891621697e-05, 'epoch': 19.57}
{'loss': 0.0001, 'learning_rate': 2.3592531198854974e-05, 'epoch': 19.59}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.3488160875767717e-05, 'epoch': 19.6}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.338399119605892e-05, 'epoch': 19.61}
{'loss': 0.0001, 'learning_rate': 2.3280022432901383e-05, 'epoch': 19.62}
{'loss': 0.0001, 'learning_rate': 2.3176254858941127e-05, 'epoch': 19.64}
{'loss': 0.0001, 'learning_rate': 2.307268874629649e-05, 'epoch': 19.65}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.2969324366557522e-05, 'epoch': 19.66}
{'loss': 0.0001, 'learning_rate': 2.2866161990785228e-05, 'epoch': 19.68}
{'loss': 0.0001, 'learning_rate': 2.2763201889510987e-05, 'epoch': 19.69}
{'loss': 0.0001, 'learning_rate': 2.266044433273562e-05, 'epoch': 19.7}
{'loss': 0.0001, 'learning_rate': 2.2557889589928815e-05, 'epoch': 19.71}
{'loss': 0.0001, 'learning_rate': 2.245553793002849e-05, 'epoch': 19.73}
{'loss': 0.0001, 'learning_rate': 2.2353389621439903e-05, 'epoch': 19.74}
{'loss': 0.0001, 'learning_rate': 2.2251444932035094e-05, 'epoch': 19.75}
{'loss': 0.0001, 'learning_rate': 2.2149704129152084e-05, 'epoch': 19.76}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.204816747959434e-05, 'epoch': 19.77}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.194683524962986e-05, 'epoch': 19.79}
{'loss': 0.0001, 'learning_rate': 2.184570770499056e-05, 'epoch': 19.8}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.174478511087171e-05, 'epoch': 19.81}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.1644067731931007e-05, 'epoch': 19.82}
{'loss': 0.0001, 'learning_rate': 2.1543555832288054e-05, 'epoch': 19.84}
{'loss': 0.0001, 'learning_rate': 2.1443249675523536e-05, 'epoch': 19.85}
{'loss': 0.0001, 'learning_rate': 2.134314952467873e-05, 'epoch': 19.86}
{'loss': 0.0001, 'learning_rate': 2.1243255642254578e-05, 'epoch': 19.88}
{'loss': 0.0001, 'learning_rate': 2.1143568290211114e-05, 'epoch': 19.89}
{'loss': 0.0001, 'learning_rate': 2.1044087729966856e-05, 'epoch': 19.9}
{'loss': 0.0001, 'learning_rate': 2.0944814222397944e-05, 'epoch': 19.91}
{'loss': 0.0001, 'learning_rate': 2.0845748027837586e-05, 'epoch': 19.93}
{'loss': 0.0001, 'learning_rate': 2.074688940607529e-05, 'epoch': 19.94}
{'loss': 0.0001, 'learning_rate': 2.0648238616356332e-05, 'epoch': 19.95}
{'loss': 0.0001, 'learning_rate': 2.0549795917380864e-05, 'epoch': 19.96}
{'loss': 0.0001, 'learning_rate': 2.045156156730338e-05, 'epoch': 19.98}
{'loss': 0.0001, 'learning_rate': 2.035353582373205e-05, 'epoch': 19.99}
{'loss': 0.0002, 'learning_rate': 2.025571894372794e-05, 'epoch': 20.0}
{'loss': 0.0001, 'learning_rate': 2.0158111183804407e-05, 'epoch': 20.01}
{'loss': 0.0001, 'learning_rate': 2.0060712799926408e-05, 'epoch': 20.02}
{'loss': 0.0001, 'learning_rate': 1.9963524047509897e-05, 'epoch': 20.04}
{'loss': 0.0001, 'learning_rate': 1.9866545181421013e-05, 'epoch': 20.05}
{'loss': 0.0001, 'learning_rate': 1.976977645597552e-05, 'epoch': 20.06}
{'loss': 0.0001, 'learning_rate': 1.967321812493813e-05, 'epoch': 20.07}
{'loss': 0.0001, 'learning_rate': 1.9576870441521833e-05, 'epoch': 20.09}
{'loss': 0.0001, 'learning_rate': 1.9480733658387175e-05, 'epoch': 20.1}
{'loss': 0.0001, 'learning_rate': 1.9384808027641666e-05, 'epoch': 20.11}
{'loss': 0.0001, 'learning_rate': 1.9289093800839066e-05, 'epoch': 20.12}
{'loss': 0.0001, 'learning_rate': 1.9193591228978814e-05, 'epoch': 20.14}
{'loss': 0.0001, 'learning_rate': 1.9098300562505266e-05, 'epoch': 20.15}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.9003222051307047e-05, 'epoch': 20.16}
{'loss': 0.0001, 'learning_rate': 1.8908355944716517e-05, 'epoch': 20.18}
{'loss': 0.0001, 'learning_rate': 1.8813702491508955e-05, 'epoch': 20.19}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.871926193990202e-05, 'epoch': 20.2}
{'loss': 0.0001, 'learning_rate': 1.8625034537555018e-05, 'epoch': 20.21}
{'loss': 0.0001, 'learning_rate': 1.8531020531568378e-05, 'epoch': 20.23}
{'loss': 0.0001, 'learning_rate': 1.8437220168482837e-05, 'epoch': 20.24}
{'loss': 0.0001, 'learning_rate': 1.8343633694278895e-05, 'epoch': 20.25}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.825026135437622e-05, 'epoch': 20.26}
{'loss': 0.0001, 'learning_rate': 1.8157103393632868e-05, 'epoch': 20.27}
{'loss': 0.0001, 'learning_rate': 1.8064160056344716e-05, 'epoch': 20.29}
{'loss': 0.0001, 'learning_rate': 1.7971431586244815e-05, 'epoch': 20.3}
{'loss': 0.0001, 'learning_rate': 1.7878918226502816e-05, 'epoch': 20.31}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.7786620219724204e-05, 'epoch': 20.32}
{'loss': 0.0001, 'learning_rate': 1.7694537807949708e-05, 'epoch': 20.34}
{'loss': 0.0001, 'learning_rate': 1.7602671232654754e-05, 'epoch': 20.35}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.751102073474873e-05, 'epoch': 20.36}
{'loss': 0.0001, 'learning_rate': 1.741958655457436e-05, 'epoch': 20.38}
{'loss': 0.0001, 'learning_rate': 1.7328368931907113e-05, 'epoch': 20.39}
{'loss': 0.0001, 'learning_rate': 1.723736810595461e-05, 'epoch': 20.4}
{'loss': 0.0001, 'learning_rate': 1.7146584315355885e-05, 'epoch': 20.41}
{'loss': 0.0001, 'learning_rate': 1.7056017798180824e-05, 'epoch': 20.43}
{'loss': 0.0001, 'learning_rate': 1.69656687919296e-05, 'epoch': 20.44}
{'loss': 0.0001, 'learning_rate': 1.6875537533531948e-05, 'epoch': 20.45}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.6785624259346557e-05, 'epoch': 20.46}
{'loss': 0.0001, 'learning_rate': 1.6695929205160487e-05, 'epoch': 20.48}
{'loss': 0.0001, 'learning_rate': 1.660645260618864e-05, 'epoch': 20.49}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.65171946970729e-05, 'epoch': 20.5}
{'loss': 0.0001, 'learning_rate': 1.6428155711881722e-05, 'epoch': 20.51}
{'loss': 0.0001, 'learning_rate': 1.6339335884109518e-05, 'epoch': 20.52}
{'loss': 0.0001, 'learning_rate': 1.6250735446675912e-05, 'epoch': 20.54}
{'loss': 0.0001, 'learning_rate': 1.6162354631925204e-05, 'epoch': 20.55}
{'loss': 0.0001, 'learning_rate': 1.607419367162577e-05, 'epoch': 20.56}
{'loss': 0.0001, 'learning_rate': 1.598625279696948e-05, 'epoch': 20.57}
{'loss': 0.0001, 'learning_rate': 1.589853223857103e-05, 'epoch': 20.59}
{'loss': 0.0001, 'learning_rate': 1.5811032226467305e-05, 'epoch': 20.6}
{'loss': 0.0001, 'learning_rate': 1.5723752990116947e-05, 'epoch': 20.61}
{'loss': 0.0001, 'learning_rate': 1.563669475839956e-05, 'epoch': 20.62}
{'loss': 0.0001, 'learning_rate': 1.5549857759615194e-05, 'epoch': 20.64}
{'loss': 0.0001, 'learning_rate': 1.5463242221483743e-05, 'epoch': 20.65}
{'loss': 0.0001, 'learning_rate': 1.5376848371144402e-05, 'epoch': 20.66}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.529067643515495e-05, 'epoch': 20.68}
{'loss': 0.0001, 'learning_rate': 1.5204726639491218e-05, 'epoch': 20.69}
{'loss': 0.0001, 'learning_rate': 1.5118999209546559e-05, 'epoch': 20.7}
{'loss': 0.0003, 'learning_rate': 1.5033494370131163e-05, 'epoch': 20.71}
{'loss': 0.0001, 'learning_rate': 1.4948212345471491e-05, 'epoch': 20.73}
{'loss': 0.0001, 'learning_rate': 1.4863153359209692e-05, 'epoch': 20.74}
{'loss': 0.0001, 'learning_rate': 1.4778317634403083e-05, 'epoch': 20.75}
{'loss': 0.0001, 'learning_rate': 1.4693705393523449e-05, 'epoch': 20.76}
{'loss': 0.0001, 'learning_rate': 1.460931685845649e-05, 'epoch': 20.77}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.4525152250501361e-05, 'epoch': 20.79}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.444121179036989e-05, 'epoch': 20.8}
{'loss': 0.0001, 'learning_rate': 1.4357495698186186e-05, 'epoch': 20.81}
{'loss': 0.0001, 'learning_rate': 1.427400419348588e-05, 'epoch': 20.82}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.4190737495215745e-05, 'epoch': 20.84}
{'loss': 0.0001, 'learning_rate': 1.4107695821733025e-05, 'epoch': 20.85}
{'loss': 0.0001, 'learning_rate': 1.402487939080479e-05, 'epoch': 20.86}
{'loss': 0.0001, 'learning_rate': 1.3942288419607475e-05, 'epoch': 20.88}
{'loss': 0.0001, 'learning_rate': 1.3859923124726281e-05, 'epoch': 20.89}
{'loss': 0.0001, 'learning_rate': 1.3777783722154603e-05, 'epoch': 20.9}
{'loss': 0.0001, 'learning_rate': 1.369587042729341e-05, 'epoch': 20.91}
{'loss': 0.0001, 'learning_rate': 1.3614183454950824e-05, 'epoch': 20.93}
{'loss': 0.0001, 'learning_rate': 1.3532723019341375e-05, 'epoch': 20.94}
{'loss': 0.0001, 'learning_rate': 1.3451489334085554e-05, 'epoch': 20.95}
{'loss': 0.0001, 'learning_rate': 1.3370482612209223e-05, 'epoch': 20.96}
{'loss': 0.0001, 'learning_rate': 1.3289703066143111e-05, 'epoch': 20.98}
{'loss': 0.0001, 'learning_rate': 1.3209150907722123e-05, 'epoch': 20.99}
{'loss': 0.0001, 'learning_rate': 1.3128826348184887e-05, 'epoch': 21.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.3048729598173248e-05, 'epoch': 21.01}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.2968860867731569e-05, 'epoch': 21.02}
{'loss': 0.0001, 'learning_rate': 1.2889220366306276e-05, 'epoch': 21.04}
{'loss': 0.0001, 'learning_rate': 1.2809808302745297e-05, 'epoch': 21.05}
{'loss': 0.0001, 'learning_rate': 1.2730624885297537e-05, 'epoch': 21.06}
{'loss': 0.0001, 'learning_rate': 1.2651670321612263e-05, 'epoch': 21.07}
{'loss': 0.0001, 'learning_rate': 1.2572944818738586e-05, 'epoch': 21.09}
{'loss': 0.0001, 'learning_rate': 1.2494448583125018e-05, 'epoch': 21.1}
{'loss': 0.0001, 'learning_rate': 1.2416181820618744e-05, 'epoch': 21.11}
{'loss': 0.0001, 'learning_rate': 1.233814473646524e-05, 'epoch': 21.12}
{'loss': 0.0001, 'learning_rate': 1.2260337535307631e-05, 'epoch': 21.14}
{'loss': 0.0001, 'learning_rate': 1.218276042118629e-05, 'epoch': 21.15}
{'loss': 0.0001, 'learning_rate': 1.2105413597538107e-05, 'epoch': 21.16}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.202829726719611e-05, 'epoch': 21.18}
{'loss': 0.0001, 'learning_rate': 1.195141163238892e-05, 'epoch': 21.19}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.1874756894740135e-05, 'epoch': 21.2}
{'loss': 0.0001, 'learning_rate': 1.1798333255267857e-05, 'epoch': 21.21}
{'loss': 0.0001, 'learning_rate': 1.172214091438416e-05, 'epoch': 21.23}
{'loss': 0.0001, 'learning_rate': 1.1646180071894607e-05, 'epoch': 21.24}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.1570450926997655e-05, 'epoch': 21.25}
{'loss': 0.0001, 'learning_rate': 1.1494953678284103e-05, 'epoch': 21.26}
{'loss': 0.0001, 'learning_rate': 1.141968852373676e-05, 'epoch': 21.27}
{'loss': 0.0001, 'learning_rate': 1.1344655660729675e-05, 'epoch': 21.29}
{'loss': 0.0001, 'learning_rate': 1.1269855286027797e-05, 'epoch': 21.3}
{'loss': 0.0001, 'learning_rate': 1.1195287595786352e-05, 'epoch': 21.31}
{'loss': 0.0001, 'learning_rate': 1.1120952785550476e-05, 'epoch': 21.32}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.1046851050254502e-05, 'epoch': 21.34}
{'loss': 0.0001, 'learning_rate': 1.0972982584221592e-05, 'epoch': 21.35}
{'loss': 0.0001, 'learning_rate': 1.0899347581163221e-05, 'epoch': 21.36}
{'loss': 0.0001, 'learning_rate': 1.0825946234178574e-05, 'epoch': 21.38}
{'loss': 0.0001, 'learning_rate': 1.075277873575412e-05, 'epoch': 21.39}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.067984527776309e-05, 'epoch': 21.4}
{'loss': 0.0001, 'learning_rate': 1.0607146051465012e-05, 'epoch': 21.41}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.0534681247505106e-05, 'epoch': 21.43}
{'loss': 0.0001, 'learning_rate': 1.0462451055913847e-05, 'epoch': 21.44}
{'loss': 0.0001, 'learning_rate': 1.0390455666106547e-05, 'epoch': 21.45}
{'loss': 0.0001, 'learning_rate': 1.0318695266882693e-05, 'epoch': 21.46}
{'loss': 0.0001, 'learning_rate': 1.024717004642557e-05, 'epoch': 21.48}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.0175880192301713e-05, 'epoch': 21.49}
{'loss': 0.0001, 'learning_rate': 1.010482589146048e-05, 'epoch': 21.5}
{'loss': 0.0001, 'learning_rate': 1.0034007330233486e-05, 'epoch': 21.51}
{'loss': 0.0001, 'learning_rate': 9.963424694334122e-06, 'epoch': 21.52}
{'loss': 0.0001, 'learning_rate': 9.893078168857173e-06, 'epoch': 21.54}
{'loss': 0.0001, 'learning_rate': 9.822967938278171e-06, 'epoch': 21.55}
{'loss': 0.0001, 'learning_rate': 9.753094186453026e-06, 'epoch': 21.56}
{'loss': 0.0001, 'learning_rate': 9.683457096617488e-06, 'epoch': 21.57}
{'loss': 0.0001, 'learning_rate': 9.614056851386744e-06, 'epoch': 21.59}
{'loss': 0.0001, 'learning_rate': 9.544893632754814e-06, 'epoch': 21.6}
{'loss': 0.0001, 'learning_rate': 9.475967622094205e-06, 'epoch': 21.61}
{'loss': 0.0001, 'learning_rate': 9.407279000155312e-06, 'epoch': 21.62}
{'loss': 0.0001, 'learning_rate': 9.338827947066076e-06, 'epoch': 21.64}
{'loss': 0.0001, 'learning_rate': 9.270614642331376e-06, 'epoch': 21.65}
{'loss': 0.0001, 'learning_rate': 9.202639264832668e-06, 'epoch': 21.66}
{'loss': 0.0001, 'learning_rate': 9.134901992827427e-06, 'epoch': 21.68}
{'loss': 0.0001, 'learning_rate': 9.067403003948782e-06, 'epoch': 21.69}
{'loss': 0.0001, 'learning_rate': 9.000142475204964e-06, 'epoch': 21.7}
{'loss': 0.0001, 'learning_rate': 8.933120582978827e-06, 'epoch': 21.71}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 8.866337503027522e-06, 'epoch': 21.73}
{'loss': 0.0001, 'learning_rate': 8.79979341048187e-06, 'epoch': 21.74}
{'loss': 0.0001, 'learning_rate': 8.733488479845997e-06, 'epoch': 21.75}
{'loss': 0.0001, 'learning_rate': 8.667422884996823e-06, 'epoch': 21.76}
{'loss': 0.0001, 'learning_rate': 8.60159679918372e-06, 'epoch': 21.77}
{'loss': 0.0001, 'learning_rate': 8.536010395027904e-06, 'epoch': 21.79}
{'loss': 0.0001, 'learning_rate': 8.470663844522052e-06, 'epoch': 21.8}
{'loss': 0.0001, 'learning_rate': 8.405557319029912e-06, 'epoch': 21.81}
{'loss': 0.0001, 'learning_rate': 8.340690989285726e-06, 'epoch': 21.82}
{'loss': 0.0001, 'learning_rate': 8.276065025393908e-06, 'epoch': 21.84}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 8.21167959682848e-06, 'epoch': 21.85}
{'loss': 0.0001, 'learning_rate': 8.14753487243276e-06, 'epoch': 21.86}
{'loss': 0.0001, 'learning_rate': 8.083631020418791e-06, 'epoch': 21.88}
{'loss': 0.0001, 'learning_rate': 8.019968208366958e-06, 'epoch': 21.89}
{'loss': 0.0001, 'learning_rate': 7.956546603225601e-06, 'epoch': 21.9}
{'loss': 0.0001, 'learning_rate': 7.893366371310462e-06, 'epoch': 21.91}
{'loss': 0.0001, 'learning_rate': 7.830427678304353e-06, 'epoch': 21.93}
{'loss': 0.0001, 'learning_rate': 7.767730689256614e-06, 'epoch': 21.94}
{'loss': 0.0001, 'learning_rate': 7.705275568582848e-06, 'epoch': 21.95}
{'loss': 0.0001, 'learning_rate': 7.6430624800643e-06, 'epoch': 21.96}
{'loss': 0.0001, 'learning_rate': 7.581091586847522e-06, 'epoch': 21.98}
{'loss': 0.0004, 'learning_rate': 7.519363051443995e-06, 'epoch': 21.99}
{'loss': 0.0001, 'learning_rate': 7.457877035729588e-06, 'epoch': 22.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 7.3966337009442e-06, 'epoch': 22.01}
{'loss': 0.0001, 'learning_rate': 7.335633207691361e-06, 'epoch': 22.02}
{'loss': 0.0001, 'learning_rate': 7.274875715937746e-06, 'epoch': 22.04}
{'loss': 0.0001, 'learning_rate': 7.21436138501278e-06, 'epoch': 22.05}
{'loss': 0.0001, 'learning_rate': 7.154090373608235e-06, 'epoch': 22.06}
{'loss': 0.0001, 'learning_rate': 7.094062839777837e-06, 'epoch': 22.07}
{'loss': 0.0001, 'learning_rate': 7.03427894093679e-06, 'epoch': 22.09}
{'loss': 0.0001, 'learning_rate': 6.974738833861383e-06, 'epoch': 22.1}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 6.915442674688632e-06, 'epoch': 22.11}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 6.856390618915775e-06, 'epoch': 22.12}
{'loss': 0.0001, 'learning_rate': 6.7975828213999725e-06, 'epoch': 22.14}
{'loss': 0.0001, 'learning_rate': 6.739019436357774e-06, 'epoch': 22.15}
{'loss': 0.0001, 'learning_rate': 6.680700617364877e-06, 'epoch': 22.16}
{'loss': 0.0003, 'learning_rate': 6.622626517355557e-06, 'epoch': 22.18}
{'loss': 0.0001, 'learning_rate': 6.564797288622371e-06, 'epoch': 22.19}
{'loss': 0.0001, 'learning_rate': 6.507213082815744e-06, 'epoch': 22.2}
{'loss': 0.0001, 'learning_rate': 6.449874050943549e-06, 'epoch': 22.21}
{'loss': 0.0001, 'learning_rate': 6.392780343370686e-06, 'epoch': 22.23}
{'loss': 0.0001, 'learning_rate': 6.335932109818754e-06, 'epoch': 22.24}
{'loss': 0.0001, 'learning_rate': 6.2793294993656494e-06, 'epoch': 22.25}
{'loss': 0.0001, 'learning_rate': 6.222972660445081e-06, 'epoch': 22.26}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 6.166861740846297e-06, 'epoch': 22.27}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 6.110996887713661e-06, 'epoch': 22.29}
{'loss': 0.0001, 'learning_rate': 6.055378247546218e-06, 'epoch': 22.3}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 6.000005966197387e-06, 'epoch': 22.31}
{'loss': 0.0001, 'learning_rate': 5.9448801888744795e-06, 'epoch': 22.32}
{'loss': 0.0001, 'learning_rate': 5.8900010601384835e-06, 'epoch': 22.34}
{'loss': 0.0001, 'learning_rate': 5.835368723903456e-06, 'epoch': 22.35}
{'loss': 0.0001, 'learning_rate': 5.780983323436373e-06, 'epoch': 22.36}
{'loss': 0.0001, 'learning_rate': 5.726845001356573e-06, 'epoch': 22.38}
{'loss': 0.0001, 'learning_rate': 5.672953899635525e-06, 'epoch': 22.39}
{'loss': 0.0001, 'learning_rate': 5.6193101595963585e-06, 'epoch': 22.4}
{'loss': 0.0001, 'learning_rate': 5.565913921913513e-06, 'epoch': 22.41}
{'loss': 0.0001, 'learning_rate': 5.512765326612379e-06, 'epoch': 22.43}
{'loss': 0.0001, 'learning_rate': 5.45986451306899e-06, 'epoch': 22.44}
{'loss': 0.0001, 'learning_rate': 5.407211620009544e-06, 'epoch': 22.45}
{'loss': 0.0001, 'learning_rate': 5.354806785510113e-06, 'epoch': 22.46}
{'loss': 0.0001, 'learning_rate': 5.30265014699628e-06, 'epoch': 22.48}
{'loss': 0.0001, 'learning_rate': 5.250741841242734e-06, 'epoch': 22.49}
{'loss': 0.0001, 'learning_rate': 5.199082004372957e-06, 'epoch': 22.5}
{'loss': 0.0001, 'learning_rate': 5.147670771858848e-06, 'epoch': 22.51}
{'loss': 0.0001, 'learning_rate': 5.096508278520384e-06, 'epoch': 22.52}
{'loss': 0.0001, 'learning_rate': 5.045594658525232e-06, 'epoch': 22.54}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 4.994930045388413e-06, 'epoch': 22.55}
{'loss': 0.0001, 'learning_rate': 4.944514571971981e-06, 'epoch': 22.56}
{'loss': 0.0001, 'learning_rate': 4.8943483704846475e-06, 'epoch': 22.57}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 4.8444315724814115e-06, 'epoch': 22.59}
{'loss': 0.0001, 'learning_rate': 4.794764308863242e-06, 'epoch': 22.6}
{'loss': 0.0001, 'learning_rate': 4.745346709876786e-06, 'epoch': 22.61}
{'loss': 0.0001, 'learning_rate': 4.6961789051139124e-06, 'epoch': 22.62}
{'loss': 0.0001, 'learning_rate': 4.647261023511451e-06, 'epoch': 22.64}
{'loss': 0.0001, 'learning_rate': 4.5985931933508754e-06, 'epoch': 22.65}
{'loss': 0.0001, 'learning_rate': 4.550175542257862e-06, 'epoch': 22.66}
{'loss': 0.0001, 'learning_rate': 4.502008197202068e-06, 'epoch': 22.68}
{'loss': 0.0001, 'learning_rate': 4.454091284496731e-06, 'epoch': 22.69}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 4.406424929798403e-06, 'epoch': 22.7}
{'loss': 0.0001, 'learning_rate': 4.3590092581065055e-06, 'epoch': 22.71}
{'loss': 0.0001, 'learning_rate': 4.311844393763109e-06, 'epoch': 22.73}
{'loss': 0.0001, 'learning_rate': 4.26493046045261e-06, 'epoch': 22.74}
{'loss': 0.0001, 'learning_rate': 4.2182675812012965e-06, 'epoch': 22.75}
{'loss': 0.0001, 'learning_rate': 4.17185587837714e-06, 'epoch': 22.76}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 4.125695473689406e-06, 'epoch': 22.77}
{'loss': 0.0001, 'learning_rate': 4.0797864881883975e-06, 'epoch': 22.79}
{'loss': 0.0001, 'learning_rate': 4.034129042265066e-06, 'epoch': 22.8}
{'loss': 0.0001, 'learning_rate': 3.988723255650728e-06, 'epoch': 22.81}
{'loss': 0.0001, 'learning_rate': 3.943569247416801e-06, 'epoch': 22.82}
{'loss': 0.0001, 'learning_rate': 3.898667135974376e-06, 'epoch': 22.84}
{'loss': 0.0001, 'learning_rate': 3.854017039074009e-06, 'epoch': 22.85}
{'loss': 0.0001, 'learning_rate': 3.8096190738053818e-06, 'epoch': 22.86}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.7654733565969826e-06, 'epoch': 22.88}
{'loss': 0.0001, 'learning_rate': 3.7215800032158075e-06, 'epoch': 22.89}
{'loss': 0.0001, 'learning_rate': 3.6779391287670494e-06, 'epoch': 22.9}
{'loss': 0.0001, 'learning_rate': 3.6345508476938297e-06, 'epoch': 22.91}
{'loss': 0.0001, 'learning_rate': 3.591415273776855e-06, 'epoch': 22.93}
{'loss': 0.0001, 'learning_rate': 3.548532520134129e-06, 'epoch': 22.94}
{'loss': 0.0001, 'learning_rate': 3.5059026992206647e-06, 'epoch': 22.95}
{'loss': 0.0001, 'learning_rate': 3.4635259228282256e-06, 'epoch': 22.96}
{'loss': 0.0001, 'learning_rate': 3.421402302084953e-06, 'epoch': 22.98}
{'loss': 0.0001, 'learning_rate': 3.379531947455128e-06, 'epoch': 22.99}
{'loss': 0.0001, 'learning_rate': 3.3379149687388867e-06, 'epoch': 23.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.2965514750718963e-06, 'epoch': 23.01}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.2554415749250888e-06, 'epoch': 23.02}
{'loss': 0.0001, 'learning_rate': 3.214585376104384e-06, 'epoch': 23.04}
{'loss': 0.0001, 'learning_rate': 3.1739829857504234e-06, 'epoch': 23.05}
{'loss': 0.0001, 'learning_rate': 3.1336345103382346e-06, 'epoch': 23.06}
{'loss': 0.0001, 'learning_rate': 3.093540055676958e-06, 'epoch': 23.07}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 3.053699726909676e-06, 'epoch': 23.09}
{'loss': 0.0001, 'learning_rate': 3.014113628512982e-06, 'epoch': 23.1}
{'loss': 0.0001, 'learning_rate': 2.9747818642967827e-06, 'epoch': 23.11}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.9357045374040825e-06, 'epoch': 23.12}
{'loss': 0.0001, 'learning_rate': 2.8968817503105983e-06, 'epoch': 23.14}
{'loss': 0.0001, 'learning_rate': 2.8583136048245697e-06, 'epoch': 23.15}
{'loss': 0.0001, 'learning_rate': 2.820000202086459e-06, 'epoch': 23.16}
{'loss': 0.0001, 'learning_rate': 2.781941642568686e-06, 'epoch': 23.18}
{'loss': 0.0001, 'learning_rate': 2.7441380260754048e-06, 'epoch': 23.19}
{'loss': 0.0001, 'learning_rate': 2.706589451742181e-06, 'epoch': 23.2}
{'loss': 0.0001, 'learning_rate': 2.6692960180357716e-06, 'epoch': 23.21}
{'loss': 0.0001, 'learning_rate': 2.632257822753881e-06, 'epoch': 23.23}
{'loss': 0.0001, 'learning_rate': 2.5954749630248353e-06, 'epoch': 23.24}
{'loss': 0.0001, 'learning_rate': 2.5589475353073988e-06, 'epoch': 23.25}
{'loss': 0.0001, 'learning_rate': 2.522675635390492e-06, 'epoch': 23.26}
{'loss': 0.0001, 'learning_rate': 2.486659358392951e-06, 'epoch': 23.27}
{'loss': 0.0001, 'learning_rate': 2.450898798763268e-06, 'epoch': 23.29}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.415394050279318e-06, 'epoch': 23.3}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.380145206048201e-06, 'epoch': 23.31}
{'loss': 0.0001, 'learning_rate': 2.3451523585058754e-06, 'epoch': 23.32}
{'loss': 0.0001, 'learning_rate': 2.310415599417004e-06, 'epoch': 23.34}
{'loss': 0.0001, 'learning_rate': 2.2759350198746976e-06, 'epoch': 23.35}
{'loss': 0.0001, 'learning_rate': 2.24171071030026e-06, 'epoch': 23.36}
{'loss': 0.0001, 'learning_rate': 2.2077427604429433e-06, 'epoch': 23.38}
{'loss': 0.0001, 'learning_rate': 2.1740312593797273e-06, 'epoch': 23.39}
{'loss': 0.0001, 'learning_rate': 2.1405762955151176e-06, 'epoch': 23.4}
{'loss': 0.0001, 'learning_rate': 2.107377956580847e-06, 'epoch': 23.41}
{'loss': 0.0001, 'learning_rate': 2.074436329635687e-06, 'epoch': 23.43}
{'loss': 0.0001, 'learning_rate': 2.041751501065203e-06, 'epoch': 23.44}
{'loss': 0.0001, 'learning_rate': 2.009323556581566e-06, 'epoch': 23.45}
{'loss': 0.0001, 'learning_rate': 1.977152581223274e-06, 'epoch': 23.46}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.945238659354953e-06, 'epoch': 23.48}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.9135818746671586e-06, 'epoch': 23.49}
{'loss': 0.0001, 'learning_rate': 1.882182310176095e-06, 'epoch': 23.5}
{'loss': 0.0001, 'learning_rate': 1.8510400482234847e-06, 'epoch': 23.51}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.8201551704762453e-06, 'epoch': 23.52}
{'loss': 0.0001, 'learning_rate': 1.7895277579264014e-06, 'epoch': 23.54}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.7591578908907724e-06, 'epoch': 23.55}
{'loss': 0.0001, 'learning_rate': 1.729045649010752e-06, 'epoch': 23.56}
{'loss': 0.0001, 'learning_rate': 1.6991911112522407e-06, 'epoch': 23.57}
{'loss': 0.0001, 'learning_rate': 1.6695943559052462e-06, 'epoch': 23.59}
{'loss': 0.0001, 'learning_rate': 1.6402554605838172e-06, 'epoch': 23.6}
{'loss': 0.0001, 'learning_rate': 1.6111745022257874e-06, 'epoch': 23.61}
{'loss': 0.0001, 'learning_rate': 1.5823515570925763e-06, 'epoch': 23.62}
{'loss': 0.0001, 'learning_rate': 1.553786700769011e-06, 'epoch': 23.64}
{'loss': 0.0001, 'learning_rate': 1.5254800081630826e-06, 'epoch': 23.65}
{'loss': 0.0001, 'learning_rate': 1.4974315535058015e-06, 'epoch': 23.66}
{'loss': 0.0001, 'learning_rate': 1.4696414103509636e-06, 'epoch': 23.68}
{'loss': 0.0001, 'learning_rate': 1.4421096515749855e-06, 'epoch': 23.69}
{'loss': 0.0001, 'learning_rate': 1.4148363493766802e-06, 'epoch': 23.7}
{'loss': 0.0001, 'learning_rate': 1.3878215752771262e-06, 'epoch': 23.71}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.361065400119399e-06, 'epoch': 23.73}
{'loss': 0.0001, 'learning_rate': 1.3345678940684613e-06, 'epoch': 23.74}
{'loss': 0.0001, 'learning_rate': 1.30832912661093e-06, 'epoch': 23.75}
{'loss': 0.0001, 'learning_rate': 1.2823491665549193e-06, 'epoch': 23.76}
{'loss': 0.0001, 'learning_rate': 1.2566280820298426e-06, 'epoch': 23.77}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.231165940486234e-06, 'epoch': 23.79}
{'loss': 0.0001, 'learning_rate': 1.2059628086956044e-06, 'epoch': 23.8}
{'loss': 0.0001, 'learning_rate': 1.1810187527502182e-06, 'epoch': 23.81}
{'loss': 0.0001, 'learning_rate': 1.1563338380629618e-06, 'epoch': 23.82}
{'loss': 0.0001, 'learning_rate': 1.1319081293671541e-06, 'epoch': 23.84}
{'loss': 0.0001, 'learning_rate': 1.1077416907163574e-06, 'epoch': 23.85}
{'loss': 0.0001, 'learning_rate': 1.0838345854842446e-06, 'epoch': 23.86}
{'loss': 0.0001, 'learning_rate': 1.0601868763643996e-06, 'epoch': 23.88}
{'loss': 0.0001, 'learning_rate': 1.0367986253701944e-06, 'epoch': 23.89}
{'loss': 0.0003, 'learning_rate': 1.0136698938346011e-06, 'epoch': 23.9}
{'loss': 0.0001, 'learning_rate': 9.90800742410003e-07, 'epoch': 23.91}
{'loss': 0.0001, 'learning_rate': 9.68191231068083e-07, 'epoch': 23.93}
{'loss': 0.0001, 'learning_rate': 9.458414190996689e-07, 'epoch': 23.94}
{'loss': 0.0001, 'learning_rate': 9.237513651145225e-07, 'epoch': 23.95}
{'loss': 0.0001, 'learning_rate': 9.019211270412275e-07, 'epoch': 23.96}
{'loss': 0.0001, 'learning_rate': 8.803507621270579e-07, 'epoch': 23.98}
{'loss': 0.0001, 'learning_rate': 8.590403269377655e-07, 'epoch': 23.99}
{'loss': 0.0001, 'learning_rate': 8.379898773574924e-07, 'epoch': 24.0}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 8.171994685885698e-07, 'epoch': 24.01}
{'loss': 0.0001, 'learning_rate': 7.966691551514527e-07, 'epoch': 24.02}
{'loss': 0.0001, 'learning_rate': 7.763989908844749e-07, 'epoch': 24.04}
{'loss': 0.0001, 'learning_rate': 7.563890289437825e-07, 'epoch': 24.05}
{'loss': 0.0001, 'learning_rate': 7.366393218031564e-07, 'epoch': 24.06}
{'loss': 0.0001, 'learning_rate': 7.171499212539123e-07, 'epoch': 24.07}
{'loss': 0.0001, 'learning_rate': 6.979208784047453e-07, 'epoch': 24.09}
{'loss': 0.0001, 'learning_rate': 6.78952243681541e-07, 'epoch': 24.1}
{'loss': 0.0001, 'learning_rate': 6.602440668273758e-07, 'epoch': 24.11}
{'loss': 0.0001, 'learning_rate': 6.41796396902239e-07, 'epoch': 24.12}
{'loss': 0.0001, 'learning_rate': 6.236092822829887e-07, 'epoch': 24.14}
{'loss': 0.0001, 'learning_rate': 6.056827706632185e-07, 'epoch': 24.15}
{'loss': 0.0001, 'learning_rate': 5.880169090531351e-07, 'epoch': 24.16}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 5.706117437793701e-07, 'epoch': 24.18}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 5.534673204849572e-07, 'epoch': 24.19}
{'loss': 0.0001, 'learning_rate': 5.365836841291438e-07, 'epoch': 24.2}
{'loss': 0.0001, 'learning_rate': 5.199608789873134e-07, 'epoch': 24.21}
{'loss': 0.0001, 'learning_rate': 5.035989486508075e-07, 'epoch': 24.23}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 4.874979360268928e-07, 'epoch': 24.24}
{'loss': 0.0001, 'learning_rate': 4.7165788333860536e-07, 'epoch': 24.25}
{'loss': 0.0001, 'learning_rate': 4.56078832124629e-07, 'epoch': 24.26}
{'loss': 0.0001, 'learning_rate': 4.4076082323920576e-07, 'epoch': 24.27}
{'loss': 0.0001, 'learning_rate': 4.257038968520366e-07, 'epoch': 24.29}
{'loss': 0.0001, 'learning_rate': 4.1090809244814785e-07, 'epoch': 24.3}
{'loss': 0.0001, 'learning_rate': 3.963734488278248e-07, 'epoch': 24.31}
{'loss': 0.0001, 'learning_rate': 3.82100004106456e-07, 'epoch': 24.32}
{'loss': 0.0001, 'learning_rate': 3.6808779571451126e-07, 'epoch': 24.34}
{'loss': 0.0001, 'learning_rate': 3.543368603973529e-07, 'epoch': 24.35}
{'loss': 0.0001, 'learning_rate': 3.4084723421521356e-07, 'epoch': 24.36}
{'loss': 0.0001, 'learning_rate': 3.2761895254306287e-07, 'epoch': 24.38}
{'loss': 0.0001, 'learning_rate': 3.146520500705297e-07, 'epoch': 24.39}
{'loss': 0.0001, 'learning_rate': 3.019465608018024e-07, 'epoch': 24.4}
{'loss': 0.0001, 'learning_rate': 2.8950251805553997e-07, 'epoch': 24.41}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.773199544648164e-07, 'epoch': 24.43}
{'loss': 0.0001, 'learning_rate': 2.6539890197695427e-07, 'epoch': 24.44}
{'loss': 0.0001, 'learning_rate': 2.537393918535358e-07, 'epoch': 24.45}
{'loss': 0.0001, 'learning_rate': 2.423414546702807e-07, 'epoch': 24.46}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 2.312051203169352e-07, 'epoch': 24.48}
{'loss': 0.0001, 'learning_rate': 2.2033041799723875e-07, 'epoch': 24.49}
{'loss': 0.0001, 'learning_rate': 2.0971737622883515e-07, 'epoch': 24.5}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.9936602284318373e-07, 'epoch': 24.51}
{'loss': 0.0001, 'learning_rate': 1.8927638498551502e-07, 'epoch': 24.52}
{'loss': 0.0001, 'learning_rate': 1.7944848911470857e-07, 'epoch': 24.54}
{'loss': 0.0001, 'learning_rate': 1.6988236100329292e-07, 'epoch': 24.55}
{'loss': 0.0003, 'learning_rate': 1.605780257373124e-07, 'epoch': 24.56}
{'loss': 0.0001, 'learning_rate': 1.5153550771630498e-07, 'epoch': 24.57}
{'loss': 0.0001, 'learning_rate': 1.427548306532134e-07, 'epoch': 24.59}
{'loss': 0.0001, 'learning_rate': 1.3423601757436287e-07, 'epoch': 24.6}
{'loss': 0.0001, 'learning_rate': 1.2597909081931702e-07, 'epoch': 24.61}
{'loss': 0.0001, 'learning_rate': 1.179840720409331e-07, 'epoch': 24.62}
{'loss': 0.0001, 'learning_rate': 1.1025098220518448e-07, 'epoch': 24.64}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.0277984159122733e-07, 'epoch': 24.65}
{'loss': 0.0001, 'learning_rate': 9.557066979123397e-08, 'epoch': 24.66}
{'loss': 0.0001, 'learning_rate': 8.862348571043733e-08, 'epoch': 24.68}
{'loss': 0.0001, 'learning_rate': 8.193830756699772e-08, 'epoch': 24.69}
{'loss': 0.0001, 'learning_rate': 7.551515289203615e-08, 'epoch': 24.7}
{'loss': 0.0001, 'learning_rate': 6.935403852950107e-08, 'epoch': 24.71}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 6.34549806362239e-08, 'epoch': 24.73}
{'loss': 0.0001, 'learning_rate': 5.7817994681774735e-08, 'epoch': 24.74}
{'loss': 0.0001, 'learning_rate': 5.2443095448506674e-08, 'epoch': 24.75}
{'loss': 0.0001, 'learning_rate': 4.7330297031467075e-08, 'epoch': 24.76}
{'loss': 0.0001, 'learning_rate': 4.247961283835311e-08, 'epoch': 24.77}
{'loss': 0.0001, 'learning_rate': 3.789105558954509e-08, 'epoch': 24.79}
{'loss': 0.0001, 'learning_rate': 3.356463731798432e-08, 'epoch': 24.8}
{'loss': 0.0001, 'learning_rate': 2.9500369369195312e-08, 'epoch': 24.81}
{'loss': 0.0001, 'learning_rate': 2.5698262401263605e-08, 'epoch': 24.82}
{'loss': 0.0001, 'learning_rate': 2.215832638474691e-08, 'epoch': 24.84}
{'loss': 0.0001, 'learning_rate': 1.888057060274173e-08, 'epoch': 24.85}
{'loss': 0.0001, 'learning_rate': 1.5865003650761266e-08, 'epoch': 24.86}
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
{'loss': 0.0001, 'learning_rate': 1.3111633436779791e-08, 'epoch': 24.88}
{'loss': 0.0001, 'learning_rate': 1.0620467181210459e-08, 'epoch': 24.89}
{'loss': 0.0001, 'learning_rate': 8.391511416816489e-09, 'epoch': 24.9}
{'loss': 0.0001, 'learning_rate': 6.42477198878888e-09, 'epoch': 24.91}
{'loss': 0.0001, 'learning_rate': 4.720254054679796e-09, 'epoch': 24.93}
{'loss': 0.0001, 'learning_rate': 3.2779620843692572e-09, 'epoch': 24.94}
{'loss': 0.0001, 'learning_rate': 2.0978998601206556e-09, 'epoch': 24.95}
{'loss': 0.0001, 'learning_rate': 1.1800704765030368e-09, 'epoch': 24.96}
{'loss': 0.0001, 'learning_rate': 5.244763404133046e-10, 'epoch': 24.98}
{'loss': 0.0001, 'learning_rate': 1.311191710651194e-10, 'epoch': 24.99}
{'loss': 0.0001, 'learning_rate': 0.0, 'epoch': 25.0}
{'train_runtime': 33855.8936, 'train_samples_per_second': 4.726, 'train_steps_per_second': 0.059, 'train_loss': 0.18218032012735422, 'epoch': 25.0}
[2025-05-03 19:27:24,321] [INFO] [launch.py:351:main] Process 65943 exits successfully.
[2025-05-03 19:27:24,322] [INFO] [launch.py:351:main] Process 65945 exits successfully.
[2025-05-03 19:27:24,323] [INFO] [launch.py:351:main] Process 65946 exits successfully.
[2025-05-03 19:27:24,324] [INFO] [launch.py:351:main] Process 65944 exits successfully.
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mcopper-bird-86[0m at: [34mhttps://wandb.ai/jt828-cornell-university/huggingface/runs/k5phshev[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250503_100255-k5phshev/logs[0m
[2025-05-03 19:27:28,325] [INFO] [launch.py:351:main] Process 65942 exits successfully.

Error Output:
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.31s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.64s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.65s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.66s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.28s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.61s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.16s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.75s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.33s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.75s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.34s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.36s/it]
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jt828 (jt828-cornell-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/zl986/backdoor-test/LLaVA/wandb/run-20250503_100255-k5phshev
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-bird-86
wandb: ⭐️ View project at https://wandb.ai/jt828-cornell-university/huggingface
wandb: 🚀 View run at https://wandb.ai/jt828-cornell-university/huggingface/runs/k5phshev

  0%|          | 0/2000 [00:00<?, ?it/s]/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/zl986/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

  0%|          | 1/2000 [00:34<18:53:46, 34.03s/it]
                                                   

  0%|          | 1/2000 [00:34<18:53:46, 34.03s/it]
  0%|          | 2/2000 [00:50<13:15:24, 23.89s/it]
                                                   

  0%|          | 2/2000 [00:50<13:15:24, 23.89s/it]
  0%|          | 3/2000 [01:07<11:32:30, 20.81s/it]
                                                   

  0%|          | 3/2000 [01:07<11:32:30, 20.81s/it]
  0%|          | 4/2000 [01:25<10:52:07, 19.60s/it]
                                                   

  0%|          | 4/2000 [01:25<10:52:07, 19.60s/it]
  0%|          | 5/2000 [01:42<10:22:03, 18.71s/it]
                                                   

  0%|          | 5/2000 [01:42<10:22:03, 18.71s/it]
  0%|          | 6/2000 [02:00<10:11:53, 18.41s/it]
                                                   

  0%|          | 6/2000 [02:00<10:11:53, 18.41s/it]
  0%|          | 7/2000 [02:17<9:56:31, 17.96s/it] 
                                                  

  0%|          | 7/2000 [02:17<9:56:31, 17.96s/it]
  0%|          | 8/2000 [02:34<9:46:25, 17.66s/it]
                                                  

  0%|          | 8/2000 [02:34<9:46:25, 17.66s/it]
  0%|          | 9/2000 [02:51<9:38:50, 17.44s/it]
                                                  

  0%|          | 9/2000 [02:51<9:38:50, 17.44s/it]
  0%|          | 10/2000 [03:08<9:34:32, 17.32s/it]
                                                   

  0%|          | 10/2000 [03:08<9:34:32, 17.32s/it]
  1%|          | 11/2000 [03:25<9:30:59, 17.22s/it]
                                                   

  1%|          | 11/2000 [03:25<9:30:59, 17.22s/it]
  1%|          | 12/2000 [03:42<9:28:02, 17.14s/it]
                                                   

  1%|          | 12/2000 [03:42<9:28:02, 17.14s/it]
  1%|          | 13/2000 [03:59<9:26:14, 17.10s/it]
                                                   

  1%|          | 13/2000 [03:59<9:26:14, 17.10s/it]
  1%|          | 14/2000 [04:16<9:24:12, 17.05s/it]
                                                   

  1%|          | 14/2000 [04:16<9:24:12, 17.05s/it]
  1%|          | 15/2000 [04:33<9:23:30, 17.03s/it]
                                                   

  1%|          | 15/2000 [04:33<9:23:30, 17.03s/it]
  1%|          | 16/2000 [04:50<9:21:31, 16.98s/it]
                                                   

  1%|          | 16/2000 [04:50<9:21:31, 16.98s/it]
  1%|          | 17/2000 [05:07<9:21:05, 16.98s/it]
                                                   

  1%|          | 17/2000 [05:07<9:21:05, 16.98s/it]
  1%|          | 18/2000 [05:24<9:20:08, 16.96s/it]
                                                   

  1%|          | 18/2000 [05:24<9:20:08, 16.96s/it]
  1%|          | 19/2000 [05:41<9:20:10, 16.97s/it]
                                                   

  1%|          | 19/2000 [05:41<9:20:10, 16.97s/it]
  1%|          | 20/2000 [05:58<9:18:40, 16.93s/it]
                                                   

  1%|          | 20/2000 [05:58<9:18:40, 16.93s/it]
  1%|          | 21/2000 [06:15<9:18:33, 16.93s/it]
                                                   

  1%|          | 21/2000 [06:15<9:18:33, 16.93s/it]
  1%|          | 22/2000 [06:31<9:17:13, 16.90s/it]
                                                   

  1%|          | 22/2000 [06:31<9:17:13, 16.90s/it]
  1%|          | 23/2000 [06:49<9:18:54, 16.96s/it]
                                                   

  1%|          | 23/2000 [06:49<9:18:54, 16.96s/it]
  1%|          | 24/2000 [07:05<9:17:43, 16.94s/it]
                                                   

  1%|          | 24/2000 [07:05<9:17:43, 16.94s/it]
  1%|▏         | 25/2000 [07:23<9:25:37, 17.18s/it]
                                                   

  1%|▏         | 25/2000 [07:23<9:25:37, 17.18s/it]
  1%|▏         | 26/2000 [07:40<9:22:10, 17.09s/it]
                                                   

  1%|▏         | 26/2000 [07:40<9:22:10, 17.09s/it]
  1%|▏         | 27/2000 [07:57<9:19:59, 17.03s/it]
                                                   

  1%|▏         | 27/2000 [07:57<9:19:59, 17.03s/it]
  1%|▏         | 28/2000 [08:14<9:19:05, 17.01s/it]
                                                   

  1%|▏         | 28/2000 [08:14<9:19:05, 17.01s/it]
  1%|▏         | 29/2000 [08:31<9:18:15, 16.99s/it]
                                                   

  1%|▏         | 29/2000 [08:31<9:18:15, 16.99s/it]
  2%|▏         | 30/2000 [08:48<9:18:40, 17.02s/it]
                                                   

  2%|▏         | 30/2000 [08:48<9:18:40, 17.02s/it]
  2%|▏         | 31/2000 [09:05<9:16:56, 16.97s/it]
                                                   

  2%|▏         | 31/2000 [09:05<9:16:56, 16.97s/it]
  2%|▏         | 32/2000 [09:22<9:15:11, 16.93s/it]
                                                   

  2%|▏         | 32/2000 [09:22<9:15:11, 16.93s/it]
  2%|▏         | 33/2000 [09:39<9:14:41, 16.92s/it]
                                                   

  2%|▏         | 33/2000 [09:39<9:14:41, 16.92s/it]
  2%|▏         | 34/2000 [09:55<9:13:29, 16.89s/it]
                                                   

  2%|▏         | 34/2000 [09:55<9:13:29, 16.89s/it]
  2%|▏         | 35/2000 [10:12<9:12:57, 16.88s/it]
                                                   

  2%|▏         | 35/2000 [10:12<9:12:57, 16.88s/it]
  2%|▏         | 36/2000 [10:29<9:12:52, 16.89s/it]
                                                   

  2%|▏         | 36/2000 [10:29<9:12:52, 16.89s/it]
  2%|▏         | 37/2000 [10:46<9:12:40, 16.89s/it]
                                                   

  2%|▏         | 37/2000 [10:46<9:12:40, 16.89s/it]
  2%|▏         | 38/2000 [11:03<9:12:21, 16.89s/it]
                                                   

  2%|▏         | 38/2000 [11:03<9:12:21, 16.89s/it]
  2%|▏         | 39/2000 [11:20<9:12:07, 16.89s/it]
                                                   

  2%|▏         | 39/2000 [11:20<9:12:07, 16.89s/it]
  2%|▏         | 40/2000 [11:37<9:12:09, 16.90s/it]
                                                   

  2%|▏         | 40/2000 [11:37<9:12:09, 16.90s/it]
  2%|▏         | 41/2000 [11:54<9:12:47, 16.93s/it]
                                                   

  2%|▏         | 41/2000 [11:54<9:12:47, 16.93s/it]
  2%|▏         | 42/2000 [12:11<9:12:28, 16.93s/it]
                                                   

  2%|▏         | 42/2000 [12:11<9:12:28, 16.93s/it]
  2%|▏         | 43/2000 [12:28<9:12:12, 16.93s/it]
                                                   

  2%|▏         | 43/2000 [12:28<9:12:12, 16.93s/it]
  2%|▏         | 44/2000 [12:44<9:11:19, 16.91s/it]
                                                   

  2%|▏         | 44/2000 [12:44<9:11:19, 16.91s/it]
  2%|▏         | 45/2000 [13:01<9:10:49, 16.91s/it]
                                                   

  2%|▏         | 45/2000 [13:01<9:10:49, 16.91s/it]
  2%|▏         | 46/2000 [13:18<9:10:11, 16.89s/it]
                                                   

  2%|▏         | 46/2000 [13:18<9:10:11, 16.89s/it]
  2%|▏         | 47/2000 [13:35<9:09:30, 16.88s/it]
                                                   

  2%|▏         | 47/2000 [13:35<9:09:30, 16.88s/it]
  2%|▏         | 48/2000 [13:52<9:08:45, 16.87s/it]
                                                   

  2%|▏         | 48/2000 [13:52<9:08:45, 16.87s/it]
  2%|▏         | 49/2000 [14:09<9:08:29, 16.87s/it]
                                                   

  2%|▏         | 49/2000 [14:09<9:08:29, 16.87s/it]
  2%|▎         | 50/2000 [14:26<9:07:49, 16.86s/it]
                                                   

  2%|▎         | 50/2000 [14:26<9:07:49, 16.86s/it]
  3%|▎         | 51/2000 [14:43<9:07:58, 16.87s/it]
                                                   

  3%|▎         | 51/2000 [14:43<9:07:58, 16.87s/it]
  3%|▎         | 52/2000 [14:59<9:07:56, 16.88s/it]
                                                   

  3%|▎         | 52/2000 [14:59<9:07:56, 16.88s/it]
  3%|▎         | 53/2000 [15:16<9:08:19, 16.90s/it]
                                                   

  3%|▎         | 53/2000 [15:16<9:08:19, 16.90s/it]
  3%|▎         | 54/2000 [15:33<9:08:29, 16.91s/it]
                                                   

  3%|▎         | 54/2000 [15:33<9:08:29, 16.91s/it]
  3%|▎         | 55/2000 [15:50<9:09:47, 16.96s/it]
                                                   

  3%|▎         | 55/2000 [15:50<9:09:47, 16.96s/it]
  3%|▎         | 56/2000 [16:07<9:09:13, 16.95s/it]
                                                   

  3%|▎         | 56/2000 [16:07<9:09:13, 16.95s/it]
  3%|▎         | 57/2000 [16:24<9:08:24, 16.94s/it]
                                                   

  3%|▎         | 57/2000 [16:24<9:08:24, 16.94s/it]
  3%|▎         | 58/2000 [16:41<9:07:18, 16.91s/it]
                                                   

  3%|▎         | 58/2000 [16:41<9:07:18, 16.91s/it]
  3%|▎         | 59/2000 [16:58<9:06:47, 16.90s/it]
                                                   

  3%|▎         | 59/2000 [16:58<9:06:47, 16.90s/it]
  3%|▎         | 60/2000 [17:15<9:05:38, 16.88s/it]
                                                   

  3%|▎         | 60/2000 [17:15<9:05:38, 16.88s/it]
  3%|▎         | 61/2000 [17:32<9:05:12, 16.87s/it]
                                                   

  3%|▎         | 61/2000 [17:32<9:05:12, 16.87s/it]
  3%|▎         | 62/2000 [17:49<9:07:18, 16.94s/it]
                                                   

  3%|▎         | 62/2000 [17:49<9:07:18, 16.94s/it]
  3%|▎         | 63/2000 [18:06<9:05:53, 16.91s/it]
                                                   

  3%|▎         | 63/2000 [18:06<9:05:53, 16.91s/it]
  3%|▎         | 64/2000 [18:23<9:06:53, 16.95s/it]
                                                   

  3%|▎         | 64/2000 [18:23<9:06:53, 16.95s/it]
  3%|▎         | 65/2000 [18:40<9:09:04, 17.03s/it]
                                                   

  3%|▎         | 65/2000 [18:40<9:09:04, 17.03s/it]
  3%|▎         | 66/2000 [18:57<9:08:28, 17.02s/it]
                                                   

  3%|▎         | 66/2000 [18:57<9:08:28, 17.02s/it]
  3%|▎         | 67/2000 [19:14<9:08:02, 17.01s/it]
                                                   

  3%|▎         | 67/2000 [19:14<9:08:02, 17.01s/it]
  3%|▎         | 68/2000 [19:31<9:06:08, 16.96s/it]
                                                   

  3%|▎         | 68/2000 [19:31<9:06:08, 16.96s/it]
  3%|▎         | 69/2000 [19:48<9:05:31, 16.95s/it]
                                                   

  3%|▎         | 69/2000 [19:48<9:05:31, 16.95s/it]
  4%|▎         | 70/2000 [20:04<9:04:39, 16.93s/it]
                                                   

  4%|▎         | 70/2000 [20:04<9:04:39, 16.93s/it]
  4%|▎         | 71/2000 [20:21<9:04:01, 16.92s/it]
                                                   

  4%|▎         | 71/2000 [20:21<9:04:01, 16.92s/it]
  4%|▎         | 72/2000 [20:38<9:02:57, 16.90s/it]
                                                   

  4%|▎         | 72/2000 [20:38<9:02:57, 16.90s/it]
  4%|▎         | 73/2000 [20:55<9:01:22, 16.86s/it]
                                                   

  4%|▎         | 73/2000 [20:55<9:01:22, 16.86s/it]
  4%|▎         | 74/2000 [21:12<9:00:22, 16.83s/it]
                                                   

  4%|▎         | 74/2000 [21:12<9:00:22, 16.83s/it]
  4%|▍         | 75/2000 [21:29<9:00:05, 16.83s/it]
                                                   

  4%|▍         | 75/2000 [21:29<9:00:05, 16.83s/it]
  4%|▍         | 76/2000 [21:45<8:59:57, 16.84s/it]
                                                   

  4%|▍         | 76/2000 [21:45<8:59:57, 16.84s/it]
  4%|▍         | 77/2000 [22:02<8:59:52, 16.84s/it]
                                                   

  4%|▍         | 77/2000 [22:02<8:59:52, 16.84s/it]
  4%|▍         | 78/2000 [22:19<8:59:27, 16.84s/it]
                                                   

  4%|▍         | 78/2000 [22:19<8:59:27, 16.84s/it]
  4%|▍         | 79/2000 [22:36<8:59:54, 16.86s/it]
                                                   

  4%|▍         | 79/2000 [22:36<8:59:54, 16.86s/it]
  4%|▍         | 80/2000 [22:53<9:01:55, 16.94s/it]
                                                   

  4%|▍         | 80/2000 [22:53<9:01:55, 16.94s/it]
  4%|▍         | 81/2000 [23:12<9:23:57, 17.63s/it]
                                                   

  4%|▍         | 81/2000 [23:12<9:23:57, 17.63s/it]
  4%|▍         | 82/2000 [23:30<9:21:26, 17.56s/it]
                                                   

  4%|▍         | 82/2000 [23:30<9:21:26, 17.56s/it]
  4%|▍         | 83/2000 [23:47<9:14:45, 17.36s/it]
                                                   

  4%|▍         | 83/2000 [23:47<9:14:45, 17.36s/it]
  4%|▍         | 84/2000 [24:04<9:10:27, 17.24s/it]
                                                   

  4%|▍         | 84/2000 [24:04<9:10:27, 17.24s/it]
  4%|▍         | 85/2000 [24:21<9:07:19, 17.15s/it]
                                                   

  4%|▍         | 85/2000 [24:21<9:07:19, 17.15s/it]
  4%|▍         | 86/2000 [24:37<9:03:59, 17.05s/it]
                                                   

  4%|▍         | 86/2000 [24:37<9:03:59, 17.05s/it]
  4%|▍         | 87/2000 [24:54<9:03:09, 17.04s/it]
                                                   

  4%|▍         | 87/2000 [24:54<9:03:09, 17.04s/it]
  4%|▍         | 88/2000 [25:11<9:01:23, 16.99s/it]
                                                   

  4%|▍         | 88/2000 [25:11<9:01:23, 16.99s/it]
  4%|▍         | 89/2000 [25:28<9:01:22, 17.00s/it]
                                                   

  4%|▍         | 89/2000 [25:28<9:01:22, 17.00s/it]
  4%|▍         | 90/2000 [25:45<9:01:10, 17.00s/it]
                                                   

  4%|▍         | 90/2000 [25:45<9:01:10, 17.00s/it]
  5%|▍         | 91/2000 [26:02<9:01:03, 17.01s/it]
                                                   

  5%|▍         | 91/2000 [26:02<9:01:03, 17.01s/it]
  5%|▍         | 92/2000 [26:19<8:58:59, 16.95s/it]
                                                   

  5%|▍         | 92/2000 [26:19<8:58:59, 16.95s/it]
  5%|▍         | 93/2000 [26:36<9:00:17, 17.00s/it]
                                                   

  5%|▍         | 93/2000 [26:36<9:00:17, 17.00s/it]
  5%|▍         | 94/2000 [26:53<9:00:16, 17.01s/it]
                                                   

  5%|▍         | 94/2000 [26:53<9:00:16, 17.01s/it]
  5%|▍         | 95/2000 [27:10<9:00:04, 17.01s/it]
                                                   

  5%|▍         | 95/2000 [27:10<9:00:04, 17.01s/it]
  5%|▍         | 96/2000 [27:27<8:59:02, 16.99s/it]
                                                   

  5%|▍         | 96/2000 [27:27<8:59:02, 16.99s/it]
  5%|▍         | 97/2000 [27:44<8:58:30, 16.98s/it]
                                                   

  5%|▍         | 97/2000 [27:44<8:58:30, 16.98s/it]
  5%|▍         | 98/2000 [28:01<8:57:12, 16.95s/it]
                                                   

  5%|▍         | 98/2000 [28:01<8:57:12, 16.95s/it]
  5%|▍         | 99/2000 [28:18<8:56:29, 16.93s/it]
                                                   

  5%|▍         | 99/2000 [28:18<8:56:29, 16.93s/it]
  5%|▌         | 100/2000 [28:35<8:55:52, 16.92s/it]
                                                    

  5%|▌         | 100/2000 [28:35<8:55:52, 16.92s/it]
  5%|▌         | 101/2000 [28:52<8:54:42, 16.89s/it]
                                                    

  5%|▌         | 101/2000 [28:52<8:54:42, 16.89s/it]
  5%|▌         | 102/2000 [29:08<8:53:02, 16.85s/it]
                                                    

  5%|▌         | 102/2000 [29:08<8:53:02, 16.85s/it]
  5%|▌         | 103/2000 [29:25<8:51:55, 16.82s/it]
                                                    

  5%|▌         | 103/2000 [29:25<8:51:55, 16.82s/it]
  5%|▌         | 104/2000 [29:42<8:51:34, 16.82s/it]
                                                    

  5%|▌         | 104/2000 [29:42<8:51:34, 16.82s/it]
  5%|▌         | 105/2000 [29:59<8:52:07, 16.85s/it]
                                                    

  5%|▌         | 105/2000 [29:59<8:52:07, 16.85s/it]
  5%|▌         | 106/2000 [30:16<8:51:13, 16.83s/it]
                                                    

  5%|▌         | 106/2000 [30:16<8:51:13, 16.83s/it]
  5%|▌         | 107/2000 [30:33<8:51:15, 16.84s/it]
                                                    

  5%|▌         | 107/2000 [30:33<8:51:15, 16.84s/it]
  5%|▌         | 108/2000 [30:49<8:51:31, 16.86s/it]
                                                    

  5%|▌         | 108/2000 [30:49<8:51:31, 16.86s/it]
  5%|▌         | 109/2000 [31:06<8:50:58, 16.85s/it]
                                                    

  5%|▌         | 109/2000 [31:06<8:50:58, 16.85s/it]
  6%|▌         | 110/2000 [31:23<8:50:39, 16.85s/it]
                                                    

  6%|▌         | 110/2000 [31:23<8:50:39, 16.85s/it]
  6%|▌         | 111/2000 [31:40<8:51:11, 16.87s/it]
                                                    

  6%|▌         | 111/2000 [31:40<8:51:11, 16.87s/it]
  6%|▌         | 112/2000 [31:57<8:50:28, 16.86s/it]
                                                    

  6%|▌         | 112/2000 [31:57<8:50:28, 16.86s/it]
  6%|▌         | 113/2000 [32:14<8:48:52, 16.82s/it]
                                                    

  6%|▌         | 113/2000 [32:14<8:48:52, 16.82s/it]
  6%|▌         | 114/2000 [32:30<8:48:16, 16.81s/it]
                                                    

  6%|▌         | 114/2000 [32:30<8:48:16, 16.81s/it]
  6%|▌         | 115/2000 [32:47<8:49:01, 16.84s/it]
                                                    

  6%|▌         | 115/2000 [32:47<8:49:01, 16.84s/it]
  6%|▌         | 116/2000 [33:04<8:50:05, 16.88s/it]
                                                    

  6%|▌         | 116/2000 [33:04<8:50:05, 16.88s/it]
  6%|▌         | 117/2000 [33:21<8:50:28, 16.90s/it]
                                                    

  6%|▌         | 117/2000 [33:21<8:50:28, 16.90s/it]
  6%|▌         | 118/2000 [33:38<8:50:41, 16.92s/it]
                                                    

  6%|▌         | 118/2000 [33:38<8:50:41, 16.92s/it]
  6%|▌         | 119/2000 [33:55<8:49:46, 16.90s/it]
                                                    

  6%|▌         | 119/2000 [33:55<8:49:46, 16.90s/it]
  6%|▌         | 120/2000 [34:13<8:56:53, 17.13s/it]
                                                    

  6%|▌         | 120/2000 [34:13<8:56:53, 17.13s/it]
  6%|▌         | 121/2000 [34:30<8:54:46, 17.08s/it]
                                                    

  6%|▌         | 121/2000 [34:30<8:54:46, 17.08s/it]
  6%|▌         | 122/2000 [34:47<8:53:05, 17.03s/it]
                                                    

  6%|▌         | 122/2000 [34:47<8:53:05, 17.03s/it]
  6%|▌         | 123/2000 [35:04<8:52:46, 17.03s/it]
                                                    

  6%|▌         | 123/2000 [35:04<8:52:46, 17.03s/it]
  6%|▌         | 124/2000 [35:20<8:50:05, 16.95s/it]
                                                    

  6%|▌         | 124/2000 [35:20<8:50:05, 16.95s/it]
  6%|▋         | 125/2000 [35:37<8:49:39, 16.95s/it]
                                                    

  6%|▋         | 125/2000 [35:37<8:49:39, 16.95s/it]
  6%|▋         | 126/2000 [35:54<8:49:30, 16.95s/it]
                                                    

  6%|▋         | 126/2000 [35:54<8:49:30, 16.95s/it]
  6%|▋         | 127/2000 [36:11<8:48:39, 16.93s/it]
                                                    

  6%|▋         | 127/2000 [36:11<8:48:39, 16.93s/it]
  6%|▋         | 128/2000 [36:28<8:47:37, 16.91s/it]
                                                    

  6%|▋         | 128/2000 [36:28<8:47:37, 16.91s/it]
  6%|▋         | 129/2000 [36:45<8:47:31, 16.92s/it]
                                                    

  6%|▋         | 129/2000 [36:45<8:47:31, 16.92s/it]
  6%|▋         | 130/2000 [37:02<8:45:57, 16.88s/it]
                                                    

  6%|▋         | 130/2000 [37:02<8:45:57, 16.88s/it]
  7%|▋         | 131/2000 [37:19<8:45:34, 16.87s/it]
                                                    

  7%|▋         | 131/2000 [37:19<8:45:34, 16.87s/it]
  7%|▋         | 132/2000 [37:36<8:46:42, 16.92s/it]
                                                    

  7%|▋         | 132/2000 [37:36<8:46:42, 16.92s/it]
  7%|▋         | 133/2000 [37:53<8:45:40, 16.89s/it]
                                                    

  7%|▋         | 133/2000 [37:53<8:45:40, 16.89s/it]
  7%|▋         | 134/2000 [38:10<8:47:09, 16.95s/it]
                                                    

  7%|▋         | 134/2000 [38:10<8:47:09, 16.95s/it]
  7%|▋         | 135/2000 [38:27<8:47:29, 16.97s/it]
                                                    

  7%|▋         | 135/2000 [38:27<8:47:29, 16.97s/it]
  7%|▋         | 136/2000 [38:43<8:46:21, 16.94s/it]
                                                    

  7%|▋         | 136/2000 [38:43<8:46:21, 16.94s/it]
  7%|▋         | 137/2000 [39:01<8:47:01, 16.97s/it]
                                                    

  7%|▋         | 137/2000 [39:01<8:47:01, 16.97s/it]
  7%|▋         | 138/2000 [39:17<8:45:58, 16.95s/it]
                                                    

  7%|▋         | 138/2000 [39:17<8:45:58, 16.95s/it]
  7%|▋         | 139/2000 [39:34<8:46:48, 16.98s/it]
                                                    

  7%|▋         | 139/2000 [39:34<8:46:48, 16.98s/it]
  7%|▋         | 140/2000 [39:51<8:46:00, 16.97s/it]
                                                    

  7%|▋         | 140/2000 [39:51<8:46:00, 16.97s/it]
  7%|▋         | 141/2000 [40:08<8:45:59, 16.98s/it]
                                                    

  7%|▋         | 141/2000 [40:08<8:45:59, 16.98s/it]
  7%|▋         | 142/2000 [40:25<8:44:40, 16.94s/it]
                                                    

  7%|▋         | 142/2000 [40:25<8:44:40, 16.94s/it]
  7%|▋         | 143/2000 [40:42<8:44:40, 16.95s/it]
                                                    

  7%|▋         | 143/2000 [40:42<8:44:40, 16.95s/it]
  7%|▋         | 144/2000 [40:59<8:43:51, 16.94s/it]
                                                    

  7%|▋         | 144/2000 [40:59<8:43:51, 16.94s/it]
  7%|▋         | 145/2000 [41:16<8:43:39, 16.94s/it]
                                                    

  7%|▋         | 145/2000 [41:16<8:43:39, 16.94s/it]
  7%|▋         | 146/2000 [41:33<8:44:38, 16.98s/it]
                                                    

  7%|▋         | 146/2000 [41:33<8:44:38, 16.98s/it]
  7%|▋         | 147/2000 [41:50<8:44:04, 16.97s/it]
                                                    

  7%|▋         | 147/2000 [41:50<8:44:04, 16.97s/it]
  7%|▋         | 148/2000 [42:07<8:44:08, 16.98s/it]
                                                    

  7%|▋         | 148/2000 [42:07<8:44:08, 16.98s/it]
  7%|▋         | 149/2000 [42:24<8:44:11, 16.99s/it]
                                                    

  7%|▋         | 149/2000 [42:24<8:44:11, 16.99s/it]
  8%|▊         | 150/2000 [42:41<8:42:41, 16.95s/it]
                                                    

  8%|▊         | 150/2000 [42:41<8:42:41, 16.95s/it]
  8%|▊         | 151/2000 [42:58<8:42:41, 16.96s/it]
                                                    

  8%|▊         | 151/2000 [42:58<8:42:41, 16.96s/it]
  8%|▊         | 152/2000 [43:15<8:40:28, 16.90s/it]
                                                    

  8%|▊         | 152/2000 [43:15<8:40:28, 16.90s/it]
  8%|▊         | 153/2000 [43:32<8:39:08, 16.86s/it]
                                                    

  8%|▊         | 153/2000 [43:32<8:39:08, 16.86s/it]
  8%|▊         | 154/2000 [43:48<8:38:56, 16.87s/it]
                                                    

  8%|▊         | 154/2000 [43:48<8:38:56, 16.87s/it]
  8%|▊         | 155/2000 [44:05<8:39:15, 16.89s/it]
                                                    

  8%|▊         | 155/2000 [44:05<8:39:15, 16.89s/it]
  8%|▊         | 156/2000 [44:22<8:38:52, 16.88s/it]
                                                    

  8%|▊         | 156/2000 [44:22<8:38:52, 16.88s/it]
  8%|▊         | 157/2000 [44:39<8:37:27, 16.85s/it]
                                                    

  8%|▊         | 157/2000 [44:39<8:37:27, 16.85s/it]
  8%|▊         | 158/2000 [44:56<8:37:52, 16.87s/it]
                                                    

  8%|▊         | 158/2000 [44:56<8:37:52, 16.87s/it]
  8%|▊         | 159/2000 [45:13<8:36:59, 16.85s/it]
                                                    

  8%|▊         | 159/2000 [45:13<8:36:59, 16.85s/it]
  8%|▊         | 160/2000 [45:30<8:37:00, 16.86s/it]
                                                    

  8%|▊         | 160/2000 [45:30<8:37:00, 16.86s/it]
  8%|▊         | 161/2000 [45:49<8:57:23, 17.53s/it]
                                                    

  8%|▊         | 161/2000 [45:49<8:57:23, 17.53s/it]
  8%|▊         | 162/2000 [46:06<8:54:09, 17.44s/it]
                                                    

  8%|▊         | 162/2000 [46:06<8:54:09, 17.44s/it]
  8%|▊         | 163/2000 [46:23<8:51:21, 17.36s/it]
                                                    

  8%|▊         | 163/2000 [46:23<8:51:21, 17.36s/it]
  8%|▊         | 164/2000 [46:41<8:54:31, 17.47s/it]
                                                    

  8%|▊         | 164/2000 [46:41<8:54:31, 17.47s/it]
  8%|▊         | 165/2000 [46:58<8:50:19, 17.34s/it]
                                                    

  8%|▊         | 165/2000 [46:58<8:50:19, 17.34s/it]
  8%|▊         | 166/2000 [47:15<8:45:52, 17.20s/it]
                                                    

  8%|▊         | 166/2000 [47:15<8:45:52, 17.20s/it]
  8%|▊         | 167/2000 [47:32<8:42:55, 17.12s/it]
                                                    

  8%|▊         | 167/2000 [47:32<8:42:55, 17.12s/it]
  8%|▊         | 168/2000 [47:49<8:40:52, 17.06s/it]
                                                    

  8%|▊         | 168/2000 [47:49<8:40:52, 17.06s/it]
  8%|▊         | 169/2000 [48:05<8:38:30, 16.99s/it]
                                                    

  8%|▊         | 169/2000 [48:05<8:38:30, 16.99s/it]
  8%|▊         | 170/2000 [48:22<8:36:39, 16.94s/it]
                                                    

  8%|▊         | 170/2000 [48:22<8:36:39, 16.94s/it]
  9%|▊         | 171/2000 [48:39<8:37:42, 16.98s/it]
                                                    

  9%|▊         | 171/2000 [48:39<8:37:42, 16.98s/it]
  9%|▊         | 172/2000 [48:56<8:35:48, 16.93s/it]
                                                    

  9%|▊         | 172/2000 [48:56<8:35:48, 16.93s/it]
  9%|▊         | 173/2000 [49:13<8:35:06, 16.92s/it]
                                                    

  9%|▊         | 173/2000 [49:13<8:35:06, 16.92s/it]
  9%|▊         | 174/2000 [49:30<8:35:00, 16.92s/it]
                                                    

  9%|▊         | 174/2000 [49:30<8:35:00, 16.92s/it]
  9%|▉         | 175/2000 [49:47<8:33:55, 16.90s/it]
                                                    

  9%|▉         | 175/2000 [49:47<8:33:55, 16.90s/it]
  9%|▉         | 176/2000 [50:04<8:33:01, 16.88s/it]
                                                    

  9%|▉         | 176/2000 [50:04<8:33:01, 16.88s/it]
  9%|▉         | 177/2000 [50:21<8:33:20, 16.90s/it]
                                                    

  9%|▉         | 177/2000 [50:21<8:33:20, 16.90s/it]
  9%|▉         | 178/2000 [50:37<8:33:09, 16.90s/it]
                                                    

  9%|▉         | 178/2000 [50:37<8:33:09, 16.90s/it]
  9%|▉         | 179/2000 [50:54<8:33:04, 16.91s/it]
                                                    

  9%|▉         | 179/2000 [50:54<8:33:04, 16.91s/it]
  9%|▉         | 180/2000 [51:11<8:34:08, 16.95s/it]
                                                    

  9%|▉         | 180/2000 [51:11<8:34:08, 16.95s/it]
  9%|▉         | 181/2000 [51:28<8:34:11, 16.96s/it]
                                                    

  9%|▉         | 181/2000 [51:28<8:34:11, 16.96s/it]
  9%|▉         | 182/2000 [51:45<8:32:57, 16.93s/it]
                                                    

  9%|▉         | 182/2000 [51:45<8:32:57, 16.93s/it]
  9%|▉         | 183/2000 [52:02<8:35:11, 17.01s/it]
                                                    

  9%|▉         | 183/2000 [52:02<8:35:11, 17.01s/it]
  9%|▉         | 184/2000 [52:19<8:33:55, 16.98s/it]
                                                    

  9%|▉         | 184/2000 [52:19<8:33:55, 16.98s/it]
  9%|▉         | 185/2000 [52:36<8:33:45, 16.98s/it]
                                                    

  9%|▉         | 185/2000 [52:36<8:33:45, 16.98s/it]
  9%|▉         | 186/2000 [52:53<8:32:25, 16.95s/it]
                                                    

  9%|▉         | 186/2000 [52:53<8:32:25, 16.95s/it]
  9%|▉         | 187/2000 [53:10<8:31:12, 16.92s/it]
                                                    

  9%|▉         | 187/2000 [53:10<8:31:12, 16.92s/it]
  9%|▉         | 188/2000 [53:27<8:30:15, 16.90s/it]
                                                    

  9%|▉         | 188/2000 [53:27<8:30:15, 16.90s/it]
  9%|▉         | 189/2000 [53:44<8:29:24, 16.88s/it]
                                                    

  9%|▉         | 189/2000 [53:44<8:29:24, 16.88s/it]
 10%|▉         | 190/2000 [54:01<8:29:09, 16.88s/it]
                                                    

 10%|▉         | 190/2000 [54:01<8:29:09, 16.88s/it]
 10%|▉         | 191/2000 [54:18<8:29:06, 16.89s/it]
                                                    

 10%|▉         | 191/2000 [54:18<8:29:06, 16.89s/it]
 10%|▉         | 192/2000 [54:34<8:28:13, 16.87s/it]
                                                    

 10%|▉         | 192/2000 [54:34<8:28:13, 16.87s/it]
 10%|▉         | 193/2000 [54:52<8:35:27, 17.12s/it]
                                                    

 10%|▉         | 193/2000 [54:52<8:35:27, 17.12s/it]
 10%|▉         | 194/2000 [55:09<8:32:37, 17.03s/it]
                                                    

 10%|▉         | 194/2000 [55:09<8:32:37, 17.03s/it]
 10%|▉         | 195/2000 [55:26<8:31:07, 16.99s/it]
                                                    

 10%|▉         | 195/2000 [55:26<8:31:07, 16.99s/it]
 10%|▉         | 196/2000 [55:43<8:30:17, 16.97s/it]
                                                    

 10%|▉         | 196/2000 [55:43<8:30:17, 16.97s/it]
 10%|▉         | 197/2000 [56:00<8:29:54, 16.97s/it]
                                                    

 10%|▉         | 197/2000 [56:00<8:29:54, 16.97s/it]
 10%|▉         | 198/2000 [56:17<8:28:48, 16.94s/it]
                                                    

 10%|▉         | 198/2000 [56:17<8:28:48, 16.94s/it]
 10%|▉         | 199/2000 [56:33<8:27:27, 16.91s/it]
                                                    

 10%|▉         | 199/2000 [56:33<8:27:27, 16.91s/it]
 10%|█         | 200/2000 [56:50<8:26:03, 16.87s/it]
                                                    

 10%|█         | 200/2000 [56:50<8:26:03, 16.87s/it]
 10%|█         | 201/2000 [57:07<8:25:16, 16.85s/it]
                                                    

 10%|█         | 201/2000 [57:07<8:25:16, 16.85s/it]
 10%|█         | 202/2000 [57:24<8:24:36, 16.84s/it]
                                                    

 10%|█         | 202/2000 [57:24<8:24:36, 16.84s/it]
 10%|█         | 203/2000 [57:41<8:24:01, 16.83s/it]
                                                    

 10%|█         | 203/2000 [57:41<8:24:01, 16.83s/it]
 10%|█         | 204/2000 [57:57<8:23:21, 16.82s/it]
                                                    

 10%|█         | 204/2000 [57:57<8:23:21, 16.82s/it]
 10%|█         | 205/2000 [58:14<8:23:03, 16.82s/it]
                                                    

 10%|█         | 205/2000 [58:14<8:23:03, 16.82s/it]
 10%|█         | 206/2000 [58:31<8:22:52, 16.82s/it]
                                                    

 10%|█         | 206/2000 [58:31<8:22:52, 16.82s/it]
 10%|█         | 207/2000 [58:48<8:22:50, 16.83s/it]
                                                    

 10%|█         | 207/2000 [58:48<8:22:50, 16.83s/it]
 10%|█         | 208/2000 [59:05<8:22:49, 16.84s/it]
                                                    

 10%|█         | 208/2000 [59:05<8:22:49, 16.84s/it]
 10%|█         | 209/2000 [59:22<8:22:21, 16.83s/it]
                                                    

 10%|█         | 209/2000 [59:22<8:22:21, 16.83s/it]
 10%|█         | 210/2000 [59:38<8:22:24, 16.84s/it]
                                                    

 10%|█         | 210/2000 [59:38<8:22:24, 16.84s/it]
 11%|█         | 211/2000 [59:55<8:21:57, 16.84s/it]
                                                    

 11%|█         | 211/2000 [59:55<8:21:57, 16.84s/it]
 11%|█         | 212/2000 [1:00:12<8:21:14, 16.82s/it]
                                                      

 11%|█         | 212/2000 [1:00:12<8:21:14, 16.82s/it]
 11%|█         | 213/2000 [1:00:29<8:20:57, 16.82s/it]
                                                      

 11%|█         | 213/2000 [1:00:29<8:20:57, 16.82s/it]
 11%|█         | 214/2000 [1:00:46<8:20:37, 16.82s/it]
                                                      

 11%|█         | 214/2000 [1:00:46<8:20:37, 16.82s/it]
 11%|█         | 215/2000 [1:01:02<8:20:32, 16.82s/it]
                                                      

 11%|█         | 215/2000 [1:01:02<8:20:32, 16.82s/it]
 11%|█         | 216/2000 [1:01:19<8:20:15, 16.82s/it]
                                                      

 11%|█         | 216/2000 [1:01:19<8:20:15, 16.82s/it]
 11%|█         | 217/2000 [1:01:36<8:19:59, 16.83s/it]
                                                      

 11%|█         | 217/2000 [1:01:36<8:19:59, 16.83s/it]
 11%|█         | 218/2000 [1:01:53<8:19:51, 16.83s/it]
                                                      

 11%|█         | 218/2000 [1:01:53<8:19:51, 16.83s/it]
 11%|█         | 219/2000 [1:02:10<8:19:43, 16.84s/it]
                                                      

 11%|█         | 219/2000 [1:02:10<8:19:43, 16.84s/it]
 11%|█         | 220/2000 [1:02:27<8:19:30, 16.84s/it]
                                                      

 11%|█         | 220/2000 [1:02:27<8:19:30, 16.84s/it]
 11%|█         | 221/2000 [1:02:43<8:19:20, 16.84s/it]
                                                      

 11%|█         | 221/2000 [1:02:43<8:19:20, 16.84s/it]
 11%|█         | 222/2000 [1:03:00<8:19:03, 16.84s/it]
                                                      

 11%|█         | 222/2000 [1:03:00<8:19:03, 16.84s/it]
 11%|█         | 223/2000 [1:03:17<8:19:02, 16.85s/it]
                                                      

 11%|█         | 223/2000 [1:03:17<8:19:02, 16.85s/it]
 11%|█         | 224/2000 [1:03:34<8:18:34, 16.84s/it]
                                                      

 11%|█         | 224/2000 [1:03:34<8:18:34, 16.84s/it]
 11%|█▏        | 225/2000 [1:03:51<8:17:44, 16.83s/it]
                                                      

 11%|█▏        | 225/2000 [1:03:51<8:17:44, 16.83s/it]
 11%|█▏        | 226/2000 [1:04:08<8:16:54, 16.81s/it]
                                                      

 11%|█▏        | 226/2000 [1:04:08<8:16:54, 16.81s/it]
 11%|█▏        | 227/2000 [1:04:24<8:17:07, 16.82s/it]
                                                      

 11%|█▏        | 227/2000 [1:04:24<8:17:07, 16.82s/it]
 11%|█▏        | 228/2000 [1:04:41<8:16:39, 16.82s/it]
                                                      

 11%|█▏        | 228/2000 [1:04:41<8:16:39, 16.82s/it]
 11%|█▏        | 229/2000 [1:04:58<8:16:20, 16.82s/it]
                                                      

 11%|█▏        | 229/2000 [1:04:58<8:16:20, 16.82s/it]
 12%|█▏        | 230/2000 [1:05:15<8:15:40, 16.80s/it]
                                                      

 12%|█▏        | 230/2000 [1:05:15<8:15:40, 16.80s/it]
 12%|█▏        | 231/2000 [1:05:32<8:15:50, 16.82s/it]
                                                      

 12%|█▏        | 231/2000 [1:05:32<8:15:50, 16.82s/it]
 12%|█▏        | 232/2000 [1:05:48<8:15:15, 16.81s/it]
                                                      

 12%|█▏        | 232/2000 [1:05:48<8:15:15, 16.81s/it]
 12%|█▏        | 233/2000 [1:06:05<8:14:48, 16.80s/it]
                                                      

 12%|█▏        | 233/2000 [1:06:05<8:14:48, 16.80s/it]
 12%|█▏        | 234/2000 [1:06:22<8:14:14, 16.79s/it]
                                                      

 12%|█▏        | 234/2000 [1:06:22<8:14:14, 16.79s/it]
 12%|█▏        | 235/2000 [1:06:39<8:14:18, 16.80s/it]
                                                      

 12%|█▏        | 235/2000 [1:06:39<8:14:18, 16.80s/it]
 12%|█▏        | 236/2000 [1:06:56<8:14:38, 16.82s/it]
                                                      

 12%|█▏        | 236/2000 [1:06:56<8:14:38, 16.82s/it]
 12%|█▏        | 237/2000 [1:07:13<8:15:00, 16.85s/it]
                                                      

 12%|█▏        | 237/2000 [1:07:13<8:15:00, 16.85s/it]
 12%|█▏        | 238/2000 [1:07:29<8:14:05, 16.83s/it]
                                                      

 12%|█▏        | 238/2000 [1:07:29<8:14:05, 16.83s/it]
 12%|█▏        | 239/2000 [1:07:46<8:13:45, 16.82s/it]
                                                      

 12%|█▏        | 239/2000 [1:07:46<8:13:45, 16.82s/it]
 12%|█▏        | 240/2000 [1:08:03<8:14:08, 16.85s/it]
                                                      

 12%|█▏        | 240/2000 [1:08:03<8:14:08, 16.85s/it]
 12%|█▏        | 241/2000 [1:08:22<8:35:03, 17.57s/it]
                                                      

 12%|█▏        | 241/2000 [1:08:22<8:35:03, 17.57s/it]
 12%|█▏        | 242/2000 [1:08:40<8:32:34, 17.49s/it]
                                                      

 12%|█▏        | 242/2000 [1:08:40<8:32:34, 17.49s/it]
 12%|█▏        | 243/2000 [1:08:57<8:26:58, 17.31s/it]
                                                      

 12%|█▏        | 243/2000 [1:08:57<8:26:58, 17.31s/it]
 12%|█▏        | 244/2000 [1:09:13<8:23:14, 17.19s/it]
                                                      

 12%|█▏        | 244/2000 [1:09:13<8:23:14, 17.19s/it]
 12%|█▏        | 245/2000 [1:09:30<8:20:14, 17.10s/it]
                                                      

 12%|█▏        | 245/2000 [1:09:30<8:20:14, 17.10s/it]
 12%|█▏        | 246/2000 [1:09:47<8:19:31, 17.09s/it]
                                                      

 12%|█▏        | 246/2000 [1:09:47<8:19:31, 17.09s/it]
 12%|█▏        | 247/2000 [1:10:04<8:18:25, 17.06s/it]
                                                      

 12%|█▏        | 247/2000 [1:10:04<8:18:25, 17.06s/it]
 12%|█▏        | 248/2000 [1:10:21<8:16:50, 17.02s/it]
                                                      

 12%|█▏        | 248/2000 [1:10:21<8:16:50, 17.02s/it]
 12%|█▏        | 249/2000 [1:10:38<8:14:57, 16.96s/it]
                                                      

 12%|█▏        | 249/2000 [1:10:38<8:14:57, 16.96s/it]
 12%|█▎        | 250/2000 [1:10:55<8:13:24, 16.92s/it]
                                                      

 12%|█▎        | 250/2000 [1:10:55<8:13:24, 16.92s/it]
 13%|█▎        | 251/2000 [1:11:12<8:14:53, 16.98s/it]
                                                      

 13%|█▎        | 251/2000 [1:11:12<8:14:53, 16.98s/it]
 13%|█▎        | 252/2000 [1:11:29<8:12:36, 16.91s/it]
                                                      

 13%|█▎        | 252/2000 [1:11:29<8:12:36, 16.91s/it]
 13%|█▎        | 253/2000 [1:11:46<8:12:00, 16.90s/it]
                                                      

 13%|█▎        | 253/2000 [1:11:46<8:12:00, 16.90s/it]
 13%|█▎        | 254/2000 [1:12:03<8:11:10, 16.88s/it]
                                                      

 13%|█▎        | 254/2000 [1:12:03<8:11:10, 16.88s/it]
 13%|█▎        | 255/2000 [1:12:19<8:10:05, 16.85s/it]
                                                      

 13%|█▎        | 255/2000 [1:12:19<8:10:05, 16.85s/it]
 13%|█▎        | 256/2000 [1:12:36<8:09:25, 16.84s/it]
                                                      

 13%|█▎        | 256/2000 [1:12:36<8:09:25, 16.84s/it]
 13%|█▎        | 257/2000 [1:12:53<8:08:47, 16.83s/it]
                                                      

 13%|█▎        | 257/2000 [1:12:53<8:08:47, 16.83s/it]
 13%|█▎        | 258/2000 [1:13:10<8:08:14, 16.82s/it]
                                                      

 13%|█▎        | 258/2000 [1:13:10<8:08:14, 16.82s/it]
 13%|█▎        | 259/2000 [1:13:27<8:07:38, 16.81s/it]
                                                      

 13%|█▎        | 259/2000 [1:13:27<8:07:38, 16.81s/it]
 13%|█▎        | 260/2000 [1:13:43<8:07:19, 16.80s/it]
                                                      

 13%|█▎        | 260/2000 [1:13:43<8:07:19, 16.80s/it]
 13%|█▎        | 261/2000 [1:14:00<8:07:08, 16.81s/it]
                                                      

 13%|█▎        | 261/2000 [1:14:00<8:07:08, 16.81s/it]
 13%|█▎        | 262/2000 [1:14:17<8:07:08, 16.82s/it]
                                                      

 13%|█▎        | 262/2000 [1:14:17<8:07:08, 16.82s/it]
 13%|█▎        | 263/2000 [1:14:34<8:06:57, 16.82s/it]
                                                      

 13%|█▎        | 263/2000 [1:14:34<8:06:57, 16.82s/it]
 13%|█▎        | 264/2000 [1:14:51<8:06:25, 16.81s/it]
                                                      

 13%|█▎        | 264/2000 [1:14:51<8:06:25, 16.81s/it]
 13%|█▎        | 265/2000 [1:15:07<8:06:33, 16.83s/it]
                                                      

 13%|█▎        | 265/2000 [1:15:07<8:06:33, 16.83s/it]
 13%|█▎        | 266/2000 [1:15:24<8:06:25, 16.83s/it]
                                                      

 13%|█▎        | 266/2000 [1:15:24<8:06:25, 16.83s/it]
 13%|█▎        | 267/2000 [1:15:41<8:06:14, 16.83s/it]
                                                      

 13%|█▎        | 267/2000 [1:15:41<8:06:14, 16.83s/it]
 13%|█▎        | 268/2000 [1:15:58<8:05:19, 16.81s/it]
                                                      

 13%|█▎        | 268/2000 [1:15:58<8:05:19, 16.81s/it]
 13%|█▎        | 269/2000 [1:16:15<8:05:08, 16.82s/it]
                                                      

 13%|█▎        | 269/2000 [1:16:15<8:05:08, 16.82s/it]
 14%|█▎        | 270/2000 [1:16:32<8:05:32, 16.84s/it]
                                                      

 14%|█▎        | 270/2000 [1:16:32<8:05:32, 16.84s/it]
 14%|█▎        | 271/2000 [1:16:48<8:04:57, 16.83s/it]
                                                      

 14%|█▎        | 271/2000 [1:16:48<8:04:57, 16.83s/it]
 14%|█▎        | 272/2000 [1:17:05<8:05:19, 16.85s/it]
                                                      

 14%|█▎        | 272/2000 [1:17:05<8:05:19, 16.85s/it]
 14%|█▎        | 273/2000 [1:17:22<8:04:23, 16.83s/it]
                                                      

 14%|█▎        | 273/2000 [1:17:22<8:04:23, 16.83s/it]
 14%|█▎        | 274/2000 [1:17:39<8:04:24, 16.84s/it]
                                                      

 14%|█▎        | 274/2000 [1:17:39<8:04:24, 16.84s/it]
 14%|█▍        | 275/2000 [1:17:56<8:04:03, 16.84s/it]
                                                      

 14%|█▍        | 275/2000 [1:17:56<8:04:03, 16.84s/it]
 14%|█▍        | 276/2000 [1:18:13<8:02:51, 16.80s/it]
                                                      

 14%|█▍        | 276/2000 [1:18:13<8:02:51, 16.80s/it]
 14%|█▍        | 277/2000 [1:18:30<8:05:07, 16.89s/it]
                                                      

 14%|█▍        | 277/2000 [1:18:30<8:05:07, 16.89s/it]
 14%|█▍        | 278/2000 [1:18:46<8:04:07, 16.87s/it]
                                                      

 14%|█▍        | 278/2000 [1:18:46<8:04:07, 16.87s/it]
 14%|█▍        | 279/2000 [1:19:03<8:03:43, 16.86s/it]
                                                      

 14%|█▍        | 279/2000 [1:19:03<8:03:43, 16.86s/it]
 14%|█▍        | 280/2000 [1:19:20<8:04:08, 16.89s/it]
                                                      

 14%|█▍        | 280/2000 [1:19:20<8:04:08, 16.89s/it]
 14%|█▍        | 281/2000 [1:19:37<8:03:02, 16.86s/it]
                                                      

 14%|█▍        | 281/2000 [1:19:37<8:03:02, 16.86s/it]
 14%|█▍        | 282/2000 [1:19:54<8:02:06, 16.84s/it]
                                                      

 14%|█▍        | 282/2000 [1:19:54<8:02:06, 16.84s/it]
 14%|█▍        | 283/2000 [1:20:11<8:01:14, 16.82s/it]
                                                      

 14%|█▍        | 283/2000 [1:20:11<8:01:14, 16.82s/it]
 14%|█▍        | 284/2000 [1:20:27<8:00:11, 16.79s/it]
                                                      

 14%|█▍        | 284/2000 [1:20:27<8:00:11, 16.79s/it]
 14%|█▍        | 285/2000 [1:20:44<8:00:31, 16.81s/it]
                                                      

 14%|█▍        | 285/2000 [1:20:44<8:00:31, 16.81s/it]
 14%|█▍        | 286/2000 [1:21:01<8:01:37, 16.86s/it]
                                                      

 14%|█▍        | 286/2000 [1:21:01<8:01:37, 16.86s/it]
 14%|█▍        | 287/2000 [1:21:18<8:01:12, 16.85s/it]
                                                      

 14%|█▍        | 287/2000 [1:21:18<8:01:12, 16.85s/it]
 14%|█▍        | 288/2000 [1:21:35<7:59:59, 16.82s/it]
                                                      

 14%|█▍        | 288/2000 [1:21:35<7:59:59, 16.82s/it]
 14%|█▍        | 289/2000 [1:21:52<8:06:28, 17.06s/it]
                                                      

 14%|█▍        | 289/2000 [1:21:52<8:06:28, 17.06s/it]
 14%|█▍        | 290/2000 [1:22:09<8:03:30, 16.97s/it]
                                                      

 14%|█▍        | 290/2000 [1:22:09<8:03:30, 16.97s/it]
 15%|█▍        | 291/2000 [1:22:26<8:04:11, 17.00s/it]
                                                      

 15%|█▍        | 291/2000 [1:22:26<8:04:11, 17.00s/it]
 15%|█▍        | 292/2000 [1:22:43<8:01:35, 16.92s/it]
                                                      

 15%|█▍        | 292/2000 [1:22:43<8:01:35, 16.92s/it]
 15%|█▍        | 293/2000 [1:23:00<8:00:14, 16.88s/it]
                                                      

 15%|█▍        | 293/2000 [1:23:00<8:00:14, 16.88s/it]
 15%|█▍        | 294/2000 [1:23:17<7:59:47, 16.87s/it]
                                                      

 15%|█▍        | 294/2000 [1:23:17<7:59:47, 16.87s/it]
 15%|█▍        | 295/2000 [1:23:33<7:59:53, 16.89s/it]
                                                      

 15%|█▍        | 295/2000 [1:23:33<7:59:53, 16.89s/it]
 15%|█▍        | 296/2000 [1:23:50<7:58:46, 16.86s/it]
                                                      

 15%|█▍        | 296/2000 [1:23:50<7:58:46, 16.86s/it]
 15%|█▍        | 297/2000 [1:24:07<7:58:29, 16.86s/it]
                                                      

 15%|█▍        | 297/2000 [1:24:07<7:58:29, 16.86s/it]
 15%|█▍        | 298/2000 [1:24:24<7:59:41, 16.91s/it]
                                                      

 15%|█▍        | 298/2000 [1:24:24<7:59:41, 16.91s/it]
 15%|█▍        | 299/2000 [1:24:41<7:59:12, 16.90s/it]
                                                      

 15%|█▍        | 299/2000 [1:24:41<7:59:12, 16.90s/it]
 15%|█▌        | 300/2000 [1:24:58<7:58:11, 16.88s/it]
                                                      

 15%|█▌        | 300/2000 [1:24:58<7:58:11, 16.88s/it]
 15%|█▌        | 301/2000 [1:25:15<7:58:04, 16.88s/it]
                                                      

 15%|█▌        | 301/2000 [1:25:15<7:58:04, 16.88s/it]
 15%|█▌        | 302/2000 [1:25:32<7:57:05, 16.86s/it]
                                                      

 15%|█▌        | 302/2000 [1:25:32<7:57:05, 16.86s/it]
 15%|█▌        | 303/2000 [1:25:48<7:56:31, 16.85s/it]
                                                      

 15%|█▌        | 303/2000 [1:25:48<7:56:31, 16.85s/it]
 15%|█▌        | 304/2000 [1:26:05<7:56:06, 16.84s/it]
                                                      

 15%|█▌        | 304/2000 [1:26:05<7:56:06, 16.84s/it]
 15%|█▌        | 305/2000 [1:26:22<7:55:11, 16.82s/it]
                                                      

 15%|█▌        | 305/2000 [1:26:22<7:55:11, 16.82s/it]
 15%|█▌        | 306/2000 [1:26:39<7:54:15, 16.80s/it]
                                                      

 15%|█▌        | 306/2000 [1:26:39<7:54:15, 16.80s/it]
 15%|█▌        | 307/2000 [1:26:55<7:53:43, 16.79s/it]
                                                      

 15%|█▌        | 307/2000 [1:26:55<7:53:43, 16.79s/it]
 15%|█▌        | 308/2000 [1:27:12<7:53:18, 16.78s/it]
                                                      

 15%|█▌        | 308/2000 [1:27:12<7:53:18, 16.78s/it]
 15%|█▌        | 309/2000 [1:27:29<7:52:52, 16.78s/it]
                                                      

 15%|█▌        | 309/2000 [1:27:29<7:52:52, 16.78s/it]
 16%|█▌        | 310/2000 [1:27:46<7:52:47, 16.79s/it]
                                                      

 16%|█▌        | 310/2000 [1:27:46<7:52:47, 16.79s/it]
 16%|█▌        | 311/2000 [1:28:03<7:52:39, 16.79s/it]
                                                      

 16%|█▌        | 311/2000 [1:28:03<7:52:39, 16.79s/it]
 16%|█▌        | 312/2000 [1:28:20<7:53:38, 16.84s/it]
                                                      

 16%|█▌        | 312/2000 [1:28:20<7:53:38, 16.84s/it]
 16%|█▌        | 313/2000 [1:28:36<7:53:09, 16.83s/it]
                                                      

 16%|█▌        | 313/2000 [1:28:36<7:53:09, 16.83s/it]
 16%|█▌        | 314/2000 [1:28:53<7:52:26, 16.81s/it]
                                                      

 16%|█▌        | 314/2000 [1:28:53<7:52:26, 16.81s/it]
 16%|█▌        | 315/2000 [1:29:10<7:52:51, 16.84s/it]
                                                      

 16%|█▌        | 315/2000 [1:29:10<7:52:51, 16.84s/it]
 16%|█▌        | 316/2000 [1:29:27<7:51:32, 16.80s/it]
                                                      

 16%|█▌        | 316/2000 [1:29:27<7:51:32, 16.80s/it]
 16%|█▌        | 317/2000 [1:29:44<7:51:23, 16.81s/it]
                                                      

 16%|█▌        | 317/2000 [1:29:44<7:51:23, 16.81s/it]
 16%|█▌        | 318/2000 [1:30:00<7:51:41, 16.83s/it]
                                                      

 16%|█▌        | 318/2000 [1:30:00<7:51:41, 16.83s/it]
 16%|█▌        | 319/2000 [1:30:17<7:50:51, 16.81s/it]
                                                      

 16%|█▌        | 319/2000 [1:30:17<7:50:51, 16.81s/it]
 16%|█▌        | 320/2000 [1:30:34<7:51:26, 16.84s/it]
                                                      

 16%|█▌        | 320/2000 [1:30:34<7:51:26, 16.84s/it]
 16%|█▌        | 321/2000 [1:30:53<8:11:56, 17.58s/it]
                                                      

 16%|█▌        | 321/2000 [1:30:53<8:11:56, 17.58s/it]
 16%|█▌        | 322/2000 [1:31:11<8:08:21, 17.46s/it]
                                                      

 16%|█▌        | 322/2000 [1:31:11<8:08:21, 17.46s/it]
 16%|█▌        | 323/2000 [1:31:27<8:02:35, 17.27s/it]
                                                      

 16%|█▌        | 323/2000 [1:31:27<8:02:35, 17.27s/it]
 16%|█▌        | 324/2000 [1:31:45<8:01:43, 17.25s/it]
                                                      

 16%|█▌        | 324/2000 [1:31:45<8:01:43, 17.25s/it]
 16%|█▋        | 325/2000 [1:32:02<7:59:32, 17.18s/it]
                                                      

 16%|█▋        | 325/2000 [1:32:02<7:59:32, 17.18s/it]
 16%|█▋        | 326/2000 [1:32:19<7:56:34, 17.08s/it]
                                                      

 16%|█▋        | 326/2000 [1:32:19<7:56:34, 17.08s/it]
 16%|█▋        | 327/2000 [1:32:35<7:53:28, 16.98s/it]
                                                      

 16%|█▋        | 327/2000 [1:32:35<7:53:28, 16.98s/it]
 16%|█▋        | 328/2000 [1:32:52<7:51:53, 16.93s/it]
                                                      

 16%|█▋        | 328/2000 [1:32:52<7:51:53, 16.93s/it]
 16%|█▋        | 329/2000 [1:33:09<7:50:43, 16.90s/it]
                                                      

 16%|█▋        | 329/2000 [1:33:09<7:50:43, 16.90s/it]
 16%|█▋        | 330/2000 [1:33:26<7:49:12, 16.86s/it]
                                                      

 16%|█▋        | 330/2000 [1:33:26<7:49:12, 16.86s/it]
 17%|█▋        | 331/2000 [1:33:43<7:56:26, 17.13s/it]
                                                      

 17%|█▋        | 331/2000 [1:33:43<7:56:26, 17.13s/it]
 17%|█▋        | 332/2000 [1:34:00<7:55:09, 17.09s/it]
                                                      

 17%|█▋        | 332/2000 [1:34:00<7:55:09, 17.09s/it]
 17%|█▋        | 333/2000 [1:34:17<7:52:49, 17.02s/it]
                                                      

 17%|█▋        | 333/2000 [1:34:17<7:52:49, 17.02s/it]
 17%|█▋        | 334/2000 [1:34:34<7:50:56, 16.96s/it]
                                                      

 17%|█▋        | 334/2000 [1:34:34<7:50:56, 16.96s/it]
 17%|█▋        | 335/2000 [1:34:51<7:49:28, 16.92s/it]
                                                      

 17%|█▋        | 335/2000 [1:34:51<7:49:28, 16.92s/it]
 17%|█▋        | 336/2000 [1:35:08<7:48:54, 16.91s/it]
                                                      

 17%|█▋        | 336/2000 [1:35:08<7:48:54, 16.91s/it]
 17%|█▋        | 337/2000 [1:35:25<7:48:59, 16.92s/it]
                                                      

 17%|█▋        | 337/2000 [1:35:25<7:48:59, 16.92s/it]
 17%|█▋        | 338/2000 [1:35:42<7:48:07, 16.90s/it]
                                                      

 17%|█▋        | 338/2000 [1:35:42<7:48:07, 16.90s/it]
 17%|█▋        | 339/2000 [1:35:58<7:47:19, 16.88s/it]
                                                      

 17%|█▋        | 339/2000 [1:35:58<7:47:19, 16.88s/it]
 17%|█▋        | 340/2000 [1:36:15<7:46:21, 16.86s/it]
                                                      

 17%|█▋        | 340/2000 [1:36:15<7:46:21, 16.86s/it]
 17%|█▋        | 341/2000 [1:36:32<7:45:27, 16.83s/it]
                                                      

 17%|█▋        | 341/2000 [1:36:32<7:45:27, 16.83s/it]
 17%|█▋        | 342/2000 [1:36:49<7:44:48, 16.82s/it]
                                                      

 17%|█▋        | 342/2000 [1:36:49<7:44:48, 16.82s/it]
 17%|█▋        | 343/2000 [1:37:06<7:44:41, 16.83s/it]
                                                      

 17%|█▋        | 343/2000 [1:37:06<7:44:41, 16.83s/it]
 17%|█▋        | 344/2000 [1:37:23<7:44:55, 16.84s/it]
                                                      

 17%|█▋        | 344/2000 [1:37:23<7:44:55, 16.84s/it]
 17%|█▋        | 345/2000 [1:37:40<7:45:30, 16.88s/it]
                                                      

 17%|█▋        | 345/2000 [1:37:40<7:45:30, 16.88s/it]
 17%|█▋        | 346/2000 [1:37:56<7:45:00, 16.87s/it]
                                                      

 17%|█▋        | 346/2000 [1:37:56<7:45:00, 16.87s/it]
 17%|█▋        | 347/2000 [1:38:13<7:44:49, 16.87s/it]
                                                      

 17%|█▋        | 347/2000 [1:38:13<7:44:49, 16.87s/it]
 17%|█▋        | 348/2000 [1:38:30<7:44:16, 16.86s/it]
                                                      

 17%|█▋        | 348/2000 [1:38:30<7:44:16, 16.86s/it]
 17%|█▋        | 349/2000 [1:38:47<7:43:36, 16.85s/it]
                                                      

 17%|█▋        | 349/2000 [1:38:47<7:43:36, 16.85s/it]
 18%|█▊        | 350/2000 [1:39:04<7:42:56, 16.83s/it]
                                                      

 18%|█▊        | 350/2000 [1:39:04<7:42:56, 16.83s/it]
 18%|█▊        | 351/2000 [1:39:21<7:42:55, 16.84s/it]
                                                      

 18%|█▊        | 351/2000 [1:39:21<7:42:55, 16.84s/it]
 18%|█▊        | 352/2000 [1:39:38<7:45:53, 16.96s/it]
                                                      

 18%|█▊        | 352/2000 [1:39:38<7:45:53, 16.96s/it]
 18%|█▊        | 353/2000 [1:39:55<7:44:41, 16.93s/it]
                                                      

 18%|█▊        | 353/2000 [1:39:55<7:44:41, 16.93s/it]
 18%|█▊        | 354/2000 [1:40:12<7:45:06, 16.95s/it]
                                                      

 18%|█▊        | 354/2000 [1:40:12<7:45:06, 16.95s/it]
 18%|█▊        | 355/2000 [1:40:29<7:45:11, 16.97s/it]
                                                      

 18%|█▊        | 355/2000 [1:40:29<7:45:11, 16.97s/it]
 18%|█▊        | 356/2000 [1:40:46<7:43:55, 16.93s/it]
                                                      

 18%|█▊        | 356/2000 [1:40:46<7:43:55, 16.93s/it]
 18%|█▊        | 357/2000 [1:41:02<7:43:04, 16.91s/it]
                                                      

 18%|█▊        | 357/2000 [1:41:02<7:43:04, 16.91s/it]
 18%|█▊        | 358/2000 [1:41:19<7:42:03, 16.88s/it]
                                                      

 18%|█▊        | 358/2000 [1:41:19<7:42:03, 16.88s/it]
 18%|█▊        | 359/2000 [1:41:36<7:42:09, 16.90s/it]
                                                      

 18%|█▊        | 359/2000 [1:41:36<7:42:09, 16.90s/it]
 18%|█▊        | 360/2000 [1:41:53<7:41:40, 16.89s/it]
                                                      

 18%|█▊        | 360/2000 [1:41:53<7:41:40, 16.89s/it]
 18%|█▊        | 361/2000 [1:42:10<7:41:28, 16.89s/it]
                                                      

 18%|█▊        | 361/2000 [1:42:10<7:41:28, 16.89s/it]
 18%|█▊        | 362/2000 [1:42:27<7:41:45, 16.91s/it]
                                                      

 18%|█▊        | 362/2000 [1:42:27<7:41:45, 16.91s/it]
 18%|█▊        | 363/2000 [1:42:44<7:40:23, 16.87s/it]
                                                      

 18%|█▊        | 363/2000 [1:42:44<7:40:23, 16.87s/it]
 18%|█▊        | 364/2000 [1:43:00<7:39:23, 16.85s/it]
                                                      

 18%|█▊        | 364/2000 [1:43:00<7:39:23, 16.85s/it]
 18%|█▊        | 365/2000 [1:43:18<7:45:38, 17.09s/it]
                                                      

 18%|█▊        | 365/2000 [1:43:18<7:45:38, 17.09s/it]
 18%|█▊        | 366/2000 [1:43:35<7:42:54, 17.00s/it]
                                                      

 18%|█▊        | 366/2000 [1:43:35<7:42:54, 17.00s/it]
 18%|█▊        | 367/2000 [1:43:52<7:42:46, 17.00s/it]
                                                      

 18%|█▊        | 367/2000 [1:43:52<7:42:46, 17.00s/it]
 18%|█▊        | 368/2000 [1:44:09<7:41:25, 16.96s/it]
                                                      

 18%|█▊        | 368/2000 [1:44:09<7:41:25, 16.96s/it]
 18%|█▊        | 369/2000 [1:44:26<7:43:15, 17.04s/it]
                                                      

 18%|█▊        | 369/2000 [1:44:26<7:43:15, 17.04s/it]
 18%|█▊        | 370/2000 [1:44:43<7:41:01, 16.97s/it]
                                                      

 18%|█▊        | 370/2000 [1:44:43<7:41:01, 16.97s/it]
 19%|█▊        | 371/2000 [1:45:00<7:41:46, 17.01s/it]
                                                      

 19%|█▊        | 371/2000 [1:45:00<7:41:46, 17.01s/it]
 19%|█▊        | 372/2000 [1:45:17<7:40:50, 16.98s/it]
                                                      

 19%|█▊        | 372/2000 [1:45:17<7:40:50, 16.98s/it]
 19%|█▊        | 373/2000 [1:45:34<7:42:49, 17.07s/it]
                                                      

 19%|█▊        | 373/2000 [1:45:34<7:42:49, 17.07s/it]
 19%|█▊        | 374/2000 [1:45:51<7:40:45, 17.00s/it]
                                                      

 19%|█▊        | 374/2000 [1:45:51<7:40:45, 17.00s/it]
 19%|█▉        | 375/2000 [1:46:08<7:41:13, 17.03s/it]
                                                      

 19%|█▉        | 375/2000 [1:46:08<7:41:13, 17.03s/it]
 19%|█▉        | 376/2000 [1:46:25<7:40:15, 17.00s/it]
                                                      

 19%|█▉        | 376/2000 [1:46:25<7:40:15, 17.00s/it]
 19%|█▉        | 377/2000 [1:46:42<7:39:47, 17.00s/it]
                                                      

 19%|█▉        | 377/2000 [1:46:42<7:39:47, 17.00s/it]
 19%|█▉        | 378/2000 [1:46:59<7:40:05, 17.02s/it]
                                                      

 19%|█▉        | 378/2000 [1:46:59<7:40:05, 17.02s/it]
 19%|█▉        | 379/2000 [1:47:16<7:38:01, 16.95s/it]
                                                      

 19%|█▉        | 379/2000 [1:47:16<7:38:01, 16.95s/it]
 19%|█▉        | 380/2000 [1:47:33<7:38:01, 16.96s/it]
                                                      

 19%|█▉        | 380/2000 [1:47:33<7:38:01, 16.96s/it]
 19%|█▉        | 381/2000 [1:47:50<7:36:08, 16.90s/it]
                                                      

 19%|█▉        | 381/2000 [1:47:50<7:36:08, 16.90s/it]
 19%|█▉        | 382/2000 [1:48:07<7:38:50, 17.02s/it]
                                                      

 19%|█▉        | 382/2000 [1:48:07<7:38:50, 17.02s/it]
 19%|█▉        | 383/2000 [1:48:24<7:38:38, 17.02s/it]
                                                      

 19%|█▉        | 383/2000 [1:48:24<7:38:38, 17.02s/it]
 19%|█▉        | 384/2000 [1:48:41<7:38:16, 17.02s/it]
                                                      

 19%|█▉        | 384/2000 [1:48:41<7:38:16, 17.02s/it]
 19%|█▉        | 385/2000 [1:48:58<7:36:04, 16.94s/it]
                                                      

 19%|█▉        | 385/2000 [1:48:58<7:36:04, 16.94s/it]
 19%|█▉        | 386/2000 [1:49:14<7:34:36, 16.90s/it]
                                                      

 19%|█▉        | 386/2000 [1:49:14<7:34:36, 16.90s/it]
 19%|█▉        | 387/2000 [1:49:31<7:33:57, 16.89s/it]
                                                      

 19%|█▉        | 387/2000 [1:49:31<7:33:57, 16.89s/it]
 19%|█▉        | 388/2000 [1:49:48<7:34:28, 16.92s/it]
                                                      

 19%|█▉        | 388/2000 [1:49:48<7:34:28, 16.92s/it]
 19%|█▉        | 389/2000 [1:50:05<7:33:31, 16.89s/it]
                                                      

 19%|█▉        | 389/2000 [1:50:05<7:33:31, 16.89s/it]
 20%|█▉        | 390/2000 [1:50:22<7:32:41, 16.87s/it]
                                                      

 20%|█▉        | 390/2000 [1:50:22<7:32:41, 16.87s/it]
 20%|█▉        | 391/2000 [1:50:39<7:33:10, 16.90s/it]
                                                      

 20%|█▉        | 391/2000 [1:50:39<7:33:10, 16.90s/it]
 20%|█▉        | 392/2000 [1:50:56<7:32:46, 16.89s/it]
                                                      

 20%|█▉        | 392/2000 [1:50:56<7:32:46, 16.89s/it]
 20%|█▉        | 393/2000 [1:51:13<7:31:24, 16.85s/it]
                                                      

 20%|█▉        | 393/2000 [1:51:13<7:31:24, 16.85s/it]
 20%|█▉        | 394/2000 [1:51:29<7:31:06, 16.85s/it]
                                                      

 20%|█▉        | 394/2000 [1:51:29<7:31:06, 16.85s/it]
 20%|█▉        | 395/2000 [1:51:46<7:30:27, 16.84s/it]
                                                      

 20%|█▉        | 395/2000 [1:51:46<7:30:27, 16.84s/it]
 20%|█▉        | 396/2000 [1:52:03<7:31:26, 16.89s/it]
                                                      

 20%|█▉        | 396/2000 [1:52:03<7:31:26, 16.89s/it]
 20%|█▉        | 397/2000 [1:52:20<7:29:47, 16.84s/it]
                                                      

 20%|█▉        | 397/2000 [1:52:20<7:29:47, 16.84s/it]
 20%|█▉        | 398/2000 [1:52:37<7:29:54, 16.85s/it]
                                                      

 20%|█▉        | 398/2000 [1:52:37<7:29:54, 16.85s/it]
 20%|█▉        | 399/2000 [1:52:54<7:30:10, 16.87s/it]
                                                      

 20%|█▉        | 399/2000 [1:52:54<7:30:10, 16.87s/it]
 20%|██        | 400/2000 [1:53:11<7:30:13, 16.88s/it]
                                                      

 20%|██        | 400/2000 [1:53:11<7:30:13, 16.88s/it]
 20%|██        | 401/2000 [1:53:30<7:51:47, 17.70s/it]
                                                      

 20%|██        | 401/2000 [1:53:30<7:51:47, 17.70s/it]
 20%|██        | 402/2000 [1:53:48<7:50:52, 17.68s/it]
                                                      

 20%|██        | 402/2000 [1:53:48<7:50:52, 17.68s/it]
 20%|██        | 403/2000 [1:54:05<7:46:16, 17.52s/it]
                                                      

 20%|██        | 403/2000 [1:54:05<7:46:16, 17.52s/it]
 20%|██        | 404/2000 [1:54:22<7:41:28, 17.35s/it]
                                                      

 20%|██        | 404/2000 [1:54:22<7:41:28, 17.35s/it]
 20%|██        | 405/2000 [1:54:39<7:38:11, 17.24s/it]
                                                      

 20%|██        | 405/2000 [1:54:39<7:38:11, 17.24s/it]
 20%|██        | 406/2000 [1:54:56<7:34:39, 17.11s/it]
                                                      

 20%|██        | 406/2000 [1:54:56<7:34:39, 17.11s/it]
 20%|██        | 407/2000 [1:55:13<7:32:41, 17.05s/it]
                                                      

 20%|██        | 407/2000 [1:55:13<7:32:41, 17.05s/it]
 20%|██        | 408/2000 [1:55:30<7:30:49, 16.99s/it]
                                                      

 20%|██        | 408/2000 [1:55:30<7:30:49, 16.99s/it]
 20%|██        | 409/2000 [1:55:46<7:30:22, 16.98s/it]
                                                      

 20%|██        | 409/2000 [1:55:46<7:30:22, 16.98s/it]
 20%|██        | 410/2000 [1:56:03<7:29:56, 16.98s/it]
                                                      

 20%|██        | 410/2000 [1:56:03<7:29:56, 16.98s/it]
 21%|██        | 411/2000 [1:56:20<7:28:23, 16.93s/it]
                                                      

 21%|██        | 411/2000 [1:56:20<7:28:23, 16.93s/it]
 21%|██        | 412/2000 [1:56:37<7:27:19, 16.90s/it]
                                                      

 21%|██        | 412/2000 [1:56:37<7:27:19, 16.90s/it]
 21%|██        | 413/2000 [1:56:54<7:26:46, 16.89s/it]
                                                      

 21%|██        | 413/2000 [1:56:54<7:26:46, 16.89s/it]
 21%|██        | 414/2000 [1:57:11<7:26:21, 16.89s/it]
                                                      

 21%|██        | 414/2000 [1:57:11<7:26:21, 16.89s/it]
 21%|██        | 415/2000 [1:57:28<7:25:48, 16.88s/it]
                                                      

 21%|██        | 415/2000 [1:57:28<7:25:48, 16.88s/it]
 21%|██        | 416/2000 [1:57:44<7:24:46, 16.85s/it]
                                                      

 21%|██        | 416/2000 [1:57:44<7:24:46, 16.85s/it]
 21%|██        | 417/2000 [1:58:02<7:26:04, 16.91s/it]
                                                      

 21%|██        | 417/2000 [1:58:02<7:26:04, 16.91s/it]
 21%|██        | 418/2000 [1:58:18<7:25:41, 16.90s/it]
                                                      

 21%|██        | 418/2000 [1:58:18<7:25:41, 16.90s/it]
 21%|██        | 419/2000 [1:58:35<7:25:11, 16.90s/it]
                                                      

 21%|██        | 419/2000 [1:58:35<7:25:11, 16.90s/it]
 21%|██        | 420/2000 [1:58:52<7:25:24, 16.91s/it]
                                                      

 21%|██        | 420/2000 [1:58:52<7:25:24, 16.91s/it]
 21%|██        | 421/2000 [1:59:09<7:26:30, 16.97s/it]
                                                      

 21%|██        | 421/2000 [1:59:09<7:26:30, 16.97s/it]
 21%|██        | 422/2000 [1:59:26<7:24:59, 16.92s/it]
                                                      

 21%|██        | 422/2000 [1:59:26<7:24:59, 16.92s/it]
 21%|██        | 423/2000 [1:59:44<7:31:29, 17.18s/it]
                                                      

 21%|██        | 423/2000 [1:59:44<7:31:29, 17.18s/it]
 21%|██        | 424/2000 [2:00:01<7:28:32, 17.08s/it]
                                                      

 21%|██        | 424/2000 [2:00:01<7:28:32, 17.08s/it]
 21%|██▏       | 425/2000 [2:00:18<7:26:33, 17.01s/it]
                                                      

 21%|██▏       | 425/2000 [2:00:18<7:26:33, 17.01s/it]
 21%|██▏       | 426/2000 [2:00:35<7:25:15, 16.97s/it]
                                                      

 21%|██▏       | 426/2000 [2:00:35<7:25:15, 16.97s/it]
 21%|██▏       | 427/2000 [2:00:51<7:24:58, 16.97s/it]
                                                      

 21%|██▏       | 427/2000 [2:00:52<7:24:58, 16.97s/it]
 21%|██▏       | 428/2000 [2:01:08<7:24:02, 16.95s/it]
                                                      

 21%|██▏       | 428/2000 [2:01:08<7:24:02, 16.95s/it]
 21%|██▏       | 429/2000 [2:01:25<7:22:59, 16.92s/it]
                                                      

 21%|██▏       | 429/2000 [2:01:25<7:22:59, 16.92s/it]
 22%|██▏       | 430/2000 [2:01:42<7:22:45, 16.92s/it]
                                                      

 22%|██▏       | 430/2000 [2:01:42<7:22:45, 16.92s/it]
 22%|██▏       | 431/2000 [2:01:59<7:22:46, 16.93s/it]
                                                      

 22%|██▏       | 431/2000 [2:01:59<7:22:46, 16.93s/it]
 22%|██▏       | 432/2000 [2:02:16<7:21:32, 16.90s/it]
                                                      

 22%|██▏       | 432/2000 [2:02:16<7:21:32, 16.90s/it]
 22%|██▏       | 433/2000 [2:02:33<7:21:18, 16.90s/it]
                                                      

 22%|██▏       | 433/2000 [2:02:33<7:21:18, 16.90s/it]
 22%|██▏       | 434/2000 [2:02:50<7:21:20, 16.91s/it]
                                                      

 22%|██▏       | 434/2000 [2:02:50<7:21:20, 16.91s/it]
 22%|██▏       | 435/2000 [2:03:07<7:20:41, 16.90s/it]
                                                      

 22%|██▏       | 435/2000 [2:03:07<7:20:41, 16.90s/it]
 22%|██▏       | 436/2000 [2:03:24<7:26:44, 17.14s/it]
                                                      

 22%|██▏       | 436/2000 [2:03:24<7:26:44, 17.14s/it]
 22%|██▏       | 437/2000 [2:03:41<7:24:19, 17.06s/it]
                                                      

 22%|██▏       | 437/2000 [2:03:41<7:24:19, 17.06s/it]
 22%|██▏       | 438/2000 [2:03:58<7:23:09, 17.02s/it]
                                                      

 22%|██▏       | 438/2000 [2:03:58<7:23:09, 17.02s/it]
 22%|██▏       | 439/2000 [2:04:15<7:22:40, 17.01s/it]
                                                      

 22%|██▏       | 439/2000 [2:04:15<7:22:40, 17.01s/it]
 22%|██▏       | 440/2000 [2:04:32<7:21:39, 16.99s/it]
                                                      

 22%|██▏       | 440/2000 [2:04:32<7:21:39, 16.99s/it]
 22%|██▏       | 441/2000 [2:04:49<7:20:03, 16.94s/it]
                                                      

 22%|██▏       | 441/2000 [2:04:49<7:20:03, 16.94s/it]
 22%|██▏       | 442/2000 [2:05:06<7:22:10, 17.03s/it]
                                                      

 22%|██▏       | 442/2000 [2:05:06<7:22:10, 17.03s/it]
 22%|██▏       | 443/2000 [2:05:23<7:21:06, 17.00s/it]
                                                      

 22%|██▏       | 443/2000 [2:05:23<7:21:06, 17.00s/it]
 22%|██▏       | 444/2000 [2:05:40<7:19:30, 16.95s/it]
                                                      

 22%|██▏       | 444/2000 [2:05:40<7:19:30, 16.95s/it]
 22%|██▏       | 445/2000 [2:05:57<7:18:25, 16.92s/it]
                                                      

 22%|██▏       | 445/2000 [2:05:57<7:18:25, 16.92s/it]
 22%|██▏       | 446/2000 [2:06:14<7:17:19, 16.89s/it]
                                                      

 22%|██▏       | 446/2000 [2:06:14<7:17:19, 16.89s/it]
 22%|██▏       | 447/2000 [2:06:30<7:16:56, 16.88s/it]
                                                      

 22%|██▏       | 447/2000 [2:06:30<7:16:56, 16.88s/it]
 22%|██▏       | 448/2000 [2:06:47<7:16:32, 16.88s/it]
                                                      

 22%|██▏       | 448/2000 [2:06:47<7:16:32, 16.88s/it]
 22%|██▏       | 449/2000 [2:07:05<7:23:21, 17.15s/it]
                                                      

 22%|██▏       | 449/2000 [2:07:05<7:23:21, 17.15s/it]
 22%|██▎       | 450/2000 [2:07:22<7:20:21, 17.05s/it]
                                                      

 22%|██▎       | 450/2000 [2:07:22<7:20:21, 17.05s/it]
 23%|██▎       | 451/2000 [2:07:40<7:25:57, 17.27s/it]
                                                      

 23%|██▎       | 451/2000 [2:07:40<7:25:57, 17.27s/it]
 23%|██▎       | 452/2000 [2:07:57<7:22:25, 17.15s/it]
                                                      

 23%|██▎       | 452/2000 [2:07:57<7:22:25, 17.15s/it]
 23%|██▎       | 453/2000 [2:08:13<7:20:34, 17.09s/it]
                                                      

 23%|██▎       | 453/2000 [2:08:13<7:20:34, 17.09s/it]
 23%|██▎       | 454/2000 [2:08:30<7:18:20, 17.01s/it]
                                                      

 23%|██▎       | 454/2000 [2:08:30<7:18:20, 17.01s/it]
 23%|██▎       | 455/2000 [2:08:47<7:17:10, 16.98s/it]
                                                      

 23%|██▎       | 455/2000 [2:08:47<7:17:10, 16.98s/it]
 23%|██▎       | 456/2000 [2:09:04<7:15:55, 16.94s/it]
                                                      

 23%|██▎       | 456/2000 [2:09:04<7:15:55, 16.94s/it]
 23%|██▎       | 457/2000 [2:09:21<7:14:59, 16.91s/it]
                                                      

 23%|██▎       | 457/2000 [2:09:21<7:14:59, 16.91s/it]
 23%|██▎       | 458/2000 [2:09:38<7:14:46, 16.92s/it]
                                                      

 23%|██▎       | 458/2000 [2:09:38<7:14:46, 16.92s/it]
 23%|██▎       | 459/2000 [2:09:55<7:14:04, 16.90s/it]
                                                      

 23%|██▎       | 459/2000 [2:09:55<7:14:04, 16.90s/it]
 23%|██▎       | 460/2000 [2:10:12<7:13:27, 16.89s/it]
                                                      

 23%|██▎       | 460/2000 [2:10:12<7:13:27, 16.89s/it]
 23%|██▎       | 461/2000 [2:10:29<7:13:56, 16.92s/it]
                                                      

 23%|██▎       | 461/2000 [2:10:29<7:13:56, 16.92s/it]
 23%|██▎       | 462/2000 [2:10:45<7:13:45, 16.92s/it]
                                                      

 23%|██▎       | 462/2000 [2:10:45<7:13:45, 16.92s/it]
 23%|██▎       | 463/2000 [2:11:02<7:13:16, 16.91s/it]
                                                      

 23%|██▎       | 463/2000 [2:11:02<7:13:16, 16.91s/it]
 23%|██▎       | 464/2000 [2:11:19<7:13:41, 16.94s/it]
                                                      

 23%|██▎       | 464/2000 [2:11:19<7:13:41, 16.94s/it]
 23%|██▎       | 465/2000 [2:11:36<7:12:42, 16.91s/it]
                                                      

 23%|██▎       | 465/2000 [2:11:36<7:12:42, 16.91s/it]
 23%|██▎       | 466/2000 [2:11:53<7:12:29, 16.92s/it]
                                                      

 23%|██▎       | 466/2000 [2:11:53<7:12:29, 16.92s/it]
 23%|██▎       | 467/2000 [2:12:10<7:11:21, 16.88s/it]
                                                      

 23%|██▎       | 467/2000 [2:12:10<7:11:21, 16.88s/it]
 23%|██▎       | 468/2000 [2:12:27<7:12:53, 16.95s/it]
                                                      

 23%|██▎       | 468/2000 [2:12:27<7:12:53, 16.95s/it]
 23%|██▎       | 469/2000 [2:12:44<7:11:38, 16.92s/it]
                                                      

 23%|██▎       | 469/2000 [2:12:44<7:11:38, 16.92s/it]
 24%|██▎       | 470/2000 [2:13:02<7:17:28, 17.16s/it]
                                                      

 24%|██▎       | 470/2000 [2:13:02<7:17:28, 17.16s/it]
 24%|██▎       | 471/2000 [2:13:18<7:14:49, 17.06s/it]
                                                      

 24%|██▎       | 471/2000 [2:13:18<7:14:49, 17.06s/it]
 24%|██▎       | 472/2000 [2:13:35<7:12:52, 17.00s/it]
                                                      

 24%|██▎       | 472/2000 [2:13:35<7:12:52, 17.00s/it]
 24%|██▎       | 473/2000 [2:13:52<7:11:32, 16.96s/it]
                                                      

 24%|██▎       | 473/2000 [2:13:52<7:11:32, 16.96s/it]
 24%|██▎       | 474/2000 [2:14:09<7:10:43, 16.94s/it]
                                                      

 24%|██▎       | 474/2000 [2:14:09<7:10:43, 16.94s/it]
 24%|██▍       | 475/2000 [2:14:26<7:09:21, 16.89s/it]
                                                      

 24%|██▍       | 475/2000 [2:14:26<7:09:21, 16.89s/it]
 24%|██▍       | 476/2000 [2:14:43<7:08:38, 16.88s/it]
                                                      

 24%|██▍       | 476/2000 [2:14:43<7:08:38, 16.88s/it]
 24%|██▍       | 477/2000 [2:14:59<7:07:47, 16.85s/it]
                                                      

 24%|██▍       | 477/2000 [2:14:59<7:07:47, 16.85s/it]
 24%|██▍       | 478/2000 [2:15:16<7:07:27, 16.85s/it]
                                                      

 24%|██▍       | 478/2000 [2:15:16<7:07:27, 16.85s/it]
 24%|██▍       | 479/2000 [2:15:33<7:07:21, 16.86s/it]
                                                      

 24%|██▍       | 479/2000 [2:15:33<7:07:21, 16.86s/it]
 24%|██▍       | 480/2000 [2:15:50<7:08:05, 16.90s/it]
                                                      

 24%|██▍       | 480/2000 [2:15:50<7:08:05, 16.90s/it]
 24%|██▍       | 481/2000 [2:16:09<7:24:06, 17.54s/it]
                                                      

 24%|██▍       | 481/2000 [2:16:09<7:24:06, 17.54s/it]
 24%|██▍       | 482/2000 [2:16:26<7:19:04, 17.36s/it]
                                                      

 24%|██▍       | 482/2000 [2:16:26<7:19:04, 17.36s/it]
 24%|██▍       | 483/2000 [2:16:43<7:14:57, 17.20s/it]
                                                      

 24%|██▍       | 483/2000 [2:16:43<7:14:57, 17.20s/it]
 24%|██▍       | 484/2000 [2:17:00<7:11:53, 17.09s/it]
                                                      

 24%|██▍       | 484/2000 [2:17:00<7:11:53, 17.09s/it]
 24%|██▍       | 485/2000 [2:17:17<7:09:30, 17.01s/it]
                                                      

 24%|██▍       | 485/2000 [2:17:17<7:09:30, 17.01s/it]
 24%|██▍       | 486/2000 [2:17:34<7:08:08, 16.97s/it]
                                                      

 24%|██▍       | 486/2000 [2:17:34<7:08:08, 16.97s/it]
 24%|██▍       | 487/2000 [2:17:50<7:06:46, 16.92s/it]
                                                      

 24%|██▍       | 487/2000 [2:17:50<7:06:46, 16.92s/it]
 24%|██▍       | 488/2000 [2:18:07<7:05:40, 16.89s/it]
                                                      

 24%|██▍       | 488/2000 [2:18:07<7:05:40, 16.89s/it]
 24%|██▍       | 489/2000 [2:18:24<7:04:42, 16.86s/it]
                                                      

 24%|██▍       | 489/2000 [2:18:24<7:04:42, 16.86s/it]
 24%|██▍       | 490/2000 [2:18:41<7:03:53, 16.84s/it]
                                                      

 24%|██▍       | 490/2000 [2:18:41<7:03:53, 16.84s/it]
 25%|██▍       | 491/2000 [2:18:58<7:03:36, 16.84s/it]
                                                      

 25%|██▍       | 491/2000 [2:18:58<7:03:36, 16.84s/it]
 25%|██▍       | 492/2000 [2:19:15<7:04:23, 16.89s/it]
                                                      

 25%|██▍       | 492/2000 [2:19:15<7:04:23, 16.89s/it]
 25%|██▍       | 493/2000 [2:19:32<7:05:06, 16.93s/it]
                                                      

 25%|██▍       | 493/2000 [2:19:32<7:05:06, 16.93s/it]
 25%|██▍       | 494/2000 [2:19:49<7:04:44, 16.92s/it]
                                                      

 25%|██▍       | 494/2000 [2:19:49<7:04:44, 16.92s/it]
 25%|██▍       | 495/2000 [2:20:05<7:04:16, 16.91s/it]
                                                      

 25%|██▍       | 495/2000 [2:20:05<7:04:16, 16.91s/it]
 25%|██▍       | 496/2000 [2:20:22<7:04:14, 16.92s/it]
                                                      

 25%|██▍       | 496/2000 [2:20:22<7:04:14, 16.92s/it]
 25%|██▍       | 497/2000 [2:20:39<7:03:24, 16.90s/it]
                                                      

 25%|██▍       | 497/2000 [2:20:39<7:03:24, 16.90s/it]
 25%|██▍       | 498/2000 [2:20:56<7:02:37, 16.88s/it]
                                                      

 25%|██▍       | 498/2000 [2:20:56<7:02:37, 16.88s/it]
 25%|██▍       | 499/2000 [2:21:13<7:02:04, 16.87s/it]
                                                      

 25%|██▍       | 499/2000 [2:21:13<7:02:04, 16.87s/it]
 25%|██▌       | 500/2000 [2:21:30<7:02:03, 16.88s/it]
                                                      

 25%|██▌       | 500/2000 [2:21:30<7:02:03, 16.88s/it]
 25%|██▌       | 501/2000 [2:21:47<7:01:25, 16.87s/it]
                                                      

 25%|██▌       | 501/2000 [2:21:47<7:01:25, 16.87s/it]
 25%|██▌       | 502/2000 [2:22:03<7:00:45, 16.85s/it]
                                                      

 25%|██▌       | 502/2000 [2:22:03<7:00:45, 16.85s/it]
 25%|██▌       | 503/2000 [2:22:20<7:01:19, 16.89s/it]
                                                      

 25%|██▌       | 503/2000 [2:22:20<7:01:19, 16.89s/it]
 25%|██▌       | 504/2000 [2:22:37<7:01:15, 16.90s/it]
                                                      

 25%|██▌       | 504/2000 [2:22:37<7:01:15, 16.90s/it]
 25%|██▌       | 505/2000 [2:22:54<7:00:24, 16.87s/it]
                                                      

 25%|██▌       | 505/2000 [2:22:54<7:00:24, 16.87s/it]
 25%|██▌       | 506/2000 [2:23:11<7:00:05, 16.87s/it]
                                                      

 25%|██▌       | 506/2000 [2:23:11<7:00:05, 16.87s/it]
 25%|██▌       | 507/2000 [2:23:28<7:00:41, 16.91s/it]
                                                      

 25%|██▌       | 507/2000 [2:23:28<7:00:41, 16.91s/it]
 25%|██▌       | 508/2000 [2:23:45<7:00:38, 16.92s/it]
                                                      

 25%|██▌       | 508/2000 [2:23:45<7:00:38, 16.92s/it]
 25%|██▌       | 509/2000 [2:24:02<6:59:35, 16.89s/it]
                                                      

 25%|██▌       | 509/2000 [2:24:02<6:59:35, 16.89s/it]
 26%|██▌       | 510/2000 [2:24:19<6:59:10, 16.88s/it]
                                                      

 26%|██▌       | 510/2000 [2:24:19<6:59:10, 16.88s/it]
 26%|██▌       | 511/2000 [2:24:35<6:56:39, 16.79s/it]
                                                      

 26%|██▌       | 511/2000 [2:24:35<6:56:39, 16.79s/it]
 26%|██▌       | 512/2000 [2:24:53<7:04:47, 17.13s/it]
                                                      

 26%|██▌       | 512/2000 [2:24:53<7:04:47, 17.13s/it]
 26%|██▌       | 513/2000 [2:25:10<7:01:33, 17.01s/it]
                                                      

 26%|██▌       | 513/2000 [2:25:10<7:01:33, 17.01s/it]
 26%|██▌       | 514/2000 [2:25:27<7:00:56, 17.00s/it]
                                                      

 26%|██▌       | 514/2000 [2:25:27<7:00:56, 17.00s/it]
 26%|██▌       | 515/2000 [2:25:44<6:59:21, 16.94s/it]
                                                      

 26%|██▌       | 515/2000 [2:25:44<6:59:21, 16.94s/it]
 26%|██▌       | 516/2000 [2:26:01<6:58:18, 16.91s/it]
                                                      

 26%|██▌       | 516/2000 [2:26:01<6:58:18, 16.91s/it]
 26%|██▌       | 517/2000 [2:26:17<6:57:25, 16.89s/it]
                                                      

 26%|██▌       | 517/2000 [2:26:17<6:57:25, 16.89s/it]
 26%|██▌       | 518/2000 [2:26:34<6:57:13, 16.89s/it]
                                                      

 26%|██▌       | 518/2000 [2:26:34<6:57:13, 16.89s/it]
 26%|██▌       | 519/2000 [2:26:51<6:57:17, 16.91s/it]
                                                      

 26%|██▌       | 519/2000 [2:26:51<6:57:17, 16.91s/it]
 26%|██▌       | 520/2000 [2:27:08<6:56:19, 16.88s/it]
                                                      

 26%|██▌       | 520/2000 [2:27:08<6:56:19, 16.88s/it]
 26%|██▌       | 521/2000 [2:27:25<6:56:01, 16.88s/it]
                                                      

 26%|██▌       | 521/2000 [2:27:25<6:56:01, 16.88s/it]
 26%|██▌       | 522/2000 [2:27:42<6:56:01, 16.89s/it]
                                                      

 26%|██▌       | 522/2000 [2:27:42<6:56:01, 16.89s/it]
 26%|██▌       | 523/2000 [2:27:59<6:54:55, 16.86s/it]
                                                      

 26%|██▌       | 523/2000 [2:27:59<6:54:55, 16.86s/it]
 26%|██▌       | 524/2000 [2:28:15<6:54:53, 16.87s/it]
                                                      

 26%|██▌       | 524/2000 [2:28:15<6:54:53, 16.87s/it]
 26%|██▋       | 525/2000 [2:28:32<6:54:37, 16.87s/it]
                                                      

 26%|██▋       | 525/2000 [2:28:32<6:54:37, 16.87s/it]
 26%|██▋       | 526/2000 [2:28:49<6:53:43, 16.84s/it]
                                                      

 26%|██▋       | 526/2000 [2:28:49<6:53:43, 16.84s/it]
 26%|██▋       | 527/2000 [2:29:06<6:53:43, 16.85s/it]
                                                      

 26%|██▋       | 527/2000 [2:29:06<6:53:43, 16.85s/it]
 26%|██▋       | 528/2000 [2:29:23<6:54:05, 16.88s/it]
                                                      

 26%|██▋       | 528/2000 [2:29:23<6:54:05, 16.88s/it]
 26%|██▋       | 529/2000 [2:29:40<6:53:27, 16.86s/it]
                                                      

 26%|██▋       | 529/2000 [2:29:40<6:53:27, 16.86s/it]
 26%|██▋       | 530/2000 [2:29:57<6:53:20, 16.87s/it]
                                                      

 26%|██▋       | 530/2000 [2:29:57<6:53:20, 16.87s/it]
 27%|██▋       | 531/2000 [2:30:14<6:53:42, 16.90s/it]
                                                      

 27%|██▋       | 531/2000 [2:30:14<6:53:42, 16.90s/it]
 27%|██▋       | 532/2000 [2:30:30<6:53:06, 16.88s/it]
                                                      

 27%|██▋       | 532/2000 [2:30:30<6:53:06, 16.88s/it]
 27%|██▋       | 533/2000 [2:30:47<6:53:42, 16.92s/it]
                                                      

 27%|██▋       | 533/2000 [2:30:47<6:53:42, 16.92s/it]
 27%|██▋       | 534/2000 [2:31:04<6:53:51, 16.94s/it]
                                                      

 27%|██▋       | 534/2000 [2:31:04<6:53:51, 16.94s/it]
 27%|██▋       | 535/2000 [2:31:21<6:53:38, 16.94s/it]
                                                      

 27%|██▋       | 535/2000 [2:31:21<6:53:38, 16.94s/it]
 27%|██▋       | 536/2000 [2:31:38<6:52:55, 16.92s/it]
                                                      

 27%|██▋       | 536/2000 [2:31:38<6:52:55, 16.92s/it]
 27%|██▋       | 537/2000 [2:31:55<6:52:25, 16.91s/it]
                                                      

 27%|██▋       | 537/2000 [2:31:55<6:52:25, 16.91s/it]
 27%|██▋       | 538/2000 [2:32:12<6:52:40, 16.94s/it]
                                                      

 27%|██▋       | 538/2000 [2:32:12<6:52:40, 16.94s/it]
 27%|██▋       | 539/2000 [2:32:29<6:51:40, 16.91s/it]
                                                      

 27%|██▋       | 539/2000 [2:32:29<6:51:40, 16.91s/it]
 27%|██▋       | 540/2000 [2:32:46<6:51:09, 16.90s/it]
                                                      

 27%|██▋       | 540/2000 [2:32:46<6:51:09, 16.90s/it]
 27%|██▋       | 541/2000 [2:33:03<6:49:58, 16.86s/it]
                                                      

 27%|██▋       | 541/2000 [2:33:03<6:49:58, 16.86s/it]
 27%|██▋       | 542/2000 [2:33:20<6:50:20, 16.89s/it]
                                                      

 27%|██▋       | 542/2000 [2:33:20<6:50:20, 16.89s/it]
 27%|██▋       | 543/2000 [2:33:36<6:49:37, 16.87s/it]
                                                      

 27%|██▋       | 543/2000 [2:33:36<6:49:37, 16.87s/it]
 27%|██▋       | 544/2000 [2:33:53<6:50:43, 16.93s/it]
                                                      

 27%|██▋       | 544/2000 [2:33:53<6:50:43, 16.93s/it]
 27%|██▋       | 545/2000 [2:34:10<6:49:00, 16.87s/it]
                                                      

 27%|██▋       | 545/2000 [2:34:10<6:49:00, 16.87s/it]
 27%|██▋       | 546/2000 [2:34:28<6:55:03, 17.13s/it]
                                                      

 27%|██▋       | 546/2000 [2:34:28<6:55:03, 17.13s/it]
 27%|██▋       | 547/2000 [2:34:45<6:52:18, 17.03s/it]
                                                      

 27%|██▋       | 547/2000 [2:34:45<6:52:18, 17.03s/it]
 27%|██▋       | 548/2000 [2:35:02<6:51:10, 16.99s/it]
                                                      

 27%|██▋       | 548/2000 [2:35:02<6:51:10, 16.99s/it]
 27%|██▋       | 549/2000 [2:35:18<6:49:32, 16.93s/it]
                                                      

 27%|██▋       | 549/2000 [2:35:18<6:49:32, 16.93s/it]
 28%|██▊       | 550/2000 [2:35:35<6:48:55, 16.92s/it]
                                                      

 28%|██▊       | 550/2000 [2:35:35<6:48:55, 16.92s/it]
 28%|██▊       | 551/2000 [2:35:52<6:47:58, 16.89s/it]
                                                      

 28%|██▊       | 551/2000 [2:35:52<6:47:58, 16.89s/it]
 28%|██▊       | 552/2000 [2:36:09<6:49:16, 16.96s/it]
                                                      

 28%|██▊       | 552/2000 [2:36:09<6:49:16, 16.96s/it]
 28%|██▊       | 553/2000 [2:36:26<6:47:53, 16.91s/it]
                                                      

 28%|██▊       | 553/2000 [2:36:26<6:47:53, 16.91s/it]
 28%|██▊       | 554/2000 [2:36:43<6:47:11, 16.90s/it]
                                                      

 28%|██▊       | 554/2000 [2:36:43<6:47:11, 16.90s/it]
 28%|██▊       | 555/2000 [2:37:00<6:46:38, 16.88s/it]
                                                      

 28%|██▊       | 555/2000 [2:37:00<6:46:38, 16.88s/it]
 28%|██▊       | 556/2000 [2:37:17<6:46:25, 16.89s/it]
                                                      

 28%|██▊       | 556/2000 [2:37:17<6:46:25, 16.89s/it]
 28%|██▊       | 557/2000 [2:37:34<6:46:02, 16.88s/it]
                                                      

 28%|██▊       | 557/2000 [2:37:34<6:46:02, 16.88s/it]
 28%|██▊       | 558/2000 [2:37:50<6:45:09, 16.86s/it]
                                                      

 28%|██▊       | 558/2000 [2:37:50<6:45:09, 16.86s/it]
 28%|██▊       | 559/2000 [2:38:07<6:44:43, 16.85s/it]
                                                      

 28%|██▊       | 559/2000 [2:38:07<6:44:43, 16.85s/it]
 28%|██▊       | 560/2000 [2:38:24<6:44:23, 16.85s/it]
                                                      

 28%|██▊       | 560/2000 [2:38:24<6:44:23, 16.85s/it]
 28%|██▊       | 561/2000 [2:38:44<7:03:17, 17.65s/it]
                                                      

 28%|██▊       | 561/2000 [2:38:44<7:03:17, 17.65s/it]
 28%|██▊       | 562/2000 [2:39:01<6:59:36, 17.51s/it]
                                                      

 28%|██▊       | 562/2000 [2:39:01<6:59:36, 17.51s/it]
 28%|██▊       | 563/2000 [2:39:18<6:55:17, 17.34s/it]
                                                      

 28%|██▊       | 563/2000 [2:39:18<6:55:17, 17.34s/it]
 28%|██▊       | 564/2000 [2:39:35<6:51:57, 17.21s/it]
                                                      

 28%|██▊       | 564/2000 [2:39:35<6:51:57, 17.21s/it]
 28%|██▊       | 565/2000 [2:39:51<6:49:27, 17.12s/it]
                                                      

 28%|██▊       | 565/2000 [2:39:51<6:49:27, 17.12s/it]
 28%|██▊       | 566/2000 [2:40:08<6:47:41, 17.06s/it]
                                                      

 28%|██▊       | 566/2000 [2:40:08<6:47:41, 17.06s/it]
 28%|██▊       | 567/2000 [2:40:25<6:46:00, 17.00s/it]
                                                      

 28%|██▊       | 567/2000 [2:40:25<6:46:00, 17.00s/it]
 28%|██▊       | 568/2000 [2:40:42<6:45:10, 16.98s/it]
                                                      

 28%|██▊       | 568/2000 [2:40:42<6:45:10, 16.98s/it]
 28%|██▊       | 569/2000 [2:40:59<6:44:42, 16.97s/it]
                                                      

 28%|██▊       | 569/2000 [2:40:59<6:44:42, 16.97s/it]
 28%|██▊       | 570/2000 [2:41:16<6:44:39, 16.98s/it]
                                                      

 28%|██▊       | 570/2000 [2:41:16<6:44:39, 16.98s/it]
 29%|██▊       | 571/2000 [2:41:33<6:44:34, 16.99s/it]
                                                      

 29%|██▊       | 571/2000 [2:41:33<6:44:34, 16.99s/it]
 29%|██▊       | 572/2000 [2:41:50<6:42:36, 16.92s/it]
                                                      

 29%|██▊       | 572/2000 [2:41:50<6:42:36, 16.92s/it]
 29%|██▊       | 573/2000 [2:42:07<6:41:57, 16.90s/it]
                                                      

 29%|██▊       | 573/2000 [2:42:07<6:41:57, 16.90s/it]
 29%|██▊       | 574/2000 [2:42:24<6:41:13, 16.88s/it]
                                                      

 29%|██▊       | 574/2000 [2:42:24<6:41:13, 16.88s/it]
 29%|██▉       | 575/2000 [2:42:41<6:43:17, 16.98s/it]
                                                      

 29%|██▉       | 575/2000 [2:42:41<6:43:17, 16.98s/it]
 29%|██▉       | 576/2000 [2:42:58<6:42:31, 16.96s/it]
                                                      

 29%|██▉       | 576/2000 [2:42:58<6:42:31, 16.96s/it]
 29%|██▉       | 577/2000 [2:43:15<6:47:09, 17.17s/it]
                                                      

 29%|██▉       | 577/2000 [2:43:15<6:47:09, 17.17s/it]
 29%|██▉       | 578/2000 [2:43:33<6:47:59, 17.21s/it]
                                                      

 29%|██▉       | 578/2000 [2:43:33<6:47:59, 17.21s/it]
 29%|██▉       | 579/2000 [2:43:50<6:45:28, 17.12s/it]
                                                      

 29%|██▉       | 579/2000 [2:43:50<6:45:28, 17.12s/it]
 29%|██▉       | 580/2000 [2:44:07<6:45:00, 17.11s/it]
                                                      

 29%|██▉       | 580/2000 [2:44:07<6:45:00, 17.11s/it]
 29%|██▉       | 581/2000 [2:44:23<6:42:29, 17.02s/it]
                                                      

 29%|██▉       | 581/2000 [2:44:23<6:42:29, 17.02s/it]
 29%|██▉       | 582/2000 [2:44:40<6:40:30, 16.95s/it]
                                                      

 29%|██▉       | 582/2000 [2:44:40<6:40:30, 16.95s/it]
 29%|██▉       | 583/2000 [2:44:57<6:39:19, 16.91s/it]
                                                      

 29%|██▉       | 583/2000 [2:44:57<6:39:19, 16.91s/it]
 29%|██▉       | 584/2000 [2:45:14<6:38:58, 16.91s/it]
                                                      

 29%|██▉       | 584/2000 [2:45:14<6:38:58, 16.91s/it]
 29%|██▉       | 585/2000 [2:45:31<6:39:50, 16.95s/it]
                                                      

 29%|██▉       | 585/2000 [2:45:31<6:39:50, 16.95s/it]
 29%|██▉       | 586/2000 [2:45:48<6:39:43, 16.96s/it]
                                                      

 29%|██▉       | 586/2000 [2:45:48<6:39:43, 16.96s/it]
 29%|██▉       | 587/2000 [2:46:05<6:38:47, 16.93s/it]
                                                      

 29%|██▉       | 587/2000 [2:46:05<6:38:47, 16.93s/it]
 29%|██▉       | 588/2000 [2:46:22<6:39:03, 16.96s/it]
                                                      

 29%|██▉       | 588/2000 [2:46:22<6:39:03, 16.96s/it]
 29%|██▉       | 589/2000 [2:46:39<6:38:23, 16.94s/it]
                                                      

 29%|██▉       | 589/2000 [2:46:39<6:38:23, 16.94s/it]
 30%|██▉       | 590/2000 [2:46:56<6:38:15, 16.95s/it]
                                                      

 30%|██▉       | 590/2000 [2:46:56<6:38:15, 16.95s/it]
 30%|██▉       | 591/2000 [2:47:13<6:39:19, 17.00s/it]
                                                      

 30%|██▉       | 591/2000 [2:47:13<6:39:19, 17.00s/it]
 30%|██▉       | 592/2000 [2:47:30<6:38:44, 16.99s/it]
                                                      

 30%|██▉       | 592/2000 [2:47:30<6:38:44, 16.99s/it]
 30%|██▉       | 593/2000 [2:47:47<6:39:03, 17.02s/it]
                                                      

 30%|██▉       | 593/2000 [2:47:47<6:39:03, 17.02s/it]
 30%|██▉       | 594/2000 [2:48:04<6:37:28, 16.96s/it]
                                                      

 30%|██▉       | 594/2000 [2:48:04<6:37:28, 16.96s/it]
 30%|██▉       | 595/2000 [2:48:21<6:37:10, 16.96s/it]
                                                      

 30%|██▉       | 595/2000 [2:48:21<6:37:10, 16.96s/it]
 30%|██▉       | 596/2000 [2:48:38<6:36:38, 16.95s/it]
                                                      

 30%|██▉       | 596/2000 [2:48:38<6:36:38, 16.95s/it]
 30%|██▉       | 597/2000 [2:48:55<6:35:40, 16.92s/it]
                                                      

 30%|██▉       | 597/2000 [2:48:55<6:35:40, 16.92s/it]
 30%|██▉       | 598/2000 [2:49:11<6:35:03, 16.91s/it]
                                                      

 30%|██▉       | 598/2000 [2:49:11<6:35:03, 16.91s/it]
 30%|██▉       | 599/2000 [2:49:28<6:34:40, 16.90s/it]
                                                      

 30%|██▉       | 599/2000 [2:49:28<6:34:40, 16.90s/it]
 30%|███       | 600/2000 [2:49:45<6:34:08, 16.89s/it]
                                                      

 30%|███       | 600/2000 [2:49:45<6:34:08, 16.89s/it]
 30%|███       | 601/2000 [2:50:02<6:33:35, 16.88s/it]
                                                      

 30%|███       | 601/2000 [2:50:02<6:33:35, 16.88s/it]
 30%|███       | 602/2000 [2:50:19<6:34:01, 16.91s/it]
                                                      

 30%|███       | 602/2000 [2:50:19<6:34:01, 16.91s/it]
 30%|███       | 603/2000 [2:50:36<6:33:52, 16.92s/it]
                                                      

 30%|███       | 603/2000 [2:50:36<6:33:52, 16.92s/it]
 30%|███       | 604/2000 [2:50:53<6:35:06, 16.98s/it]
                                                      

 30%|███       | 604/2000 [2:50:53<6:35:06, 16.98s/it]
 30%|███       | 605/2000 [2:51:10<6:34:02, 16.95s/it]
                                                      

 30%|███       | 605/2000 [2:51:10<6:34:02, 16.95s/it]
 30%|███       | 606/2000 [2:51:27<6:34:20, 16.97s/it]
                                                      

 30%|███       | 606/2000 [2:51:27<6:34:20, 16.97s/it]
 30%|███       | 607/2000 [2:51:44<6:34:40, 17.00s/it]
                                                      

 30%|███       | 607/2000 [2:51:44<6:34:40, 17.00s/it]
 30%|███       | 608/2000 [2:52:01<6:34:20, 17.00s/it]
                                                      

 30%|███       | 608/2000 [2:52:01<6:34:20, 17.00s/it]
 30%|███       | 609/2000 [2:52:18<6:33:17, 16.96s/it]
                                                      

 30%|███       | 609/2000 [2:52:18<6:33:17, 16.96s/it]
 30%|███       | 610/2000 [2:52:35<6:33:18, 16.98s/it]
                                                      

 30%|███       | 610/2000 [2:52:35<6:33:18, 16.98s/it]
 31%|███       | 611/2000 [2:52:52<6:32:25, 16.95s/it]
                                                      

 31%|███       | 611/2000 [2:52:52<6:32:25, 16.95s/it]
 31%|███       | 612/2000 [2:53:09<6:33:17, 17.00s/it]
                                                      

 31%|███       | 612/2000 [2:53:09<6:33:17, 17.00s/it]
 31%|███       | 613/2000 [2:53:26<6:32:26, 16.98s/it]
                                                      

 31%|███       | 613/2000 [2:53:26<6:32:26, 16.98s/it]
 31%|███       | 614/2000 [2:53:43<6:31:46, 16.96s/it]
                                                      

 31%|███       | 614/2000 [2:53:43<6:31:46, 16.96s/it]
 31%|███       | 615/2000 [2:54:00<6:31:59, 16.98s/it]
                                                      

 31%|███       | 615/2000 [2:54:00<6:31:59, 16.98s/it]
 31%|███       | 616/2000 [2:54:17<6:30:37, 16.93s/it]
                                                      

 31%|███       | 616/2000 [2:54:17<6:30:37, 16.93s/it]
 31%|███       | 617/2000 [2:54:33<6:29:47, 16.91s/it]
                                                      

 31%|███       | 617/2000 [2:54:33<6:29:47, 16.91s/it]
 31%|███       | 618/2000 [2:54:50<6:28:38, 16.87s/it]
                                                      

 31%|███       | 618/2000 [2:54:50<6:28:38, 16.87s/it]
 31%|███       | 619/2000 [2:55:07<6:28:05, 16.86s/it]
                                                      

 31%|███       | 619/2000 [2:55:07<6:28:05, 16.86s/it]
 31%|███       | 620/2000 [2:55:24<6:28:10, 16.88s/it]
                                                      

 31%|███       | 620/2000 [2:55:24<6:28:10, 16.88s/it]
 31%|███       | 621/2000 [2:55:41<6:28:33, 16.91s/it]
                                                      

 31%|███       | 621/2000 [2:55:41<6:28:33, 16.91s/it]
 31%|███       | 622/2000 [2:55:58<6:28:56, 16.93s/it]
                                                      

 31%|███       | 622/2000 [2:55:58<6:28:56, 16.93s/it]
 31%|███       | 623/2000 [2:56:15<6:28:24, 16.92s/it]
                                                      

 31%|███       | 623/2000 [2:56:15<6:28:24, 16.92s/it]
 31%|███       | 624/2000 [2:56:32<6:28:36, 16.95s/it]
                                                      

 31%|███       | 624/2000 [2:56:32<6:28:36, 16.95s/it]
 31%|███▏      | 625/2000 [2:56:49<6:29:40, 17.00s/it]
                                                      

 31%|███▏      | 625/2000 [2:56:49<6:29:40, 17.00s/it]
 31%|███▏      | 626/2000 [2:57:06<6:28:45, 16.98s/it]
                                                      

 31%|███▏      | 626/2000 [2:57:06<6:28:45, 16.98s/it]
 31%|███▏      | 627/2000 [2:57:23<6:28:43, 16.99s/it]
                                                      

 31%|███▏      | 627/2000 [2:57:23<6:28:43, 16.99s/it]
 31%|███▏      | 628/2000 [2:57:40<6:29:29, 17.03s/it]
                                                      

 31%|███▏      | 628/2000 [2:57:40<6:29:29, 17.03s/it]
 31%|███▏      | 629/2000 [2:57:57<6:27:36, 16.96s/it]
                                                      

 31%|███▏      | 629/2000 [2:57:57<6:27:36, 16.96s/it]
 32%|███▏      | 630/2000 [2:58:14<6:28:16, 17.00s/it]
                                                      

 32%|███▏      | 630/2000 [2:58:14<6:28:16, 17.00s/it]
 32%|███▏      | 631/2000 [2:58:31<6:26:37, 16.94s/it]
                                                      

 32%|███▏      | 631/2000 [2:58:31<6:26:37, 16.94s/it]
 32%|███▏      | 632/2000 [2:58:48<6:25:56, 16.93s/it]
                                                      

 32%|███▏      | 632/2000 [2:58:48<6:25:56, 16.93s/it]
 32%|███▏      | 633/2000 [2:59:04<6:24:46, 16.89s/it]
                                                      

 32%|███▏      | 633/2000 [2:59:04<6:24:46, 16.89s/it]
 32%|███▏      | 634/2000 [2:59:21<6:24:37, 16.89s/it]
                                                      

 32%|███▏      | 634/2000 [2:59:21<6:24:37, 16.89s/it]
 32%|███▏      | 635/2000 [2:59:38<6:24:42, 16.91s/it]
                                                      

 32%|███▏      | 635/2000 [2:59:38<6:24:42, 16.91s/it]
 32%|███▏      | 636/2000 [2:59:55<6:25:29, 16.96s/it]
                                                      

 32%|███▏      | 636/2000 [2:59:55<6:25:29, 16.96s/it]
 32%|███▏      | 637/2000 [3:00:12<6:24:34, 16.93s/it]
                                                      

 32%|███▏      | 637/2000 [3:00:12<6:24:34, 16.93s/it]
 32%|███▏      | 638/2000 [3:00:29<6:25:29, 16.98s/it]
                                                      

 32%|███▏      | 638/2000 [3:00:29<6:25:29, 16.98s/it]
 32%|███▏      | 639/2000 [3:00:46<6:24:09, 16.94s/it]
                                                      

 32%|███▏      | 639/2000 [3:00:46<6:24:09, 16.94s/it]
 32%|███▏      | 640/2000 [3:01:03<6:24:15, 16.95s/it]
                                                      

 32%|███▏      | 640/2000 [3:01:03<6:24:15, 16.95s/it]
 32%|███▏      | 641/2000 [3:01:23<6:42:31, 17.77s/it]
                                                      

 32%|███▏      | 641/2000 [3:01:23<6:42:31, 17.77s/it]
 32%|███▏      | 642/2000 [3:01:41<6:42:59, 17.81s/it]
                                                      

 32%|███▏      | 642/2000 [3:01:41<6:42:59, 17.81s/it]
 32%|███▏      | 643/2000 [3:01:58<6:36:43, 17.54s/it]
                                                      

 32%|███▏      | 643/2000 [3:01:58<6:36:43, 17.54s/it]
 32%|███▏      | 644/2000 [3:02:15<6:32:02, 17.35s/it]
                                                      

 32%|███▏      | 644/2000 [3:02:15<6:32:02, 17.35s/it]
 32%|███▏      | 645/2000 [3:02:32<6:30:04, 17.27s/it]
                                                      

 32%|███▏      | 645/2000 [3:02:32<6:30:04, 17.27s/it]
 32%|███▏      | 646/2000 [3:02:49<6:29:00, 17.24s/it]
                                                      

 32%|███▏      | 646/2000 [3:02:49<6:29:00, 17.24s/it]
 32%|███▏      | 647/2000 [3:03:06<6:26:46, 17.15s/it]
                                                      

 32%|███▏      | 647/2000 [3:03:06<6:26:46, 17.15s/it]
 32%|███▏      | 648/2000 [3:03:23<6:24:32, 17.07s/it]
                                                      

 32%|███▏      | 648/2000 [3:03:23<6:24:32, 17.07s/it]
 32%|███▏      | 649/2000 [3:03:40<6:23:56, 17.05s/it]
                                                      

 32%|███▏      | 649/2000 [3:03:40<6:23:56, 17.05s/it]
 32%|███▎      | 650/2000 [3:03:56<6:22:12, 16.99s/it]
                                                      

 32%|███▎      | 650/2000 [3:03:57<6:22:12, 16.99s/it]
 33%|███▎      | 651/2000 [3:04:14<6:26:44, 17.20s/it]
                                                      

 33%|███▎      | 651/2000 [3:04:14<6:26:44, 17.20s/it]
 33%|███▎      | 652/2000 [3:04:31<6:23:27, 17.07s/it]
                                                      

 33%|███▎      | 652/2000 [3:04:31<6:23:27, 17.07s/it]
 33%|███▎      | 653/2000 [3:04:48<6:21:42, 17.00s/it]
                                                      

 33%|███▎      | 653/2000 [3:04:48<6:21:42, 17.00s/it]
 33%|███▎      | 654/2000 [3:05:05<6:20:27, 16.96s/it]
                                                      

 33%|███▎      | 654/2000 [3:05:05<6:20:27, 16.96s/it]
 33%|███▎      | 655/2000 [3:05:22<6:24:54, 17.17s/it]
                                                      

 33%|███▎      | 655/2000 [3:05:22<6:24:54, 17.17s/it]
 33%|███▎      | 656/2000 [3:05:39<6:21:48, 17.04s/it]
                                                      

 33%|███▎      | 656/2000 [3:05:39<6:21:48, 17.04s/it]
 33%|███▎      | 657/2000 [3:05:56<6:21:14, 17.03s/it]
                                                      

 33%|███▎      | 657/2000 [3:05:56<6:21:14, 17.03s/it]
 33%|███▎      | 658/2000 [3:06:13<6:21:00, 17.03s/it]
                                                      

 33%|███▎      | 658/2000 [3:06:13<6:21:00, 17.03s/it]
 33%|███▎      | 659/2000 [3:06:30<6:19:54, 17.00s/it]
                                                      

 33%|███▎      | 659/2000 [3:06:30<6:19:54, 17.00s/it]
 33%|███▎      | 660/2000 [3:06:47<6:18:52, 16.96s/it]
                                                      

 33%|███▎      | 660/2000 [3:06:47<6:18:52, 16.96s/it]
 33%|███▎      | 661/2000 [3:07:04<6:18:28, 16.96s/it]
                                                      

 33%|███▎      | 661/2000 [3:07:04<6:18:28, 16.96s/it]
 33%|███▎      | 662/2000 [3:07:21<6:18:59, 17.00s/it]
                                                      

 33%|███▎      | 662/2000 [3:07:21<6:18:59, 17.00s/it]
 33%|███▎      | 663/2000 [3:07:38<6:18:12, 16.97s/it]
                                                      

 33%|███▎      | 663/2000 [3:07:38<6:18:12, 16.97s/it]
 33%|███▎      | 664/2000 [3:07:55<6:16:58, 16.93s/it]
                                                      

 33%|███▎      | 664/2000 [3:07:55<6:16:58, 16.93s/it]
 33%|███▎      | 665/2000 [3:08:12<6:17:26, 16.96s/it]
                                                      

 33%|███▎      | 665/2000 [3:08:12<6:17:26, 16.96s/it]
 33%|███▎      | 666/2000 [3:08:29<6:16:49, 16.95s/it]
                                                      

 33%|███▎      | 666/2000 [3:08:29<6:16:49, 16.95s/it]
 33%|███▎      | 667/2000 [3:08:45<6:15:33, 16.90s/it]
                                                      

 33%|███▎      | 667/2000 [3:08:45<6:15:33, 16.90s/it]
 33%|███▎      | 668/2000 [3:09:02<6:14:23, 16.86s/it]
                                                      

 33%|███▎      | 668/2000 [3:09:02<6:14:23, 16.86s/it]
 33%|███▎      | 669/2000 [3:09:19<6:14:13, 16.87s/it]
                                                      

 33%|███▎      | 669/2000 [3:09:19<6:14:13, 16.87s/it]
 34%|███▎      | 670/2000 [3:09:36<6:14:37, 16.90s/it]
                                                      

 34%|███▎      | 670/2000 [3:09:36<6:14:37, 16.90s/it]
 34%|███▎      | 671/2000 [3:09:53<6:13:45, 16.87s/it]
                                                      

 34%|███▎      | 671/2000 [3:09:53<6:13:45, 16.87s/it]
 34%|███▎      | 672/2000 [3:10:10<6:13:23, 16.87s/it]
                                                      

 34%|███▎      | 672/2000 [3:10:10<6:13:23, 16.87s/it]
 34%|███▎      | 673/2000 [3:10:27<6:12:50, 16.86s/it]
                                                      

 34%|███▎      | 673/2000 [3:10:27<6:12:50, 16.86s/it]
 34%|███▎      | 674/2000 [3:10:43<6:11:57, 16.83s/it]
                                                      

 34%|███▎      | 674/2000 [3:10:43<6:11:57, 16.83s/it]
 34%|███▍      | 675/2000 [3:11:00<6:11:41, 16.83s/it]
                                                      

 34%|███▍      | 675/2000 [3:11:00<6:11:41, 16.83s/it]
 34%|███▍      | 676/2000 [3:11:17<6:12:19, 16.87s/it]
                                                      

 34%|███▍      | 676/2000 [3:11:17<6:12:19, 16.87s/it]
 34%|███▍      | 677/2000 [3:11:34<6:11:31, 16.85s/it]
                                                      

 34%|███▍      | 677/2000 [3:11:34<6:11:31, 16.85s/it]
 34%|███▍      | 678/2000 [3:11:51<6:11:21, 16.85s/it]
                                                      

 34%|███▍      | 678/2000 [3:11:51<6:11:21, 16.85s/it]
 34%|███▍      | 679/2000 [3:12:08<6:11:26, 16.87s/it]
                                                      

 34%|███▍      | 679/2000 [3:12:08<6:11:26, 16.87s/it]
 34%|███▍      | 680/2000 [3:12:24<6:10:19, 16.83s/it]
                                                      

 34%|███▍      | 680/2000 [3:12:24<6:10:19, 16.83s/it]
 34%|███▍      | 681/2000 [3:12:41<6:09:53, 16.83s/it]
                                                      

 34%|███▍      | 681/2000 [3:12:41<6:09:53, 16.83s/it]
 34%|███▍      | 682/2000 [3:12:58<6:10:19, 16.86s/it]
                                                      

 34%|███▍      | 682/2000 [3:12:58<6:10:19, 16.86s/it]
 34%|███▍      | 683/2000 [3:13:15<6:11:05, 16.91s/it]
                                                      

 34%|███▍      | 683/2000 [3:13:15<6:11:05, 16.91s/it]
 34%|███▍      | 684/2000 [3:13:32<6:10:26, 16.89s/it]
                                                      

 34%|███▍      | 684/2000 [3:13:32<6:10:26, 16.89s/it]
 34%|███▍      | 685/2000 [3:13:49<6:09:46, 16.87s/it]
                                                      

 34%|███▍      | 685/2000 [3:13:49<6:09:46, 16.87s/it]
 34%|███▍      | 686/2000 [3:14:06<6:09:52, 16.89s/it]
                                                      

 34%|███▍      | 686/2000 [3:14:06<6:09:52, 16.89s/it]
 34%|███▍      | 687/2000 [3:14:23<6:10:15, 16.92s/it]
                                                      

 34%|███▍      | 687/2000 [3:14:23<6:10:15, 16.92s/it]
 34%|███▍      | 688/2000 [3:14:40<6:09:37, 16.90s/it]
                                                      

 34%|███▍      | 688/2000 [3:14:40<6:09:37, 16.90s/it]
 34%|███▍      | 689/2000 [3:14:57<6:09:03, 16.89s/it]
                                                      

 34%|███▍      | 689/2000 [3:14:57<6:09:03, 16.89s/it]
 34%|███▍      | 690/2000 [3:15:13<6:09:01, 16.90s/it]
                                                      

 34%|███▍      | 690/2000 [3:15:13<6:09:01, 16.90s/it]
 35%|███▍      | 691/2000 [3:15:30<6:08:58, 16.91s/it]
                                                      

 35%|███▍      | 691/2000 [3:15:30<6:08:58, 16.91s/it]
 35%|███▍      | 692/2000 [3:15:47<6:08:30, 16.90s/it]
                                                      

 35%|███▍      | 692/2000 [3:15:47<6:08:30, 16.90s/it]
 35%|███▍      | 693/2000 [3:16:04<6:07:18, 16.86s/it]
                                                      

 35%|███▍      | 693/2000 [3:16:04<6:07:18, 16.86s/it]
 35%|███▍      | 694/2000 [3:16:21<6:07:30, 16.88s/it]
                                                      

 35%|███▍      | 694/2000 [3:16:21<6:07:30, 16.88s/it]
 35%|███▍      | 695/2000 [3:16:38<6:07:51, 16.91s/it]
                                                      

 35%|███▍      | 695/2000 [3:16:38<6:07:51, 16.91s/it]
 35%|███▍      | 696/2000 [3:16:55<6:07:16, 16.90s/it]
                                                      

 35%|███▍      | 696/2000 [3:16:55<6:07:16, 16.90s/it]
 35%|███▍      | 697/2000 [3:17:12<6:06:38, 16.88s/it]
                                                      

 35%|███▍      | 697/2000 [3:17:12<6:06:38, 16.88s/it]
 35%|███▍      | 698/2000 [3:17:29<6:06:05, 16.87s/it]
                                                      

 35%|███▍      | 698/2000 [3:17:29<6:06:05, 16.87s/it]
 35%|███▍      | 699/2000 [3:17:45<6:05:43, 16.87s/it]
                                                      

 35%|███▍      | 699/2000 [3:17:45<6:05:43, 16.87s/it]
 35%|███▌      | 700/2000 [3:18:02<6:05:59, 16.89s/it]
                                                      

 35%|███▌      | 700/2000 [3:18:02<6:05:59, 16.89s/it]
 35%|███▌      | 701/2000 [3:18:19<6:05:18, 16.87s/it]
                                                      

 35%|███▌      | 701/2000 [3:18:19<6:05:18, 16.87s/it]
 35%|███▌      | 702/2000 [3:18:36<6:04:33, 16.85s/it]
                                                      

 35%|███▌      | 702/2000 [3:18:36<6:04:33, 16.85s/it]
 35%|███▌      | 703/2000 [3:18:53<6:04:12, 16.85s/it]
                                                      

 35%|███▌      | 703/2000 [3:18:53<6:04:12, 16.85s/it]
 35%|███▌      | 704/2000 [3:19:10<6:03:44, 16.84s/it]
                                                      

 35%|███▌      | 704/2000 [3:19:10<6:03:44, 16.84s/it]
 35%|███▌      | 705/2000 [3:19:27<6:03:44, 16.85s/it]
                                                      

 35%|███▌      | 705/2000 [3:19:27<6:03:44, 16.85s/it]
 35%|███▌      | 706/2000 [3:19:43<6:03:57, 16.88s/it]
                                                      

 35%|███▌      | 706/2000 [3:19:43<6:03:57, 16.88s/it]
 35%|███▌      | 707/2000 [3:20:00<6:03:01, 16.85s/it]
                                                      

 35%|███▌      | 707/2000 [3:20:00<6:03:01, 16.85s/it]
 35%|███▌      | 708/2000 [3:20:17<6:04:57, 16.95s/it]
                                                      

 35%|███▌      | 708/2000 [3:20:17<6:04:57, 16.95s/it]
 35%|███▌      | 709/2000 [3:20:34<6:04:22, 16.93s/it]
                                                      

 35%|███▌      | 709/2000 [3:20:34<6:04:22, 16.93s/it]
 36%|███▌      | 710/2000 [3:20:51<6:03:35, 16.91s/it]
                                                      

 36%|███▌      | 710/2000 [3:20:51<6:03:35, 16.91s/it]
 36%|███▌      | 711/2000 [3:21:08<6:02:40, 16.88s/it]
                                                      

 36%|███▌      | 711/2000 [3:21:08<6:02:40, 16.88s/it]
 36%|███▌      | 712/2000 [3:21:25<6:02:17, 16.88s/it]
                                                      

 36%|███▌      | 712/2000 [3:21:25<6:02:17, 16.88s/it]
 36%|███▌      | 713/2000 [3:21:42<6:01:35, 16.86s/it]
                                                      

 36%|███▌      | 713/2000 [3:21:42<6:01:35, 16.86s/it]
 36%|███▌      | 714/2000 [3:21:58<6:00:59, 16.84s/it]
                                                      

 36%|███▌      | 714/2000 [3:21:58<6:00:59, 16.84s/it]
 36%|███▌      | 715/2000 [3:22:15<6:00:12, 16.82s/it]
                                                      

 36%|███▌      | 715/2000 [3:22:15<6:00:12, 16.82s/it]
 36%|███▌      | 716/2000 [3:22:33<6:05:33, 17.08s/it]
                                                      

 36%|███▌      | 716/2000 [3:22:33<6:05:33, 17.08s/it]
 36%|███▌      | 717/2000 [3:22:50<6:03:39, 17.01s/it]
                                                      

 36%|███▌      | 717/2000 [3:22:50<6:03:39, 17.01s/it]
 36%|███▌      | 718/2000 [3:23:07<6:02:37, 16.97s/it]
                                                      

 36%|███▌      | 718/2000 [3:23:07<6:02:37, 16.97s/it]
 36%|███▌      | 719/2000 [3:23:24<6:01:56, 16.95s/it]
                                                      

 36%|███▌      | 719/2000 [3:23:24<6:01:56, 16.95s/it]
 36%|███▌      | 720/2000 [3:23:41<6:01:49, 16.96s/it]
                                                      

 36%|███▌      | 720/2000 [3:23:41<6:01:49, 16.96s/it]
 36%|███▌      | 721/2000 [3:24:00<6:19:59, 17.83s/it]
                                                      

 36%|███▌      | 721/2000 [3:24:00<6:19:59, 17.83s/it]
 36%|███▌      | 722/2000 [3:24:17<6:15:03, 17.61s/it]
                                                      

 36%|███▌      | 722/2000 [3:24:17<6:15:03, 17.61s/it]
 36%|███▌      | 723/2000 [3:24:34<6:10:52, 17.43s/it]
                                                      

 36%|███▌      | 723/2000 [3:24:34<6:10:52, 17.43s/it]
 36%|███▌      | 724/2000 [3:24:51<6:07:44, 17.29s/it]
                                                      

 36%|███▌      | 724/2000 [3:24:51<6:07:44, 17.29s/it]
 36%|███▋      | 725/2000 [3:25:09<6:10:13, 17.42s/it]
                                                      

 36%|███▋      | 725/2000 [3:25:09<6:10:13, 17.42s/it]
 36%|███▋      | 726/2000 [3:25:26<6:06:12, 17.25s/it]
                                                      

 36%|███▋      | 726/2000 [3:25:26<6:06:12, 17.25s/it]
 36%|███▋      | 727/2000 [3:25:43<6:03:53, 17.15s/it]
                                                      

 36%|███▋      | 727/2000 [3:25:43<6:03:53, 17.15s/it]
 36%|███▋      | 728/2000 [3:26:00<6:01:57, 17.07s/it]
                                                      

 36%|███▋      | 728/2000 [3:26:00<6:01:57, 17.07s/it]
 36%|███▋      | 729/2000 [3:26:17<6:02:05, 17.09s/it]
                                                      

 36%|███▋      | 729/2000 [3:26:17<6:02:05, 17.09s/it]
 36%|███▋      | 730/2000 [3:26:34<6:00:07, 17.01s/it]
                                                      

 36%|███▋      | 730/2000 [3:26:34<6:00:07, 17.01s/it]
 37%|███▋      | 731/2000 [3:26:51<5:59:48, 17.01s/it]
                                                      

 37%|███▋      | 731/2000 [3:26:51<5:59:48, 17.01s/it]
 37%|███▋      | 732/2000 [3:27:08<5:58:37, 16.97s/it]
                                                      

 37%|███▋      | 732/2000 [3:27:08<5:58:37, 16.97s/it]
 37%|███▋      | 733/2000 [3:27:25<6:03:02, 17.19s/it]
                                                      

 37%|███▋      | 733/2000 [3:27:25<6:03:02, 17.19s/it]
 37%|███▋      | 734/2000 [3:27:42<6:00:05, 17.07s/it]
                                                      

 37%|███▋      | 734/2000 [3:27:42<6:00:05, 17.07s/it]
 37%|███▋      | 735/2000 [3:27:59<5:58:17, 16.99s/it]
                                                      

 37%|███▋      | 735/2000 [3:27:59<5:58:17, 16.99s/it]
 37%|███▋      | 736/2000 [3:28:16<5:56:37, 16.93s/it]
                                                      

 37%|███▋      | 736/2000 [3:28:16<5:56:37, 16.93s/it]
 37%|███▋      | 737/2000 [3:28:33<5:56:28, 16.93s/it]
                                                      

 37%|███▋      | 737/2000 [3:28:33<5:56:28, 16.93s/it]
 37%|███▋      | 738/2000 [3:28:50<5:56:14, 16.94s/it]
                                                      

 37%|███▋      | 738/2000 [3:28:50<5:56:14, 16.94s/it]
 37%|███▋      | 739/2000 [3:29:07<5:55:22, 16.91s/it]
                                                      

 37%|███▋      | 739/2000 [3:29:07<5:55:22, 16.91s/it]
 37%|███▋      | 740/2000 [3:29:24<5:55:33, 16.93s/it]
                                                      

 37%|███▋      | 740/2000 [3:29:24<5:55:33, 16.93s/it]
 37%|███▋      | 741/2000 [3:29:40<5:54:58, 16.92s/it]
                                                      

 37%|███▋      | 741/2000 [3:29:40<5:54:58, 16.92s/it]
 37%|███▋      | 742/2000 [3:29:57<5:54:11, 16.89s/it]
                                                      

 37%|███▋      | 742/2000 [3:29:57<5:54:11, 16.89s/it]
 37%|███▋      | 743/2000 [3:30:14<5:53:41, 16.88s/it]
                                                      

 37%|███▋      | 743/2000 [3:30:14<5:53:41, 16.88s/it]
 37%|███▋      | 744/2000 [3:30:31<5:52:48, 16.85s/it]
                                                      

 37%|███▋      | 744/2000 [3:30:31<5:52:48, 16.85s/it]
 37%|███▋      | 745/2000 [3:30:48<5:52:18, 16.84s/it]
                                                      

 37%|███▋      | 745/2000 [3:30:48<5:52:18, 16.84s/it]
 37%|███▋      | 746/2000 [3:31:05<5:51:50, 16.83s/it]
                                                      

 37%|███▋      | 746/2000 [3:31:05<5:51:50, 16.83s/it]
 37%|███▋      | 747/2000 [3:31:21<5:51:43, 16.84s/it]
                                                      

 37%|███▋      | 747/2000 [3:31:21<5:51:43, 16.84s/it]
 37%|███▋      | 748/2000 [3:31:38<5:51:19, 16.84s/it]
                                                      

 37%|███▋      | 748/2000 [3:31:38<5:51:19, 16.84s/it]
 37%|███▋      | 749/2000 [3:31:55<5:50:41, 16.82s/it]
                                                      

 37%|███▋      | 749/2000 [3:31:55<5:50:41, 16.82s/it]
 38%|███▊      | 750/2000 [3:32:12<5:50:19, 16.82s/it]
                                                      

 38%|███▊      | 750/2000 [3:32:12<5:50:19, 16.82s/it]
 38%|███▊      | 751/2000 [3:32:29<5:50:08, 16.82s/it]
                                                      

 38%|███▊      | 751/2000 [3:32:29<5:50:08, 16.82s/it]
 38%|███▊      | 752/2000 [3:32:45<5:49:58, 16.83s/it]
                                                      

 38%|███▊      | 752/2000 [3:32:45<5:49:58, 16.83s/it]
 38%|███▊      | 753/2000 [3:33:02<5:49:19, 16.81s/it]
                                                      

 38%|███▊      | 753/2000 [3:33:02<5:49:19, 16.81s/it]
 38%|███▊      | 754/2000 [3:33:19<5:48:47, 16.80s/it]
                                                      

 38%|███▊      | 754/2000 [3:33:19<5:48:47, 16.80s/it]
 38%|███▊      | 755/2000 [3:33:36<5:48:37, 16.80s/it]
                                                      

 38%|███▊      | 755/2000 [3:33:36<5:48:37, 16.80s/it]
 38%|███▊      | 756/2000 [3:33:53<5:49:04, 16.84s/it]
                                                      

 38%|███▊      | 756/2000 [3:33:53<5:49:04, 16.84s/it]
 38%|███▊      | 757/2000 [3:34:10<5:49:20, 16.86s/it]
                                                      

 38%|███▊      | 757/2000 [3:34:10<5:49:20, 16.86s/it]
 38%|███▊      | 758/2000 [3:34:26<5:48:58, 16.86s/it]
                                                      

 38%|███▊      | 758/2000 [3:34:26<5:48:58, 16.86s/it]
 38%|███▊      | 759/2000 [3:34:43<5:49:13, 16.88s/it]
                                                      

 38%|███▊      | 759/2000 [3:34:43<5:49:13, 16.88s/it]
 38%|███▊      | 760/2000 [3:35:00<5:49:15, 16.90s/it]
                                                      

 38%|███▊      | 760/2000 [3:35:00<5:49:15, 16.90s/it]
 38%|███▊      | 761/2000 [3:35:17<5:48:40, 16.89s/it]
                                                      

 38%|███▊      | 761/2000 [3:35:17<5:48:40, 16.89s/it]
 38%|███▊      | 762/2000 [3:35:34<5:48:50, 16.91s/it]
                                                      

 38%|███▊      | 762/2000 [3:35:34<5:48:50, 16.91s/it]
 38%|███▊      | 763/2000 [3:35:51<5:49:01, 16.93s/it]
                                                      

 38%|███▊      | 763/2000 [3:35:51<5:49:01, 16.93s/it]
 38%|███▊      | 764/2000 [3:36:08<5:47:55, 16.89s/it]
                                                      

 38%|███▊      | 764/2000 [3:36:08<5:47:55, 16.89s/it]
 38%|███▊      | 765/2000 [3:36:25<5:47:35, 16.89s/it]
                                                      

 38%|███▊      | 765/2000 [3:36:25<5:47:35, 16.89s/it]
 38%|███▊      | 766/2000 [3:36:42<5:47:14, 16.88s/it]
                                                      

 38%|███▊      | 766/2000 [3:36:42<5:47:14, 16.88s/it]
 38%|███▊      | 767/2000 [3:36:59<5:46:41, 16.87s/it]
                                                      

 38%|███▊      | 767/2000 [3:36:59<5:46:41, 16.87s/it]
 38%|███▊      | 768/2000 [3:37:15<5:46:20, 16.87s/it]
                                                      

 38%|███▊      | 768/2000 [3:37:15<5:46:20, 16.87s/it]
 38%|███▊      | 769/2000 [3:37:32<5:45:54, 16.86s/it]
                                                      

 38%|███▊      | 769/2000 [3:37:32<5:45:54, 16.86s/it]
 38%|███▊      | 770/2000 [3:37:49<5:45:23, 16.85s/it]
                                                      

 38%|███▊      | 770/2000 [3:37:49<5:45:23, 16.85s/it]
 39%|███▊      | 771/2000 [3:38:06<5:45:44, 16.88s/it]
                                                      

 39%|███▊      | 771/2000 [3:38:06<5:45:44, 16.88s/it]
 39%|███▊      | 772/2000 [3:38:23<5:45:19, 16.87s/it]
                                                      

 39%|███▊      | 772/2000 [3:38:23<5:45:19, 16.87s/it]
 39%|███▊      | 773/2000 [3:38:40<5:44:44, 16.86s/it]
                                                      

 39%|███▊      | 773/2000 [3:38:40<5:44:44, 16.86s/it]
 39%|███▊      | 774/2000 [3:38:57<5:44:24, 16.86s/it]
                                                      

 39%|███▊      | 774/2000 [3:38:57<5:44:24, 16.86s/it]
 39%|███▉      | 775/2000 [3:39:13<5:43:36, 16.83s/it]
                                                      

 39%|███▉      | 775/2000 [3:39:13<5:43:36, 16.83s/it]
 39%|███▉      | 776/2000 [3:39:30<5:43:10, 16.82s/it]
                                                      

 39%|███▉      | 776/2000 [3:39:30<5:43:10, 16.82s/it]
 39%|███▉      | 777/2000 [3:39:47<5:42:23, 16.80s/it]
                                                      

 39%|███▉      | 777/2000 [3:39:47<5:42:23, 16.80s/it]
 39%|███▉      | 778/2000 [3:40:04<5:42:06, 16.80s/it]
                                                      

 39%|███▉      | 778/2000 [3:40:04<5:42:06, 16.80s/it]
 39%|███▉      | 779/2000 [3:40:21<5:42:07, 16.81s/it]
                                                      

 39%|███▉      | 779/2000 [3:40:21<5:42:07, 16.81s/it]
 39%|███▉      | 780/2000 [3:40:37<5:42:40, 16.85s/it]
                                                      

 39%|███▉      | 780/2000 [3:40:37<5:42:40, 16.85s/it]
 39%|███▉      | 781/2000 [3:40:54<5:41:52, 16.83s/it]
                                                      

 39%|███▉      | 781/2000 [3:40:54<5:41:52, 16.83s/it]
 39%|███▉      | 782/2000 [3:41:11<5:41:33, 16.83s/it]
                                                      

 39%|███▉      | 782/2000 [3:41:11<5:41:33, 16.83s/it]
 39%|███▉      | 783/2000 [3:41:28<5:42:16, 16.87s/it]
                                                      

 39%|███▉      | 783/2000 [3:41:28<5:42:16, 16.87s/it]
 39%|███▉      | 784/2000 [3:41:45<5:42:45, 16.91s/it]
                                                      

 39%|███▉      | 784/2000 [3:41:45<5:42:45, 16.91s/it]
 39%|███▉      | 785/2000 [3:42:02<5:42:40, 16.92s/it]
                                                      

 39%|███▉      | 785/2000 [3:42:02<5:42:40, 16.92s/it]
 39%|███▉      | 786/2000 [3:42:19<5:42:25, 16.92s/it]
                                                      

 39%|███▉      | 786/2000 [3:42:19<5:42:25, 16.92s/it]
 39%|███▉      | 787/2000 [3:42:36<5:43:18, 16.98s/it]
                                                      

 39%|███▉      | 787/2000 [3:42:36<5:43:18, 16.98s/it]
 39%|███▉      | 788/2000 [3:42:53<5:42:47, 16.97s/it]
                                                      

 39%|███▉      | 788/2000 [3:42:53<5:42:47, 16.97s/it]
 39%|███▉      | 789/2000 [3:43:10<5:42:21, 16.96s/it]
                                                      

 39%|███▉      | 789/2000 [3:43:10<5:42:21, 16.96s/it]
 40%|███▉      | 790/2000 [3:43:27<5:42:32, 16.99s/it]
                                                      

 40%|███▉      | 790/2000 [3:43:27<5:42:32, 16.99s/it]
 40%|███▉      | 791/2000 [3:43:44<5:42:18, 16.99s/it]
                                                      

 40%|███▉      | 791/2000 [3:43:44<5:42:18, 16.99s/it]
 40%|███▉      | 792/2000 [3:44:02<5:46:24, 17.21s/it]
                                                      

 40%|███▉      | 792/2000 [3:44:02<5:46:24, 17.21s/it]
 40%|███▉      | 793/2000 [3:44:18<5:43:49, 17.09s/it]
                                                      

 40%|███▉      | 793/2000 [3:44:18<5:43:49, 17.09s/it]
 40%|███▉      | 794/2000 [3:44:35<5:42:57, 17.06s/it]
                                                      

 40%|███▉      | 794/2000 [3:44:35<5:42:57, 17.06s/it]
 40%|███▉      | 795/2000 [3:44:52<5:41:06, 16.98s/it]
                                                      

 40%|███▉      | 795/2000 [3:44:52<5:41:06, 16.98s/it]
 40%|███▉      | 796/2000 [3:45:09<5:39:44, 16.93s/it]
                                                      

 40%|███▉      | 796/2000 [3:45:09<5:39:44, 16.93s/it]
 40%|███▉      | 797/2000 [3:45:26<5:38:50, 16.90s/it]
                                                      

 40%|███▉      | 797/2000 [3:45:26<5:38:50, 16.90s/it]
 40%|███▉      | 798/2000 [3:45:43<5:38:03, 16.87s/it]
                                                      

 40%|███▉      | 798/2000 [3:45:43<5:38:03, 16.87s/it]
 40%|███▉      | 799/2000 [3:46:00<5:37:48, 16.88s/it]
                                                      

 40%|███▉      | 799/2000 [3:46:00<5:37:48, 16.88s/it]
 40%|████      | 800/2000 [3:46:17<5:38:42, 16.94s/it]
                                                      

 40%|████      | 800/2000 [3:46:17<5:38:42, 16.94s/it]
 40%|████      | 801/2000 [3:46:37<5:56:27, 17.84s/it]
                                                      

 40%|████      | 801/2000 [3:46:37<5:56:27, 17.84s/it]
 40%|████      | 802/2000 [3:46:54<5:55:11, 17.79s/it]
                                                      

 40%|████      | 802/2000 [3:46:54<5:55:11, 17.79s/it]
 40%|████      | 803/2000 [3:47:11<5:49:08, 17.50s/it]
                                                      

 40%|████      | 803/2000 [3:47:11<5:49:08, 17.50s/it]
 40%|████      | 804/2000 [3:47:28<5:45:51, 17.35s/it]
                                                      

 40%|████      | 804/2000 [3:47:28<5:45:51, 17.35s/it]
 40%|████      | 805/2000 [3:47:45<5:42:48, 17.21s/it]
                                                      

 40%|████      | 805/2000 [3:47:45<5:42:48, 17.21s/it]
 40%|████      | 806/2000 [3:48:02<5:40:12, 17.10s/it]
                                                      

 40%|████      | 806/2000 [3:48:02<5:40:12, 17.10s/it]
 40%|████      | 807/2000 [3:48:19<5:38:33, 17.03s/it]
                                                      

 40%|████      | 807/2000 [3:48:19<5:38:33, 17.03s/it]
 40%|████      | 808/2000 [3:48:35<5:36:46, 16.95s/it]
                                                      

 40%|████      | 808/2000 [3:48:35<5:36:46, 16.95s/it]
 40%|████      | 809/2000 [3:48:52<5:36:03, 16.93s/it]
                                                      

 40%|████      | 809/2000 [3:48:52<5:36:03, 16.93s/it]
 40%|████      | 810/2000 [3:49:09<5:36:07, 16.95s/it]
                                                      

 40%|████      | 810/2000 [3:49:09<5:36:07, 16.95s/it]
 41%|████      | 811/2000 [3:49:26<5:35:03, 16.91s/it]
                                                      

 41%|████      | 811/2000 [3:49:26<5:35:03, 16.91s/it]
 41%|████      | 812/2000 [3:49:43<5:33:58, 16.87s/it]
                                                      

 41%|████      | 812/2000 [3:49:43<5:33:58, 16.87s/it]
 41%|████      | 813/2000 [3:50:00<5:33:50, 16.87s/it]
                                                      

 41%|████      | 813/2000 [3:50:00<5:33:50, 16.87s/it]
 41%|████      | 814/2000 [3:50:17<5:33:06, 16.85s/it]
                                                      

 41%|████      | 814/2000 [3:50:17<5:33:06, 16.85s/it]
 41%|████      | 815/2000 [3:50:33<5:32:39, 16.84s/it]
                                                      

 41%|████      | 815/2000 [3:50:33<5:32:39, 16.84s/it]
 41%|████      | 816/2000 [3:50:50<5:32:18, 16.84s/it]
                                                      

 41%|████      | 816/2000 [3:50:50<5:32:18, 16.84s/it]
 41%|████      | 817/2000 [3:51:07<5:31:42, 16.82s/it]
                                                      

 41%|████      | 817/2000 [3:51:07<5:31:42, 16.82s/it]
 41%|████      | 818/2000 [3:51:24<5:31:11, 16.81s/it]
                                                      

 41%|████      | 818/2000 [3:51:24<5:31:11, 16.81s/it]
 41%|████      | 819/2000 [3:51:41<5:31:18, 16.83s/it]
                                                      

 41%|████      | 819/2000 [3:51:41<5:31:18, 16.83s/it]
 41%|████      | 820/2000 [3:51:58<5:31:06, 16.84s/it]
                                                      

 41%|████      | 820/2000 [3:51:58<5:31:06, 16.84s/it]
 41%|████      | 821/2000 [3:52:14<5:30:51, 16.84s/it]
                                                      

 41%|████      | 821/2000 [3:52:14<5:30:51, 16.84s/it]
 41%|████      | 822/2000 [3:52:31<5:31:19, 16.88s/it]
                                                      

 41%|████      | 822/2000 [3:52:31<5:31:19, 16.88s/it]
 41%|████      | 823/2000 [3:52:48<5:30:48, 16.86s/it]
                                                      

 41%|████      | 823/2000 [3:52:48<5:30:48, 16.86s/it]
 41%|████      | 824/2000 [3:53:05<5:30:37, 16.87s/it]
                                                      

 41%|████      | 824/2000 [3:53:05<5:30:37, 16.87s/it]
 41%|████▏     | 825/2000 [3:53:22<5:30:59, 16.90s/it]
                                                      

 41%|████▏     | 825/2000 [3:53:22<5:30:59, 16.90s/it]
 41%|████▏     | 826/2000 [3:53:39<5:30:16, 16.88s/it]
                                                      

 41%|████▏     | 826/2000 [3:53:39<5:30:16, 16.88s/it]
 41%|████▏     | 827/2000 [3:53:56<5:29:27, 16.85s/it]
                                                      

 41%|████▏     | 827/2000 [3:53:56<5:29:27, 16.85s/it]
 41%|████▏     | 828/2000 [3:54:13<5:28:53, 16.84s/it]
                                                      

 41%|████▏     | 828/2000 [3:54:13<5:28:53, 16.84s/it]
 41%|████▏     | 829/2000 [3:54:29<5:28:27, 16.83s/it]
                                                      

 41%|████▏     | 829/2000 [3:54:29<5:28:27, 16.83s/it]
 42%|████▏     | 830/2000 [3:54:46<5:28:44, 16.86s/it]
                                                      

 42%|████▏     | 830/2000 [3:54:46<5:28:44, 16.86s/it]
 42%|████▏     | 831/2000 [3:55:03<5:28:53, 16.88s/it]
                                                      

 42%|████▏     | 831/2000 [3:55:03<5:28:53, 16.88s/it]
 42%|████▏     | 832/2000 [3:55:20<5:28:13, 16.86s/it]
                                                      

 42%|████▏     | 832/2000 [3:55:20<5:28:13, 16.86s/it]
 42%|████▏     | 833/2000 [3:55:37<5:27:37, 16.84s/it]
                                                      

 42%|████▏     | 833/2000 [3:55:37<5:27:37, 16.84s/it]
 42%|████▏     | 834/2000 [3:55:54<5:26:57, 16.82s/it]
                                                      

 42%|████▏     | 834/2000 [3:55:54<5:26:57, 16.82s/it]
 42%|████▏     | 835/2000 [3:56:10<5:26:29, 16.82s/it]
                                                      

 42%|████▏     | 835/2000 [3:56:10<5:26:29, 16.82s/it]
 42%|████▏     | 836/2000 [3:56:27<5:26:16, 16.82s/it]
                                                      

 42%|████▏     | 836/2000 [3:56:27<5:26:16, 16.82s/it]
 42%|████▏     | 837/2000 [3:56:44<5:26:20, 16.84s/it]
                                                      

 42%|████▏     | 837/2000 [3:56:44<5:26:20, 16.84s/it]
 42%|████▏     | 838/2000 [3:57:01<5:25:51, 16.83s/it]
                                                      

 42%|████▏     | 838/2000 [3:57:01<5:25:51, 16.83s/it]
 42%|████▏     | 839/2000 [3:57:18<5:25:30, 16.82s/it]
                                                      

 42%|████▏     | 839/2000 [3:57:18<5:25:30, 16.82s/it]
 42%|████▏     | 840/2000 [3:57:35<5:25:48, 16.85s/it]
                                                      

 42%|████▏     | 840/2000 [3:57:35<5:25:48, 16.85s/it]
 42%|████▏     | 841/2000 [3:57:51<5:25:06, 16.83s/it]
                                                      

 42%|████▏     | 841/2000 [3:57:51<5:25:06, 16.83s/it]
 42%|████▏     | 842/2000 [3:58:08<5:25:04, 16.84s/it]
                                                      

 42%|████▏     | 842/2000 [3:58:08<5:25:04, 16.84s/it]
 42%|████▏     | 843/2000 [3:58:25<5:25:27, 16.88s/it]
                                                      

 42%|████▏     | 843/2000 [3:58:25<5:25:27, 16.88s/it]
 42%|████▏     | 844/2000 [3:58:42<5:25:04, 16.87s/it]
                                                      

 42%|████▏     | 844/2000 [3:58:42<5:25:04, 16.87s/it]
 42%|████▏     | 845/2000 [3:58:59<5:24:42, 16.87s/it]
                                                      

 42%|████▏     | 845/2000 [3:58:59<5:24:42, 16.87s/it]
 42%|████▏     | 846/2000 [3:59:16<5:24:48, 16.89s/it]
                                                      

 42%|████▏     | 846/2000 [3:59:16<5:24:48, 16.89s/it]
 42%|████▏     | 847/2000 [3:59:33<5:24:19, 16.88s/it]
                                                      

 42%|████▏     | 847/2000 [3:59:33<5:24:19, 16.88s/it]
 42%|████▏     | 848/2000 [3:59:50<5:23:32, 16.85s/it]
                                                      

 42%|████▏     | 848/2000 [3:59:50<5:23:32, 16.85s/it]
 42%|████▏     | 849/2000 [4:00:06<5:23:06, 16.84s/it]
                                                      

 42%|████▏     | 849/2000 [4:00:06<5:23:06, 16.84s/it]
 42%|████▎     | 850/2000 [4:00:23<5:22:43, 16.84s/it]
                                                      

 42%|████▎     | 850/2000 [4:00:23<5:22:43, 16.84s/it]
 43%|████▎     | 851/2000 [4:00:40<5:22:02, 16.82s/it]
                                                      

 43%|████▎     | 851/2000 [4:00:40<5:22:02, 16.82s/it]
 43%|████▎     | 852/2000 [4:00:57<5:21:37, 16.81s/it]
                                                      

 43%|████▎     | 852/2000 [4:00:57<5:21:37, 16.81s/it]
 43%|████▎     | 853/2000 [4:01:14<5:21:15, 16.81s/it]
                                                      

 43%|████▎     | 853/2000 [4:01:14<5:21:15, 16.81s/it]
 43%|████▎     | 854/2000 [4:01:30<5:20:55, 16.80s/it]
                                                      

 43%|████▎     | 854/2000 [4:01:30<5:20:55, 16.80s/it]
 43%|████▎     | 855/2000 [4:01:47<5:20:59, 16.82s/it]
                                                      

 43%|████▎     | 855/2000 [4:01:47<5:20:59, 16.82s/it]
 43%|████▎     | 856/2000 [4:02:04<5:20:47, 16.82s/it]
                                                      

 43%|████▎     | 856/2000 [4:02:04<5:20:47, 16.82s/it]
 43%|████▎     | 857/2000 [4:02:21<5:19:57, 16.80s/it]
                                                      

 43%|████▎     | 857/2000 [4:02:21<5:19:57, 16.80s/it]
 43%|████▎     | 858/2000 [4:02:37<5:19:23, 16.78s/it]
                                                      

 43%|████▎     | 858/2000 [4:02:37<5:19:23, 16.78s/it]
 43%|████▎     | 859/2000 [4:02:54<5:19:15, 16.79s/it]
                                                      

 43%|████▎     | 859/2000 [4:02:54<5:19:15, 16.79s/it]
 43%|████▎     | 860/2000 [4:03:11<5:18:48, 16.78s/it]
                                                      

 43%|████▎     | 860/2000 [4:03:11<5:18:48, 16.78s/it]
 43%|████▎     | 861/2000 [4:03:28<5:18:48, 16.79s/it]
                                                      

 43%|████▎     | 861/2000 [4:03:28<5:18:48, 16.79s/it]
 43%|████▎     | 862/2000 [4:03:45<5:19:01, 16.82s/it]
                                                      

 43%|████▎     | 862/2000 [4:03:45<5:19:01, 16.82s/it]
 43%|████▎     | 863/2000 [4:04:02<5:19:14, 16.85s/it]
                                                      

 43%|████▎     | 863/2000 [4:04:02<5:19:14, 16.85s/it]
 43%|████▎     | 864/2000 [4:04:18<5:18:33, 16.83s/it]
                                                      

 43%|████▎     | 864/2000 [4:04:18<5:18:33, 16.83s/it]
 43%|████▎     | 865/2000 [4:04:35<5:18:24, 16.83s/it]
                                                      

 43%|████▎     | 865/2000 [4:04:35<5:18:24, 16.83s/it]
 43%|████▎     | 866/2000 [4:04:52<5:18:35, 16.86s/it]
                                                      

 43%|████▎     | 866/2000 [4:04:52<5:18:35, 16.86s/it]
 43%|████▎     | 867/2000 [4:05:09<5:17:40, 16.82s/it]
                                                      

 43%|████▎     | 867/2000 [4:05:09<5:17:40, 16.82s/it]
 43%|████▎     | 868/2000 [4:05:26<5:17:36, 16.83s/it]
                                                      

 43%|████▎     | 868/2000 [4:05:26<5:17:36, 16.83s/it]
 43%|████▎     | 869/2000 [4:05:43<5:17:44, 16.86s/it]
                                                      

 43%|████▎     | 869/2000 [4:05:43<5:17:44, 16.86s/it]
 44%|████▎     | 870/2000 [4:06:00<5:17:41, 16.87s/it]
                                                      

 44%|████▎     | 870/2000 [4:06:00<5:17:41, 16.87s/it]
 44%|████▎     | 871/2000 [4:06:16<5:17:20, 16.86s/it]
                                                      

 44%|████▎     | 871/2000 [4:06:16<5:17:20, 16.86s/it]
 44%|████▎     | 872/2000 [4:06:33<5:17:19, 16.88s/it]
                                                      

 44%|████▎     | 872/2000 [4:06:33<5:17:19, 16.88s/it]
 44%|████▎     | 873/2000 [4:06:50<5:16:33, 16.85s/it]
                                                      

 44%|████▎     | 873/2000 [4:06:50<5:16:33, 16.85s/it]
 44%|████▎     | 874/2000 [4:07:07<5:16:04, 16.84s/it]
                                                      

 44%|████▎     | 874/2000 [4:07:07<5:16:04, 16.84s/it]
 44%|████▍     | 875/2000 [4:07:24<5:15:21, 16.82s/it]
                                                      

 44%|████▍     | 875/2000 [4:07:24<5:15:21, 16.82s/it]
 44%|████▍     | 876/2000 [4:07:41<5:14:49, 16.81s/it]
                                                      

 44%|████▍     | 876/2000 [4:07:41<5:14:49, 16.81s/it]
 44%|████▍     | 877/2000 [4:07:57<5:14:28, 16.80s/it]
                                                      

 44%|████▍     | 877/2000 [4:07:57<5:14:28, 16.80s/it]
 44%|████▍     | 878/2000 [4:08:14<5:13:57, 16.79s/it]
                                                      

 44%|████▍     | 878/2000 [4:08:14<5:13:57, 16.79s/it]
 44%|████▍     | 879/2000 [4:08:31<5:13:23, 16.77s/it]
                                                      

 44%|████▍     | 879/2000 [4:08:31<5:13:23, 16.77s/it]
 44%|████▍     | 880/2000 [4:08:48<5:13:54, 16.82s/it]
                                                      

 44%|████▍     | 880/2000 [4:08:48<5:13:54, 16.82s/it]
 44%|████▍     | 881/2000 [4:09:07<5:29:35, 17.67s/it]
                                                      

 44%|████▍     | 881/2000 [4:09:07<5:29:35, 17.67s/it]
 44%|████▍     | 882/2000 [4:09:25<5:29:02, 17.66s/it]
                                                      

 44%|████▍     | 882/2000 [4:09:25<5:29:02, 17.66s/it]
 44%|████▍     | 883/2000 [4:09:42<5:23:42, 17.39s/it]
                                                      

 44%|████▍     | 883/2000 [4:09:42<5:23:42, 17.39s/it]
 44%|████▍     | 884/2000 [4:09:59<5:20:44, 17.24s/it]
                                                      

 44%|████▍     | 884/2000 [4:09:59<5:20:44, 17.24s/it]
 44%|████▍     | 885/2000 [4:10:15<5:17:58, 17.11s/it]
                                                      

 44%|████▍     | 885/2000 [4:10:16<5:17:58, 17.11s/it]
 44%|████▍     | 886/2000 [4:10:32<5:16:14, 17.03s/it]
                                                      

 44%|████▍     | 886/2000 [4:10:32<5:16:14, 17.03s/it]
 44%|████▍     | 887/2000 [4:10:49<5:14:38, 16.96s/it]
                                                      

 44%|████▍     | 887/2000 [4:10:49<5:14:38, 16.96s/it]
 44%|████▍     | 888/2000 [4:11:06<5:13:37, 16.92s/it]
                                                      

 44%|████▍     | 888/2000 [4:11:06<5:13:37, 16.92s/it]
 44%|████▍     | 889/2000 [4:11:23<5:12:46, 16.89s/it]
                                                      

 44%|████▍     | 889/2000 [4:11:23<5:12:46, 16.89s/it]
 44%|████▍     | 890/2000 [4:11:40<5:11:58, 16.86s/it]
                                                      

 44%|████▍     | 890/2000 [4:11:40<5:11:58, 16.86s/it]
 45%|████▍     | 891/2000 [4:11:56<5:11:35, 16.86s/it]
                                                      

 45%|████▍     | 891/2000 [4:11:56<5:11:35, 16.86s/it]
 45%|████▍     | 892/2000 [4:12:13<5:11:14, 16.85s/it]
                                                      

 45%|████▍     | 892/2000 [4:12:13<5:11:14, 16.85s/it]
 45%|████▍     | 893/2000 [4:12:30<5:10:34, 16.83s/it]
                                                      

 45%|████▍     | 893/2000 [4:12:30<5:10:34, 16.83s/it]
 45%|████▍     | 894/2000 [4:12:47<5:10:10, 16.83s/it]
                                                      

 45%|████▍     | 894/2000 [4:12:47<5:10:10, 16.83s/it]
 45%|████▍     | 895/2000 [4:13:04<5:09:43, 16.82s/it]
                                                      

 45%|████▍     | 895/2000 [4:13:04<5:09:43, 16.82s/it]
 45%|████▍     | 896/2000 [4:13:20<5:09:19, 16.81s/it]
                                                      

 45%|████▍     | 896/2000 [4:13:20<5:09:19, 16.81s/it]
 45%|████▍     | 897/2000 [4:13:37<5:09:09, 16.82s/it]
                                                      

 45%|████▍     | 897/2000 [4:13:37<5:09:09, 16.82s/it]
 45%|████▍     | 898/2000 [4:13:54<5:08:53, 16.82s/it]
                                                      

 45%|████▍     | 898/2000 [4:13:54<5:08:53, 16.82s/it]
 45%|████▍     | 899/2000 [4:14:11<5:08:39, 16.82s/it]
                                                      

 45%|████▍     | 899/2000 [4:14:11<5:08:39, 16.82s/it]
 45%|████▌     | 900/2000 [4:14:28<5:08:23, 16.82s/it]
                                                      

 45%|████▌     | 900/2000 [4:14:28<5:08:23, 16.82s/it]
 45%|████▌     | 901/2000 [4:14:45<5:07:53, 16.81s/it]
                                                      

 45%|████▌     | 901/2000 [4:14:45<5:07:53, 16.81s/it]
 45%|████▌     | 902/2000 [4:15:01<5:07:32, 16.81s/it]
                                                      

 45%|████▌     | 902/2000 [4:15:01<5:07:32, 16.81s/it]
 45%|████▌     | 903/2000 [4:15:18<5:07:22, 16.81s/it]
                                                      

 45%|████▌     | 903/2000 [4:15:18<5:07:22, 16.81s/it]
 45%|████▌     | 904/2000 [4:15:35<5:07:32, 16.84s/it]
                                                      

 45%|████▌     | 904/2000 [4:15:35<5:07:32, 16.84s/it]
 45%|████▌     | 905/2000 [4:15:52<5:07:30, 16.85s/it]
                                                      

 45%|████▌     | 905/2000 [4:15:52<5:07:30, 16.85s/it]
 45%|████▌     | 906/2000 [4:16:09<5:07:09, 16.85s/it]
                                                      

 45%|████▌     | 906/2000 [4:16:09<5:07:09, 16.85s/it]
 45%|████▌     | 907/2000 [4:16:26<5:06:39, 16.83s/it]
                                                      

 45%|████▌     | 907/2000 [4:16:26<5:06:39, 16.83s/it]
 45%|████▌     | 908/2000 [4:16:42<5:06:12, 16.82s/it]
                                                      

 45%|████▌     | 908/2000 [4:16:42<5:06:12, 16.82s/it]
 45%|████▌     | 909/2000 [4:16:59<5:05:55, 16.82s/it]
                                                      

 45%|████▌     | 909/2000 [4:16:59<5:05:55, 16.82s/it]
 46%|████▌     | 910/2000 [4:17:16<5:05:44, 16.83s/it]
                                                      

 46%|████▌     | 910/2000 [4:17:16<5:05:44, 16.83s/it]
 46%|████▌     | 911/2000 [4:17:33<5:05:11, 16.82s/it]
                                                      

 46%|████▌     | 911/2000 [4:17:33<5:05:11, 16.82s/it]
 46%|████▌     | 912/2000 [4:17:50<5:05:13, 16.83s/it]
                                                      

 46%|████▌     | 912/2000 [4:17:50<5:05:13, 16.83s/it]
 46%|████▌     | 913/2000 [4:18:07<5:05:00, 16.84s/it]
                                                      

 46%|████▌     | 913/2000 [4:18:07<5:05:00, 16.84s/it]
 46%|████▌     | 914/2000 [4:18:23<5:04:10, 16.81s/it]
                                                      

 46%|████▌     | 914/2000 [4:18:23<5:04:10, 16.81s/it]
 46%|████▌     | 915/2000 [4:18:40<5:03:41, 16.79s/it]
                                                      

 46%|████▌     | 915/2000 [4:18:40<5:03:41, 16.79s/it]
 46%|████▌     | 916/2000 [4:18:57<5:03:22, 16.79s/it]
                                                      

 46%|████▌     | 916/2000 [4:18:57<5:03:22, 16.79s/it]
 46%|████▌     | 917/2000 [4:19:14<5:02:57, 16.78s/it]
                                                      

 46%|████▌     | 917/2000 [4:19:14<5:02:57, 16.78s/it]
 46%|████▌     | 918/2000 [4:19:30<5:02:35, 16.78s/it]
                                                      

 46%|████▌     | 918/2000 [4:19:30<5:02:35, 16.78s/it]
 46%|████▌     | 919/2000 [4:19:47<5:02:27, 16.79s/it]
                                                      

 46%|████▌     | 919/2000 [4:19:47<5:02:27, 16.79s/it]
 46%|████▌     | 920/2000 [4:20:04<5:02:51, 16.83s/it]
                                                      

 46%|████▌     | 920/2000 [4:20:04<5:02:51, 16.83s/it]
 46%|████▌     | 921/2000 [4:20:21<5:03:27, 16.87s/it]
                                                      

 46%|████▌     | 921/2000 [4:20:21<5:03:27, 16.87s/it]
 46%|████▌     | 922/2000 [4:20:38<5:02:41, 16.85s/it]
                                                      

 46%|████▌     | 922/2000 [4:20:38<5:02:41, 16.85s/it]
 46%|████▌     | 923/2000 [4:20:55<5:02:24, 16.85s/it]
                                                      

 46%|████▌     | 923/2000 [4:20:55<5:02:24, 16.85s/it]
 46%|████▌     | 924/2000 [4:21:12<5:01:50, 16.83s/it]
                                                      

 46%|████▌     | 924/2000 [4:21:12<5:01:50, 16.83s/it]
 46%|████▋     | 925/2000 [4:21:28<5:01:23, 16.82s/it]
                                                      

 46%|████▋     | 925/2000 [4:21:28<5:01:23, 16.82s/it]
 46%|████▋     | 926/2000 [4:21:45<5:00:43, 16.80s/it]
                                                      

 46%|████▋     | 926/2000 [4:21:45<5:00:43, 16.80s/it]
 46%|████▋     | 927/2000 [4:22:02<5:00:37, 16.81s/it]
                                                      

 46%|████▋     | 927/2000 [4:22:02<5:00:37, 16.81s/it]
 46%|████▋     | 928/2000 [4:22:19<5:00:53, 16.84s/it]
                                                      

 46%|████▋     | 928/2000 [4:22:19<5:00:53, 16.84s/it]
 46%|████▋     | 929/2000 [4:22:36<5:01:13, 16.88s/it]
                                                      

 46%|████▋     | 929/2000 [4:22:36<5:01:13, 16.88s/it]
 46%|████▋     | 930/2000 [4:22:53<5:00:44, 16.86s/it]
                                                      

 46%|████▋     | 930/2000 [4:22:53<5:00:44, 16.86s/it]
 47%|████▋     | 931/2000 [4:23:09<5:00:33, 16.87s/it]
                                                      

 47%|████▋     | 931/2000 [4:23:09<5:00:33, 16.87s/it]
 47%|████▋     | 932/2000 [4:23:26<5:00:11, 16.86s/it]
                                                      

 47%|████▋     | 932/2000 [4:23:26<5:00:11, 16.86s/it]
 47%|████▋     | 933/2000 [4:23:43<4:59:37, 16.85s/it]
                                                      

 47%|████▋     | 933/2000 [4:23:43<4:59:37, 16.85s/it]
 47%|████▋     | 934/2000 [4:24:00<4:59:13, 16.84s/it]
                                                      

 47%|████▋     | 934/2000 [4:24:00<4:59:13, 16.84s/it]
 47%|████▋     | 935/2000 [4:24:17<4:59:13, 16.86s/it]
                                                      

 47%|████▋     | 935/2000 [4:24:17<4:59:13, 16.86s/it]
 47%|████▋     | 936/2000 [4:24:34<4:58:43, 16.85s/it]
                                                      

 47%|████▋     | 936/2000 [4:24:34<4:58:43, 16.85s/it]
 47%|████▋     | 937/2000 [4:24:50<4:58:13, 16.83s/it]
                                                      

 47%|████▋     | 937/2000 [4:24:51<4:58:13, 16.83s/it]
 47%|████▋     | 938/2000 [4:25:07<4:57:34, 16.81s/it]
                                                      

 47%|████▋     | 938/2000 [4:25:07<4:57:34, 16.81s/it]
 47%|████▋     | 939/2000 [4:25:24<4:57:04, 16.80s/it]
                                                      

 47%|████▋     | 939/2000 [4:25:24<4:57:04, 16.80s/it]
 47%|████▋     | 940/2000 [4:25:41<4:56:48, 16.80s/it]
                                                      

 47%|████▋     | 940/2000 [4:25:41<4:56:48, 16.80s/it]
 47%|████▋     | 941/2000 [4:25:58<4:56:26, 16.80s/it]
                                                      

 47%|████▋     | 941/2000 [4:25:58<4:56:26, 16.80s/it]
 47%|████▋     | 942/2000 [4:26:14<4:56:04, 16.79s/it]
                                                      

 47%|████▋     | 942/2000 [4:26:14<4:56:04, 16.79s/it]
 47%|████▋     | 943/2000 [4:26:31<4:55:29, 16.77s/it]
                                                      

 47%|████▋     | 943/2000 [4:26:31<4:55:29, 16.77s/it]
 47%|████▋     | 944/2000 [4:26:48<4:55:28, 16.79s/it]
                                                      

 47%|████▋     | 944/2000 [4:26:48<4:55:28, 16.79s/it]
 47%|████▋     | 945/2000 [4:27:05<4:55:20, 16.80s/it]
                                                      

 47%|████▋     | 945/2000 [4:27:05<4:55:20, 16.80s/it]
 47%|████▋     | 946/2000 [4:27:22<4:54:53, 16.79s/it]
                                                      

 47%|████▋     | 946/2000 [4:27:22<4:54:53, 16.79s/it]
 47%|████▋     | 947/2000 [4:27:38<4:54:24, 16.78s/it]
                                                      

 47%|████▋     | 947/2000 [4:27:38<4:54:24, 16.78s/it]
 47%|████▋     | 948/2000 [4:27:55<4:54:19, 16.79s/it]
                                                      

 47%|████▋     | 948/2000 [4:27:55<4:54:19, 16.79s/it]
 47%|████▋     | 949/2000 [4:28:12<4:53:58, 16.78s/it]
                                                      

 47%|████▋     | 949/2000 [4:28:12<4:53:58, 16.78s/it]
 48%|████▊     | 950/2000 [4:28:29<4:53:55, 16.80s/it]
                                                      

 48%|████▊     | 950/2000 [4:28:29<4:53:55, 16.80s/it]
 48%|████▊     | 951/2000 [4:28:46<4:54:23, 16.84s/it]
                                                      

 48%|████▊     | 951/2000 [4:28:46<4:54:23, 16.84s/it]
 48%|████▊     | 952/2000 [4:29:02<4:53:57, 16.83s/it]
                                                      

 48%|████▊     | 952/2000 [4:29:02<4:53:57, 16.83s/it]
 48%|████▊     | 953/2000 [4:29:19<4:53:29, 16.82s/it]
                                                      

 48%|████▊     | 953/2000 [4:29:19<4:53:29, 16.82s/it]
 48%|████▊     | 954/2000 [4:29:36<4:52:51, 16.80s/it]
                                                      

 48%|████▊     | 954/2000 [4:29:36<4:52:51, 16.80s/it]
 48%|████▊     | 955/2000 [4:29:53<4:52:15, 16.78s/it]
                                                      

 48%|████▊     | 955/2000 [4:29:53<4:52:15, 16.78s/it]
 48%|████▊     | 956/2000 [4:30:10<4:52:06, 16.79s/it]
                                                      

 48%|████▊     | 956/2000 [4:30:10<4:52:06, 16.79s/it]
 48%|████▊     | 957/2000 [4:30:26<4:51:57, 16.80s/it]
                                                      

 48%|████▊     | 957/2000 [4:30:26<4:51:57, 16.80s/it]
 48%|████▊     | 958/2000 [4:30:43<4:51:42, 16.80s/it]
                                                      

 48%|████▊     | 958/2000 [4:30:43<4:51:42, 16.80s/it]
 48%|████▊     | 959/2000 [4:31:00<4:51:27, 16.80s/it]
                                                      

 48%|████▊     | 959/2000 [4:31:00<4:51:27, 16.80s/it]
 48%|████▊     | 960/2000 [4:31:17<4:51:24, 16.81s/it]
                                                      

 48%|████▊     | 960/2000 [4:31:17<4:51:24, 16.81s/it]
 48%|████▊     | 961/2000 [4:31:36<5:05:55, 17.67s/it]
                                                      

 48%|████▊     | 961/2000 [4:31:36<5:05:55, 17.67s/it]
 48%|████▊     | 962/2000 [4:31:53<5:01:16, 17.41s/it]
                                                      

 48%|████▊     | 962/2000 [4:31:53<5:01:16, 17.41s/it]
 48%|████▊     | 963/2000 [4:32:10<4:58:01, 17.24s/it]
                                                      

 48%|████▊     | 963/2000 [4:32:10<4:58:01, 17.24s/it]
 48%|████▊     | 964/2000 [4:32:27<4:55:24, 17.11s/it]
                                                      

 48%|████▊     | 964/2000 [4:32:27<4:55:24, 17.11s/it]
 48%|████▊     | 965/2000 [4:32:44<4:53:28, 17.01s/it]
                                                      

 48%|████▊     | 965/2000 [4:32:44<4:53:28, 17.01s/it]
 48%|████▊     | 966/2000 [4:33:01<4:52:16, 16.96s/it]
                                                      

 48%|████▊     | 966/2000 [4:33:01<4:52:16, 16.96s/it]
 48%|████▊     | 967/2000 [4:33:17<4:51:09, 16.91s/it]
                                                      

 48%|████▊     | 967/2000 [4:33:17<4:51:09, 16.91s/it]
 48%|████▊     | 968/2000 [4:33:34<4:50:14, 16.87s/it]
                                                      

 48%|████▊     | 968/2000 [4:33:34<4:50:14, 16.87s/it]
 48%|████▊     | 969/2000 [4:33:51<4:49:36, 16.85s/it]
                                                      

 48%|████▊     | 969/2000 [4:33:51<4:49:36, 16.85s/it]
 48%|████▊     | 970/2000 [4:34:08<4:48:59, 16.83s/it]
                                                      

 48%|████▊     | 970/2000 [4:34:08<4:48:59, 16.83s/it]
 49%|████▊     | 971/2000 [4:34:25<4:48:34, 16.83s/it]
                                                      

 49%|████▊     | 971/2000 [4:34:25<4:48:34, 16.83s/it]
 49%|████▊     | 972/2000 [4:34:41<4:48:32, 16.84s/it]
                                                      

 49%|████▊     | 972/2000 [4:34:41<4:48:32, 16.84s/it]
 49%|████▊     | 973/2000 [4:34:58<4:48:33, 16.86s/it]
                                                      

 49%|████▊     | 973/2000 [4:34:58<4:48:33, 16.86s/it]
 49%|████▊     | 974/2000 [4:35:15<4:48:05, 16.85s/it]
                                                      

 49%|████▊     | 974/2000 [4:35:15<4:48:05, 16.85s/it]
 49%|████▉     | 975/2000 [4:35:32<4:47:34, 16.83s/it]
                                                      

 49%|████▉     | 975/2000 [4:35:32<4:47:34, 16.83s/it]
 49%|████▉     | 976/2000 [4:35:49<4:47:25, 16.84s/it]
                                                      

 49%|████▉     | 976/2000 [4:35:49<4:47:25, 16.84s/it]
 49%|████▉     | 977/2000 [4:36:06<4:47:29, 16.86s/it]
                                                      

 49%|████▉     | 977/2000 [4:36:06<4:47:29, 16.86s/it]
 49%|████▉     | 978/2000 [4:36:23<4:46:57, 16.85s/it]
                                                      

 49%|████▉     | 978/2000 [4:36:23<4:46:57, 16.85s/it]
 49%|████▉     | 979/2000 [4:36:39<4:46:43, 16.85s/it]
                                                      

 49%|████▉     | 979/2000 [4:36:39<4:46:43, 16.85s/it]
 49%|████▉     | 980/2000 [4:36:56<4:46:49, 16.87s/it]
                                                      

 49%|████▉     | 980/2000 [4:36:56<4:46:49, 16.87s/it]
 49%|████▉     | 981/2000 [4:37:13<4:46:41, 16.88s/it]
                                                      

 49%|████▉     | 981/2000 [4:37:13<4:46:41, 16.88s/it]
 49%|████▉     | 982/2000 [4:37:30<4:46:06, 16.86s/it]
                                                      

 49%|████▉     | 982/2000 [4:37:30<4:46:06, 16.86s/it]
 49%|████▉     | 983/2000 [4:37:47<4:45:23, 16.84s/it]
                                                      

 49%|████▉     | 983/2000 [4:37:47<4:45:23, 16.84s/it]
 49%|████▉     | 984/2000 [4:38:04<4:44:56, 16.83s/it]
                                                      

 49%|████▉     | 984/2000 [4:38:04<4:44:56, 16.83s/it]
 49%|████▉     | 985/2000 [4:38:20<4:44:39, 16.83s/it]
                                                      

 49%|████▉     | 985/2000 [4:38:20<4:44:39, 16.83s/it]
 49%|████▉     | 986/2000 [4:38:37<4:44:52, 16.86s/it]
                                                      

 49%|████▉     | 986/2000 [4:38:37<4:44:52, 16.86s/it]
 49%|████▉     | 987/2000 [4:38:54<4:44:22, 16.84s/it]
                                                      

 49%|████▉     | 987/2000 [4:38:54<4:44:22, 16.84s/it]
 49%|████▉     | 988/2000 [4:39:11<4:44:01, 16.84s/it]
                                                      

 49%|████▉     | 988/2000 [4:39:11<4:44:01, 16.84s/it]
 49%|████▉     | 989/2000 [4:39:28<4:43:34, 16.83s/it]
                                                      

 49%|████▉     | 989/2000 [4:39:28<4:43:34, 16.83s/it]
 50%|████▉     | 990/2000 [4:39:45<4:43:17, 16.83s/it]
                                                      

 50%|████▉     | 990/2000 [4:39:45<4:43:17, 16.83s/it]
 50%|████▉     | 991/2000 [4:40:02<4:43:25, 16.85s/it]
                                                      

 50%|████▉     | 991/2000 [4:40:02<4:43:25, 16.85s/it]
 50%|████▉     | 992/2000 [4:40:18<4:43:28, 16.87s/it]
                                                      

 50%|████▉     | 992/2000 [4:40:18<4:43:28, 16.87s/it]
 50%|████▉     | 993/2000 [4:40:35<4:43:27, 16.89s/it]
                                                      

 50%|████▉     | 993/2000 [4:40:35<4:43:27, 16.89s/it]
 50%|████▉     | 994/2000 [4:40:52<4:42:43, 16.86s/it]
                                                      

 50%|████▉     | 994/2000 [4:40:52<4:42:43, 16.86s/it]
 50%|████▉     | 995/2000 [4:41:09<4:42:06, 16.84s/it]
                                                      

 50%|████▉     | 995/2000 [4:41:09<4:42:06, 16.84s/it]
 50%|████▉     | 996/2000 [4:41:26<4:41:29, 16.82s/it]
                                                      

 50%|████▉     | 996/2000 [4:41:26<4:41:29, 16.82s/it]
 50%|████▉     | 997/2000 [4:41:43<4:41:44, 16.85s/it]
                                                      

 50%|████▉     | 997/2000 [4:41:43<4:41:44, 16.85s/it]
 50%|████▉     | 998/2000 [4:41:59<4:41:04, 16.83s/it]
                                                      

 50%|████▉     | 998/2000 [4:41:59<4:41:04, 16.83s/it]
 50%|████▉     | 999/2000 [4:42:17<4:42:13, 16.92s/it]
                                                      

 50%|████▉     | 999/2000 [4:42:17<4:42:13, 16.92s/it]
 50%|█████     | 1000/2000 [4:42:33<4:41:12, 16.87s/it]
                                                       

 50%|█████     | 1000/2000 [4:42:33<4:41:12, 16.87s/it]
 50%|█████     | 1001/2000 [4:42:50<4:41:36, 16.91s/it]
                                                       

 50%|█████     | 1001/2000 [4:42:50<4:41:36, 16.91s/it]
 50%|█████     | 1002/2000 [4:43:07<4:40:51, 16.89s/it]
                                                       

 50%|█████     | 1002/2000 [4:43:07<4:40:51, 16.89s/it]
 50%|█████     | 1003/2000 [4:43:24<4:40:32, 16.88s/it]
                                                       

 50%|█████     | 1003/2000 [4:43:24<4:40:32, 16.88s/it]
 50%|█████     | 1004/2000 [4:43:41<4:40:06, 16.87s/it]
                                                       

 50%|█████     | 1004/2000 [4:43:41<4:40:06, 16.87s/it]
 50%|█████     | 1005/2000 [4:43:58<4:39:28, 16.85s/it]
                                                       

 50%|█████     | 1005/2000 [4:43:58<4:39:28, 16.85s/it]
 50%|█████     | 1006/2000 [4:44:14<4:38:52, 16.83s/it]
                                                       

 50%|█████     | 1006/2000 [4:44:14<4:38:52, 16.83s/it]
 50%|█████     | 1007/2000 [4:44:32<4:40:20, 16.94s/it]
                                                       

 50%|█████     | 1007/2000 [4:44:32<4:40:20, 16.94s/it]
 50%|█████     | 1008/2000 [4:44:49<4:39:49, 16.93s/it]
                                                       

 50%|█████     | 1008/2000 [4:44:49<4:39:49, 16.93s/it]
 50%|█████     | 1009/2000 [4:45:05<4:39:10, 16.90s/it]
                                                       

 50%|█████     | 1009/2000 [4:45:05<4:39:10, 16.90s/it]
 50%|█████     | 1010/2000 [4:45:22<4:38:29, 16.88s/it]
                                                       

 50%|█████     | 1010/2000 [4:45:22<4:38:29, 16.88s/it]
 51%|█████     | 1011/2000 [4:45:39<4:38:16, 16.88s/it]
                                                       

 51%|█████     | 1011/2000 [4:45:39<4:38:16, 16.88s/it]
 51%|█████     | 1012/2000 [4:45:56<4:38:29, 16.91s/it]
                                                       

 51%|█████     | 1012/2000 [4:45:56<4:38:29, 16.91s/it]
 51%|█████     | 1013/2000 [4:46:13<4:37:50, 16.89s/it]
                                                       

 51%|█████     | 1013/2000 [4:46:13<4:37:50, 16.89s/it]
 51%|█████     | 1014/2000 [4:46:30<4:37:01, 16.86s/it]
                                                       

 51%|█████     | 1014/2000 [4:46:30<4:37:01, 16.86s/it]
 51%|█████     | 1015/2000 [4:46:47<4:36:44, 16.86s/it]
                                                       

 51%|█████     | 1015/2000 [4:46:47<4:36:44, 16.86s/it]
 51%|█████     | 1016/2000 [4:47:03<4:36:39, 16.87s/it]
                                                       

 51%|█████     | 1016/2000 [4:47:03<4:36:39, 16.87s/it]
 51%|█████     | 1017/2000 [4:47:20<4:36:07, 16.85s/it]
                                                       

 51%|█████     | 1017/2000 [4:47:20<4:36:07, 16.85s/it]
 51%|█████     | 1018/2000 [4:47:37<4:35:29, 16.83s/it]
                                                       

 51%|█████     | 1018/2000 [4:47:37<4:35:29, 16.83s/it]
 51%|█████     | 1019/2000 [4:47:54<4:35:28, 16.85s/it]
                                                       

 51%|█████     | 1019/2000 [4:47:54<4:35:28, 16.85s/it]
 51%|█████     | 1020/2000 [4:48:11<4:35:44, 16.88s/it]
                                                       

 51%|█████     | 1020/2000 [4:48:11<4:35:44, 16.88s/it]
 51%|█████     | 1021/2000 [4:48:28<4:35:31, 16.89s/it]
                                                       

 51%|█████     | 1021/2000 [4:48:28<4:35:31, 16.89s/it]
 51%|█████     | 1022/2000 [4:48:45<4:34:54, 16.87s/it]
                                                       

 51%|█████     | 1022/2000 [4:48:45<4:34:54, 16.87s/it]
 51%|█████     | 1023/2000 [4:49:01<4:34:01, 16.83s/it]
                                                       

 51%|█████     | 1023/2000 [4:49:01<4:34:01, 16.83s/it]
 51%|█████     | 1024/2000 [4:49:18<4:33:32, 16.82s/it]
                                                       

 51%|█████     | 1024/2000 [4:49:18<4:33:32, 16.82s/it]
 51%|█████▏    | 1025/2000 [4:49:35<4:33:13, 16.81s/it]
                                                       

 51%|█████▏    | 1025/2000 [4:49:35<4:33:13, 16.81s/it]
 51%|█████▏    | 1026/2000 [4:49:52<4:32:43, 16.80s/it]
                                                       

 51%|█████▏    | 1026/2000 [4:49:52<4:32:43, 16.80s/it]
 51%|█████▏    | 1027/2000 [4:50:09<4:32:28, 16.80s/it]
                                                       

 51%|█████▏    | 1027/2000 [4:50:09<4:32:28, 16.80s/it]
 51%|█████▏    | 1028/2000 [4:50:25<4:31:59, 16.79s/it]
                                                       

 51%|█████▏    | 1028/2000 [4:50:25<4:31:59, 16.79s/it]
 51%|█████▏    | 1029/2000 [4:50:42<4:31:28, 16.78s/it]
                                                       

 51%|█████▏    | 1029/2000 [4:50:42<4:31:28, 16.78s/it]
 52%|█████▏    | 1030/2000 [4:50:59<4:31:24, 16.79s/it]
                                                       

 52%|█████▏    | 1030/2000 [4:50:59<4:31:24, 16.79s/it]
 52%|█████▏    | 1031/2000 [4:51:16<4:31:13, 16.79s/it]
                                                       

 52%|█████▏    | 1031/2000 [4:51:16<4:31:13, 16.79s/it]
 52%|█████▏    | 1032/2000 [4:51:32<4:30:49, 16.79s/it]
                                                       

 52%|█████▏    | 1032/2000 [4:51:32<4:30:49, 16.79s/it]
 52%|█████▏    | 1033/2000 [4:51:49<4:30:22, 16.78s/it]
                                                       

 52%|█████▏    | 1033/2000 [4:51:49<4:30:22, 16.78s/it]
 52%|█████▏    | 1034/2000 [4:52:06<4:29:42, 16.75s/it]
                                                       

 52%|█████▏    | 1034/2000 [4:52:06<4:29:42, 16.75s/it]
 52%|█████▏    | 1035/2000 [4:52:23<4:29:40, 16.77s/it]
                                                       

 52%|█████▏    | 1035/2000 [4:52:23<4:29:40, 16.77s/it]
 52%|█████▏    | 1036/2000 [4:52:40<4:29:31, 16.78s/it]
                                                       

 52%|█████▏    | 1036/2000 [4:52:40<4:29:31, 16.78s/it]
 52%|█████▏    | 1037/2000 [4:52:56<4:29:09, 16.77s/it]
                                                       

 52%|█████▏    | 1037/2000 [4:52:56<4:29:09, 16.77s/it]
 52%|█████▏    | 1038/2000 [4:53:13<4:29:03, 16.78s/it]
                                                       

 52%|█████▏    | 1038/2000 [4:53:13<4:29:03, 16.78s/it]
 52%|█████▏    | 1039/2000 [4:53:30<4:28:42, 16.78s/it]
                                                       

 52%|█████▏    | 1039/2000 [4:53:30<4:28:42, 16.78s/it]
 52%|█████▏    | 1040/2000 [4:53:47<4:28:48, 16.80s/it]
                                                       

 52%|█████▏    | 1040/2000 [4:53:47<4:28:48, 16.80s/it]
 52%|█████▏    | 1041/2000 [4:54:06<4:40:45, 17.57s/it]
                                                       

 52%|█████▏    | 1041/2000 [4:54:06<4:40:45, 17.57s/it]
 52%|█████▏    | 1042/2000 [4:54:23<4:38:32, 17.44s/it]
                                                       

 52%|█████▏    | 1042/2000 [4:54:23<4:38:32, 17.44s/it]
 52%|█████▏    | 1043/2000 [4:54:40<4:35:08, 17.25s/it]
                                                       

 52%|█████▏    | 1043/2000 [4:54:40<4:35:08, 17.25s/it]
 52%|█████▏    | 1044/2000 [4:54:57<4:32:47, 17.12s/it]
                                                       

 52%|█████▏    | 1044/2000 [4:54:57<4:32:47, 17.12s/it]
 52%|█████▏    | 1045/2000 [4:55:14<4:31:10, 17.04s/it]
                                                       

 52%|█████▏    | 1045/2000 [4:55:14<4:31:10, 17.04s/it]
 52%|█████▏    | 1046/2000 [4:55:30<4:29:36, 16.96s/it]
                                                       

 52%|█████▏    | 1046/2000 [4:55:30<4:29:36, 16.96s/it]
 52%|█████▏    | 1047/2000 [4:55:47<4:28:40, 16.92s/it]
                                                       

 52%|█████▏    | 1047/2000 [4:55:47<4:28:40, 16.92s/it]
 52%|█████▏    | 1048/2000 [4:56:04<4:27:50, 16.88s/it]
                                                       

 52%|█████▏    | 1048/2000 [4:56:04<4:27:50, 16.88s/it]
 52%|█████▏    | 1049/2000 [4:56:21<4:27:19, 16.87s/it]
                                                       

 52%|█████▏    | 1049/2000 [4:56:21<4:27:19, 16.87s/it]
 52%|█████▎    | 1050/2000 [4:56:38<4:26:46, 16.85s/it]
                                                       

 52%|█████▎    | 1050/2000 [4:56:38<4:26:46, 16.85s/it]
 53%|█████▎    | 1051/2000 [4:56:54<4:26:11, 16.83s/it]
                                                       

 53%|█████▎    | 1051/2000 [4:56:54<4:26:11, 16.83s/it]
 53%|█████▎    | 1052/2000 [4:57:11<4:25:35, 16.81s/it]
                                                       

 53%|█████▎    | 1052/2000 [4:57:11<4:25:35, 16.81s/it]
 53%|█████▎    | 1053/2000 [4:57:28<4:25:45, 16.84s/it]
                                                       

 53%|█████▎    | 1053/2000 [4:57:28<4:25:45, 16.84s/it]
 53%|█████▎    | 1054/2000 [4:57:45<4:25:40, 16.85s/it]
                                                       

 53%|█████▎    | 1054/2000 [4:57:45<4:25:40, 16.85s/it]
 53%|█████▎    | 1055/2000 [4:58:02<4:25:23, 16.85s/it]
                                                       

 53%|█████▎    | 1055/2000 [4:58:02<4:25:23, 16.85s/it]
 53%|█████▎    | 1056/2000 [4:58:19<4:24:35, 16.82s/it]
                                                       

 53%|█████▎    | 1056/2000 [4:58:19<4:24:35, 16.82s/it]
 53%|█████▎    | 1057/2000 [4:58:35<4:24:14, 16.81s/it]
                                                       

 53%|█████▎    | 1057/2000 [4:58:35<4:24:14, 16.81s/it]
 53%|█████▎    | 1058/2000 [4:58:53<4:27:43, 17.05s/it]
                                                       

 53%|█████▎    | 1058/2000 [4:58:53<4:27:43, 17.05s/it]
 53%|█████▎    | 1059/2000 [4:59:10<4:26:40, 17.00s/it]
                                                       

 53%|█████▎    | 1059/2000 [4:59:10<4:26:40, 17.00s/it]
 53%|█████▎    | 1060/2000 [4:59:27<4:25:22, 16.94s/it]
                                                       

 53%|█████▎    | 1060/2000 [4:59:27<4:25:22, 16.94s/it]
 53%|█████▎    | 1061/2000 [4:59:44<4:24:56, 16.93s/it]
                                                       

 53%|█████▎    | 1061/2000 [4:59:44<4:24:56, 16.93s/it]
 53%|█████▎    | 1062/2000 [5:00:01<4:24:47, 16.94s/it]
                                                       

 53%|█████▎    | 1062/2000 [5:00:01<4:24:47, 16.94s/it]
 53%|█████▎    | 1063/2000 [5:00:17<4:24:08, 16.91s/it]
                                                       

 53%|█████▎    | 1063/2000 [5:00:17<4:24:08, 16.91s/it]
 53%|█████▎    | 1064/2000 [5:00:34<4:23:22, 16.88s/it]
                                                       

 53%|█████▎    | 1064/2000 [5:00:34<4:23:22, 16.88s/it]
 53%|█████▎    | 1065/2000 [5:00:51<4:23:02, 16.88s/it]
                                                       

 53%|█████▎    | 1065/2000 [5:00:51<4:23:02, 16.88s/it]
 53%|█████▎    | 1066/2000 [5:01:08<4:22:28, 16.86s/it]
                                                       

 53%|█████▎    | 1066/2000 [5:01:08<4:22:28, 16.86s/it]
 53%|█████▎    | 1067/2000 [5:01:25<4:23:36, 16.95s/it]
                                                       

 53%|█████▎    | 1067/2000 [5:01:25<4:23:36, 16.95s/it]
 53%|█████▎    | 1068/2000 [5:01:42<4:22:35, 16.90s/it]
                                                       

 53%|█████▎    | 1068/2000 [5:01:42<4:22:35, 16.90s/it]
 53%|█████▎    | 1069/2000 [5:01:59<4:22:01, 16.89s/it]
                                                       

 53%|█████▎    | 1069/2000 [5:01:59<4:22:01, 16.89s/it]
 54%|█████▎    | 1070/2000 [5:02:16<4:21:42, 16.88s/it]
                                                       

 54%|█████▎    | 1070/2000 [5:02:16<4:21:42, 16.88s/it]
 54%|█████▎    | 1071/2000 [5:02:33<4:21:45, 16.91s/it]
                                                       

 54%|█████▎    | 1071/2000 [5:02:33<4:21:45, 16.91s/it]
 54%|█████▎    | 1072/2000 [5:02:49<4:21:04, 16.88s/it]
                                                       

 54%|█████▎    | 1072/2000 [5:02:49<4:21:04, 16.88s/it]
 54%|█████▎    | 1073/2000 [5:03:06<4:21:44, 16.94s/it]
                                                       

 54%|█████▎    | 1073/2000 [5:03:06<4:21:44, 16.94s/it]
 54%|█████▎    | 1074/2000 [5:03:23<4:21:03, 16.92s/it]
                                                       

 54%|█████▎    | 1074/2000 [5:03:23<4:21:03, 16.92s/it]
 54%|█████▍    | 1075/2000 [5:03:40<4:20:24, 16.89s/it]
                                                       

 54%|█████▍    | 1075/2000 [5:03:40<4:20:24, 16.89s/it]
 54%|█████▍    | 1076/2000 [5:03:57<4:19:55, 16.88s/it]
                                                       

 54%|█████▍    | 1076/2000 [5:03:57<4:19:55, 16.88s/it]
 54%|█████▍    | 1077/2000 [5:04:14<4:19:38, 16.88s/it]
                                                       

 54%|█████▍    | 1077/2000 [5:04:14<4:19:38, 16.88s/it]
 54%|█████▍    | 1078/2000 [5:04:31<4:19:46, 16.90s/it]
                                                       

 54%|█████▍    | 1078/2000 [5:04:31<4:19:46, 16.90s/it]
 54%|█████▍    | 1079/2000 [5:04:48<4:19:01, 16.88s/it]
                                                       

 54%|█████▍    | 1079/2000 [5:04:48<4:19:01, 16.88s/it]
 54%|█████▍    | 1080/2000 [5:05:05<4:18:52, 16.88s/it]
                                                       

 54%|█████▍    | 1080/2000 [5:05:05<4:18:52, 16.88s/it]
 54%|█████▍    | 1081/2000 [5:05:21<4:18:37, 16.88s/it]
                                                       

 54%|█████▍    | 1081/2000 [5:05:21<4:18:37, 16.88s/it]
 54%|█████▍    | 1082/2000 [5:05:38<4:18:04, 16.87s/it]
                                                       

 54%|█████▍    | 1082/2000 [5:05:38<4:18:04, 16.87s/it]
 54%|█████▍    | 1083/2000 [5:05:55<4:17:46, 16.87s/it]
                                                       

 54%|█████▍    | 1083/2000 [5:05:55<4:17:46, 16.87s/it]
 54%|█████▍    | 1084/2000 [5:06:12<4:17:36, 16.87s/it]
                                                       

 54%|█████▍    | 1084/2000 [5:06:12<4:17:36, 16.87s/it]
 54%|█████▍    | 1085/2000 [5:06:29<4:16:59, 16.85s/it]
                                                       

 54%|█████▍    | 1085/2000 [5:06:29<4:16:59, 16.85s/it]
 54%|█████▍    | 1086/2000 [5:06:46<4:16:59, 16.87s/it]
                                                       

 54%|█████▍    | 1086/2000 [5:06:46<4:16:59, 16.87s/it]
 54%|█████▍    | 1087/2000 [5:07:03<4:17:09, 16.90s/it]
                                                       

 54%|█████▍    | 1087/2000 [5:07:03<4:17:09, 16.90s/it]
 54%|█████▍    | 1088/2000 [5:07:20<4:16:43, 16.89s/it]
                                                       

 54%|█████▍    | 1088/2000 [5:07:20<4:16:43, 16.89s/it]
 54%|█████▍    | 1089/2000 [5:07:36<4:16:12, 16.87s/it]
                                                       

 54%|█████▍    | 1089/2000 [5:07:36<4:16:12, 16.87s/it]
 55%|█████▍    | 1090/2000 [5:07:53<4:15:37, 16.85s/it]
                                                       

 55%|█████▍    | 1090/2000 [5:07:53<4:15:37, 16.85s/it]
 55%|█████▍    | 1091/2000 [5:08:10<4:15:27, 16.86s/it]
                                                       

 55%|█████▍    | 1091/2000 [5:08:10<4:15:27, 16.86s/it]
 55%|█████▍    | 1092/2000 [5:08:27<4:14:46, 16.83s/it]
                                                       

 55%|█████▍    | 1092/2000 [5:08:27<4:14:46, 16.83s/it]
 55%|█████▍    | 1093/2000 [5:08:44<4:14:43, 16.85s/it]
                                                       

 55%|█████▍    | 1093/2000 [5:08:44<4:14:43, 16.85s/it]
 55%|█████▍    | 1094/2000 [5:09:01<4:14:59, 16.89s/it]
                                                       

 55%|█████▍    | 1094/2000 [5:09:01<4:14:59, 16.89s/it]
 55%|█████▍    | 1095/2000 [5:09:18<4:14:10, 16.85s/it]
                                                       

 55%|█████▍    | 1095/2000 [5:09:18<4:14:10, 16.85s/it]
 55%|█████▍    | 1096/2000 [5:09:34<4:13:37, 16.83s/it]
                                                       

 55%|█████▍    | 1096/2000 [5:09:34<4:13:37, 16.83s/it]
 55%|█████▍    | 1097/2000 [5:09:51<4:13:15, 16.83s/it]
                                                       

 55%|█████▍    | 1097/2000 [5:09:51<4:13:15, 16.83s/it]
 55%|█████▍    | 1098/2000 [5:10:08<4:12:50, 16.82s/it]
                                                       

 55%|█████▍    | 1098/2000 [5:10:08<4:12:50, 16.82s/it]
 55%|█████▍    | 1099/2000 [5:10:25<4:12:33, 16.82s/it]
                                                       

 55%|█████▍    | 1099/2000 [5:10:25<4:12:33, 16.82s/it]
 55%|█████▌    | 1100/2000 [5:10:42<4:12:09, 16.81s/it]
                                                       

 55%|█████▌    | 1100/2000 [5:10:42<4:12:09, 16.81s/it]
 55%|█████▌    | 1101/2000 [5:10:58<4:11:56, 16.82s/it]
                                                       

 55%|█████▌    | 1101/2000 [5:10:58<4:11:56, 16.82s/it]
 55%|█████▌    | 1102/2000 [5:11:15<4:11:36, 16.81s/it]
                                                       

 55%|█████▌    | 1102/2000 [5:11:15<4:11:36, 16.81s/it]
 55%|█████▌    | 1103/2000 [5:11:32<4:11:23, 16.82s/it]
                                                       

 55%|█████▌    | 1103/2000 [5:11:32<4:11:23, 16.82s/it]
 55%|█████▌    | 1104/2000 [5:11:49<4:11:03, 16.81s/it]
                                                       

 55%|█████▌    | 1104/2000 [5:11:49<4:11:03, 16.81s/it]
 55%|█████▌    | 1105/2000 [5:12:06<4:10:37, 16.80s/it]
                                                       

 55%|█████▌    | 1105/2000 [5:12:06<4:10:37, 16.80s/it]
 55%|█████▌    | 1106/2000 [5:12:22<4:10:14, 16.80s/it]
                                                       

 55%|█████▌    | 1106/2000 [5:12:22<4:10:14, 16.80s/it]
 55%|█████▌    | 1107/2000 [5:12:39<4:10:10, 16.81s/it]
                                                       

 55%|█████▌    | 1107/2000 [5:12:39<4:10:10, 16.81s/it]
 55%|█████▌    | 1108/2000 [5:12:56<4:09:37, 16.79s/it]
                                                       

 55%|█████▌    | 1108/2000 [5:12:56<4:09:37, 16.79s/it]
 55%|█████▌    | 1109/2000 [5:13:13<4:09:23, 16.79s/it]
                                                       

 55%|█████▌    | 1109/2000 [5:13:13<4:09:23, 16.79s/it]
 56%|█████▌    | 1110/2000 [5:13:29<4:08:50, 16.78s/it]
                                                       

 56%|█████▌    | 1110/2000 [5:13:29<4:08:50, 16.78s/it]
 56%|█████▌    | 1111/2000 [5:13:47<4:13:20, 17.10s/it]
                                                       

 56%|█████▌    | 1111/2000 [5:13:47<4:13:20, 17.10s/it]
 56%|█████▌    | 1112/2000 [5:14:04<4:11:20, 16.98s/it]
                                                       

 56%|█████▌    | 1112/2000 [5:14:04<4:11:20, 16.98s/it]
 56%|█████▌    | 1113/2000 [5:14:21<4:11:08, 16.99s/it]
                                                       

 56%|█████▌    | 1113/2000 [5:14:21<4:11:08, 16.99s/it]
 56%|█████▌    | 1114/2000 [5:14:38<4:09:51, 16.92s/it]
                                                       

 56%|█████▌    | 1114/2000 [5:14:38<4:09:51, 16.92s/it]
 56%|█████▌    | 1115/2000 [5:14:55<4:10:11, 16.96s/it]
                                                       

 56%|█████▌    | 1115/2000 [5:14:55<4:10:11, 16.96s/it]
 56%|█████▌    | 1116/2000 [5:15:12<4:08:52, 16.89s/it]
                                                       

 56%|█████▌    | 1116/2000 [5:15:12<4:08:52, 16.89s/it]
 56%|█████▌    | 1117/2000 [5:15:28<4:08:06, 16.86s/it]
                                                       

 56%|█████▌    | 1117/2000 [5:15:28<4:08:06, 16.86s/it]
 56%|█████▌    | 1118/2000 [5:15:45<4:07:18, 16.82s/it]
                                                       

 56%|█████▌    | 1118/2000 [5:15:45<4:07:18, 16.82s/it]
 56%|█████▌    | 1119/2000 [5:16:02<4:06:50, 16.81s/it]
                                                       

 56%|█████▌    | 1119/2000 [5:16:02<4:06:50, 16.81s/it]
 56%|█████▌    | 1120/2000 [5:16:19<4:06:47, 16.83s/it]
                                                       

 56%|█████▌    | 1120/2000 [5:16:19<4:06:47, 16.83s/it]
 56%|█████▌    | 1121/2000 [5:16:38<4:19:12, 17.69s/it]
                                                       

 56%|█████▌    | 1121/2000 [5:16:38<4:19:12, 17.69s/it]
 56%|█████▌    | 1122/2000 [5:16:56<4:17:17, 17.58s/it]
                                                       

 56%|█████▌    | 1122/2000 [5:16:56<4:17:17, 17.58s/it]
 56%|█████▌    | 1123/2000 [5:17:13<4:14:06, 17.39s/it]
                                                       

 56%|█████▌    | 1123/2000 [5:17:13<4:14:06, 17.39s/it]
 56%|█████▌    | 1124/2000 [5:17:30<4:11:29, 17.23s/it]
                                                       

 56%|█████▌    | 1124/2000 [5:17:30<4:11:29, 17.23s/it]
 56%|█████▋    | 1125/2000 [5:17:46<4:09:20, 17.10s/it]
                                                       

 56%|█████▋    | 1125/2000 [5:17:46<4:09:20, 17.10s/it]
 56%|█████▋    | 1126/2000 [5:18:03<4:07:41, 17.00s/it]
                                                       

 56%|█████▋    | 1126/2000 [5:18:03<4:07:41, 17.00s/it]
 56%|█████▋    | 1127/2000 [5:18:20<4:06:22, 16.93s/it]
                                                       

 56%|█████▋    | 1127/2000 [5:18:20<4:06:22, 16.93s/it]
 56%|█████▋    | 1128/2000 [5:18:37<4:05:22, 16.88s/it]
                                                       

 56%|█████▋    | 1128/2000 [5:18:37<4:05:22, 16.88s/it]
 56%|█████▋    | 1129/2000 [5:18:53<4:04:44, 16.86s/it]
                                                       

 56%|█████▋    | 1129/2000 [5:18:53<4:04:44, 16.86s/it]
 56%|█████▋    | 1130/2000 [5:19:10<4:04:06, 16.83s/it]
                                                       

 56%|█████▋    | 1130/2000 [5:19:10<4:04:06, 16.83s/it]
 57%|█████▋    | 1131/2000 [5:19:27<4:03:36, 16.82s/it]
                                                       

 57%|█████▋    | 1131/2000 [5:19:27<4:03:36, 16.82s/it]
 57%|█████▋    | 1132/2000 [5:19:44<4:03:32, 16.83s/it]
                                                       

 57%|█████▋    | 1132/2000 [5:19:44<4:03:32, 16.83s/it]
 57%|█████▋    | 1133/2000 [5:20:01<4:03:50, 16.87s/it]
                                                       

 57%|█████▋    | 1133/2000 [5:20:01<4:03:50, 16.87s/it]
 57%|█████▋    | 1134/2000 [5:20:18<4:03:19, 16.86s/it]
                                                       

 57%|█████▋    | 1134/2000 [5:20:18<4:03:19, 16.86s/it]
 57%|█████▋    | 1135/2000 [5:20:35<4:02:55, 16.85s/it]
                                                       

 57%|█████▋    | 1135/2000 [5:20:35<4:02:55, 16.85s/it]
 57%|█████▋    | 1136/2000 [5:20:51<4:02:42, 16.86s/it]
                                                       

 57%|█████▋    | 1136/2000 [5:20:51<4:02:42, 16.86s/it]
 57%|█████▋    | 1137/2000 [5:21:08<4:02:48, 16.88s/it]
                                                       

 57%|█████▋    | 1137/2000 [5:21:08<4:02:48, 16.88s/it]
 57%|█████▋    | 1138/2000 [5:21:25<4:02:28, 16.88s/it]
                                                       

 57%|█████▋    | 1138/2000 [5:21:25<4:02:28, 16.88s/it]
 57%|█████▋    | 1139/2000 [5:21:42<4:01:35, 16.84s/it]
                                                       

 57%|█████▋    | 1139/2000 [5:21:42<4:01:35, 16.84s/it]
 57%|█████▋    | 1140/2000 [5:21:59<4:01:13, 16.83s/it]
                                                       

 57%|█████▋    | 1140/2000 [5:21:59<4:01:13, 16.83s/it]
 57%|█████▋    | 1141/2000 [5:22:16<4:01:20, 16.86s/it]
                                                       

 57%|█████▋    | 1141/2000 [5:22:16<4:01:20, 16.86s/it]
 57%|█████▋    | 1142/2000 [5:22:33<4:01:30, 16.89s/it]
                                                       

 57%|█████▋    | 1142/2000 [5:22:33<4:01:30, 16.89s/it]
 57%|█████▋    | 1143/2000 [5:22:49<4:00:54, 16.87s/it]
                                                       

 57%|█████▋    | 1143/2000 [5:22:49<4:00:54, 16.87s/it]
 57%|█████▋    | 1144/2000 [5:23:06<4:00:42, 16.87s/it]
                                                       

 57%|█████▋    | 1144/2000 [5:23:06<4:00:42, 16.87s/it]
 57%|█████▋    | 1145/2000 [5:23:23<4:00:28, 16.88s/it]
                                                       

 57%|█████▋    | 1145/2000 [5:23:23<4:00:28, 16.88s/it]
 57%|█████▋    | 1146/2000 [5:23:40<3:59:58, 16.86s/it]
                                                       

 57%|█████▋    | 1146/2000 [5:23:40<3:59:58, 16.86s/it]
 57%|█████▋    | 1147/2000 [5:23:57<3:59:25, 16.84s/it]
                                                       

 57%|█████▋    | 1147/2000 [5:23:57<3:59:25, 16.84s/it]
 57%|█████▋    | 1148/2000 [5:24:14<3:59:00, 16.83s/it]
                                                       

 57%|█████▋    | 1148/2000 [5:24:14<3:59:00, 16.83s/it]
 57%|█████▋    | 1149/2000 [5:24:31<3:58:55, 16.85s/it]
                                                       

 57%|█████▋    | 1149/2000 [5:24:31<3:58:55, 16.85s/it]
 57%|█████▊    | 1150/2000 [5:24:47<3:58:53, 16.86s/it]
                                                       

 57%|█████▊    | 1150/2000 [5:24:47<3:58:53, 16.86s/it]
 58%|█████▊    | 1151/2000 [5:25:04<3:58:51, 16.88s/it]
                                                       

 58%|█████▊    | 1151/2000 [5:25:04<3:58:51, 16.88s/it]
 58%|█████▊    | 1152/2000 [5:25:21<3:58:18, 16.86s/it]
                                                       

 58%|█████▊    | 1152/2000 [5:25:21<3:58:18, 16.86s/it]
 58%|█████▊    | 1153/2000 [5:25:38<3:57:46, 16.84s/it]
                                                       

 58%|█████▊    | 1153/2000 [5:25:38<3:57:46, 16.84s/it]
 58%|█████▊    | 1154/2000 [5:25:55<3:57:20, 16.83s/it]
                                                       

 58%|█████▊    | 1154/2000 [5:25:55<3:57:20, 16.83s/it]
 58%|█████▊    | 1155/2000 [5:26:12<3:57:02, 16.83s/it]
                                                       

 58%|█████▊    | 1155/2000 [5:26:12<3:57:02, 16.83s/it]
 58%|█████▊    | 1156/2000 [5:26:28<3:56:43, 16.83s/it]
                                                       

 58%|█████▊    | 1156/2000 [5:26:28<3:56:43, 16.83s/it]
 58%|█████▊    | 1157/2000 [5:26:45<3:56:40, 16.84s/it]
                                                       

 58%|█████▊    | 1157/2000 [5:26:45<3:56:40, 16.84s/it]
 58%|█████▊    | 1158/2000 [5:27:02<3:56:43, 16.87s/it]
                                                       

 58%|█████▊    | 1158/2000 [5:27:02<3:56:43, 16.87s/it]
 58%|█████▊    | 1159/2000 [5:27:19<3:56:16, 16.86s/it]
                                                       

 58%|█████▊    | 1159/2000 [5:27:19<3:56:16, 16.86s/it]
 58%|█████▊    | 1160/2000 [5:27:36<3:55:36, 16.83s/it]
                                                       

 58%|█████▊    | 1160/2000 [5:27:36<3:55:36, 16.83s/it]
 58%|█████▊    | 1161/2000 [5:27:53<3:54:59, 16.81s/it]
                                                       

 58%|█████▊    | 1161/2000 [5:27:53<3:54:59, 16.81s/it]
 58%|█████▊    | 1162/2000 [5:28:09<3:54:40, 16.80s/it]
                                                       

 58%|█████▊    | 1162/2000 [5:28:09<3:54:40, 16.80s/it]
 58%|█████▊    | 1163/2000 [5:28:26<3:54:49, 16.83s/it]
                                                       

 58%|█████▊    | 1163/2000 [5:28:26<3:54:49, 16.83s/it]
 58%|█████▊    | 1164/2000 [5:28:43<3:54:53, 16.86s/it]
                                                       

 58%|█████▊    | 1164/2000 [5:28:43<3:54:53, 16.86s/it]
 58%|█████▊    | 1165/2000 [5:29:00<3:54:23, 16.84s/it]
                                                       

 58%|█████▊    | 1165/2000 [5:29:00<3:54:23, 16.84s/it]
 58%|█████▊    | 1166/2000 [5:29:17<3:53:50, 16.82s/it]
                                                       

 58%|█████▊    | 1166/2000 [5:29:17<3:53:50, 16.82s/it]
 58%|█████▊    | 1167/2000 [5:29:34<3:53:24, 16.81s/it]
                                                       

 58%|█████▊    | 1167/2000 [5:29:34<3:53:24, 16.81s/it]
 58%|█████▊    | 1168/2000 [5:29:50<3:52:56, 16.80s/it]
                                                       

 58%|█████▊    | 1168/2000 [5:29:50<3:52:56, 16.80s/it]
 58%|█████▊    | 1169/2000 [5:30:07<3:52:39, 16.80s/it]
                                                       

 58%|█████▊    | 1169/2000 [5:30:07<3:52:39, 16.80s/it]
 58%|█████▊    | 1170/2000 [5:30:24<3:52:20, 16.80s/it]
                                                       

 58%|█████▊    | 1170/2000 [5:30:24<3:52:20, 16.80s/it]
 59%|█████▊    | 1171/2000 [5:30:41<3:52:09, 16.80s/it]
                                                       

 59%|█████▊    | 1171/2000 [5:30:41<3:52:09, 16.80s/it]
 59%|█████▊    | 1172/2000 [5:30:58<3:51:54, 16.81s/it]
                                                       

 59%|█████▊    | 1172/2000 [5:30:58<3:51:54, 16.81s/it]
 59%|█████▊    | 1173/2000 [5:31:14<3:51:25, 16.79s/it]
                                                       

 59%|█████▊    | 1173/2000 [5:31:14<3:51:25, 16.79s/it]
 59%|█████▊    | 1174/2000 [5:31:31<3:51:13, 16.80s/it]
                                                       

 59%|█████▊    | 1174/2000 [5:31:31<3:51:13, 16.80s/it]
 59%|█████▉    | 1175/2000 [5:31:48<3:50:48, 16.79s/it]
                                                       

 59%|█████▉    | 1175/2000 [5:31:48<3:50:48, 16.79s/it]
 59%|█████▉    | 1176/2000 [5:32:05<3:50:15, 16.77s/it]
                                                       

 59%|█████▉    | 1176/2000 [5:32:05<3:50:15, 16.77s/it]
 59%|█████▉    | 1177/2000 [5:32:21<3:50:04, 16.77s/it]
                                                       

 59%|█████▉    | 1177/2000 [5:32:21<3:50:04, 16.77s/it]
 59%|█████▉    | 1178/2000 [5:32:38<3:49:47, 16.77s/it]
                                                       

 59%|█████▉    | 1178/2000 [5:32:38<3:49:47, 16.77s/it]
 59%|█████▉    | 1179/2000 [5:32:55<3:49:35, 16.78s/it]
                                                       

 59%|█████▉    | 1179/2000 [5:32:55<3:49:35, 16.78s/it]
 59%|█████▉    | 1180/2000 [5:33:12<3:49:22, 16.78s/it]
                                                       

 59%|█████▉    | 1180/2000 [5:33:12<3:49:22, 16.78s/it]
 59%|█████▉    | 1181/2000 [5:33:29<3:49:16, 16.80s/it]
                                                       

 59%|█████▉    | 1181/2000 [5:33:29<3:49:16, 16.80s/it]
 59%|█████▉    | 1182/2000 [5:33:45<3:48:56, 16.79s/it]
                                                       

 59%|█████▉    | 1182/2000 [5:33:45<3:48:56, 16.79s/it]
 59%|█████▉    | 1183/2000 [5:34:02<3:48:40, 16.79s/it]
                                                       

 59%|█████▉    | 1183/2000 [5:34:02<3:48:40, 16.79s/it]
 59%|█████▉    | 1184/2000 [5:34:19<3:48:30, 16.80s/it]
                                                       

 59%|█████▉    | 1184/2000 [5:34:19<3:48:30, 16.80s/it]
 59%|█████▉    | 1185/2000 [5:34:36<3:48:17, 16.81s/it]
                                                       

 59%|█████▉    | 1185/2000 [5:34:36<3:48:17, 16.81s/it]
 59%|█████▉    | 1186/2000 [5:34:53<3:47:48, 16.79s/it]
                                                       

 59%|█████▉    | 1186/2000 [5:34:53<3:47:48, 16.79s/it]
 59%|█████▉    | 1187/2000 [5:35:09<3:47:31, 16.79s/it]
                                                       

 59%|█████▉    | 1187/2000 [5:35:09<3:47:31, 16.79s/it]
 59%|█████▉    | 1188/2000 [5:35:26<3:47:06, 16.78s/it]
                                                       

 59%|█████▉    | 1188/2000 [5:35:26<3:47:06, 16.78s/it]
 59%|█████▉    | 1189/2000 [5:35:43<3:46:45, 16.78s/it]
                                                       

 59%|█████▉    | 1189/2000 [5:35:43<3:46:45, 16.78s/it]
 60%|█████▉    | 1190/2000 [5:36:00<3:46:34, 16.78s/it]
                                                       

 60%|█████▉    | 1190/2000 [5:36:00<3:46:34, 16.78s/it]
 60%|█████▉    | 1191/2000 [5:36:17<3:46:39, 16.81s/it]
                                                       

 60%|█████▉    | 1191/2000 [5:36:17<3:46:39, 16.81s/it]
 60%|█████▉    | 1192/2000 [5:36:33<3:46:13, 16.80s/it]
                                                       

 60%|█████▉    | 1192/2000 [5:36:33<3:46:13, 16.80s/it]
 60%|█████▉    | 1193/2000 [5:36:50<3:45:56, 16.80s/it]
                                                       

 60%|█████▉    | 1193/2000 [5:36:50<3:45:56, 16.80s/it]
 60%|█████▉    | 1194/2000 [5:37:07<3:45:40, 16.80s/it]
                                                       

 60%|█████▉    | 1194/2000 [5:37:07<3:45:40, 16.80s/it]
 60%|█████▉    | 1195/2000 [5:37:24<3:45:23, 16.80s/it]
                                                       

 60%|█████▉    | 1195/2000 [5:37:24<3:45:23, 16.80s/it]
 60%|█████▉    | 1196/2000 [5:37:40<3:44:57, 16.79s/it]
                                                       

 60%|█████▉    | 1196/2000 [5:37:40<3:44:57, 16.79s/it]
 60%|█████▉    | 1197/2000 [5:37:57<3:44:41, 16.79s/it]
                                                       

 60%|█████▉    | 1197/2000 [5:37:57<3:44:41, 16.79s/it]
 60%|█████▉    | 1198/2000 [5:38:14<3:44:32, 16.80s/it]
                                                       

 60%|█████▉    | 1198/2000 [5:38:14<3:44:32, 16.80s/it]
 60%|█████▉    | 1199/2000 [5:38:31<3:44:18, 16.80s/it]
                                                       

 60%|█████▉    | 1199/2000 [5:38:31<3:44:18, 16.80s/it]
 60%|██████    | 1200/2000 [5:38:48<3:44:23, 16.83s/it]
                                                       

 60%|██████    | 1200/2000 [5:38:48<3:44:23, 16.83s/it]
 60%|██████    | 1201/2000 [5:39:07<3:54:55, 17.64s/it]
                                                       

 60%|██████    | 1201/2000 [5:39:07<3:54:55, 17.64s/it]
 60%|██████    | 1202/2000 [5:39:24<3:52:18, 17.47s/it]
                                                       

 60%|██████    | 1202/2000 [5:39:24<3:52:18, 17.47s/it]
 60%|██████    | 1203/2000 [5:39:41<3:49:12, 17.26s/it]
                                                       

 60%|██████    | 1203/2000 [5:39:41<3:49:12, 17.26s/it]
 60%|██████    | 1204/2000 [5:39:58<3:47:26, 17.14s/it]
                                                       

 60%|██████    | 1204/2000 [5:39:58<3:47:26, 17.14s/it]
 60%|██████    | 1205/2000 [5:40:15<3:45:47, 17.04s/it]
                                                       

 60%|██████    | 1205/2000 [5:40:15<3:45:47, 17.04s/it]
 60%|██████    | 1206/2000 [5:40:32<3:44:27, 16.96s/it]
                                                       

 60%|██████    | 1206/2000 [5:40:32<3:44:27, 16.96s/it]
 60%|██████    | 1207/2000 [5:40:48<3:43:31, 16.91s/it]
                                                       

 60%|██████    | 1207/2000 [5:40:48<3:43:31, 16.91s/it]
 60%|██████    | 1208/2000 [5:41:05<3:42:43, 16.87s/it]
                                                       

 60%|██████    | 1208/2000 [5:41:05<3:42:43, 16.87s/it]
 60%|██████    | 1209/2000 [5:41:22<3:42:09, 16.85s/it]
                                                       

 60%|██████    | 1209/2000 [5:41:22<3:42:09, 16.85s/it]
 60%|██████    | 1210/2000 [5:41:39<3:41:42, 16.84s/it]
                                                       

 60%|██████    | 1210/2000 [5:41:39<3:41:42, 16.84s/it]
 61%|██████    | 1211/2000 [5:41:56<3:41:21, 16.83s/it]
                                                       

 61%|██████    | 1211/2000 [5:41:56<3:41:21, 16.83s/it]
 61%|██████    | 1212/2000 [5:42:12<3:40:56, 16.82s/it]
                                                       

 61%|██████    | 1212/2000 [5:42:12<3:40:56, 16.82s/it]
 61%|██████    | 1213/2000 [5:42:29<3:40:39, 16.82s/it]
                                                       

 61%|██████    | 1213/2000 [5:42:29<3:40:39, 16.82s/it]
 61%|██████    | 1214/2000 [5:42:46<3:40:16, 16.81s/it]
                                                       

 61%|██████    | 1214/2000 [5:42:46<3:40:16, 16.81s/it]
 61%|██████    | 1215/2000 [5:43:03<3:40:18, 16.84s/it]
                                                       

 61%|██████    | 1215/2000 [5:43:03<3:40:18, 16.84s/it]
 61%|██████    | 1216/2000 [5:43:20<3:40:18, 16.86s/it]
                                                       

 61%|██████    | 1216/2000 [5:43:20<3:40:18, 16.86s/it]
 61%|██████    | 1217/2000 [5:43:37<3:40:04, 16.86s/it]
                                                       

 61%|██████    | 1217/2000 [5:43:37<3:40:04, 16.86s/it]
 61%|██████    | 1218/2000 [5:43:54<3:39:52, 16.87s/it]
                                                       

 61%|██████    | 1218/2000 [5:43:54<3:39:52, 16.87s/it]
 61%|██████    | 1219/2000 [5:44:10<3:39:40, 16.88s/it]
                                                       

 61%|██████    | 1219/2000 [5:44:10<3:39:40, 16.88s/it]
 61%|██████    | 1220/2000 [5:44:27<3:39:09, 16.86s/it]
                                                       

 61%|██████    | 1220/2000 [5:44:27<3:39:09, 16.86s/it]
 61%|██████    | 1221/2000 [5:44:44<3:39:02, 16.87s/it]
                                                       

 61%|██████    | 1221/2000 [5:44:44<3:39:02, 16.87s/it]
 61%|██████    | 1222/2000 [5:45:01<3:38:56, 16.89s/it]
                                                       

 61%|██████    | 1222/2000 [5:45:01<3:38:56, 16.89s/it]
 61%|██████    | 1223/2000 [5:45:18<3:38:19, 16.86s/it]
                                                       

 61%|██████    | 1223/2000 [5:45:18<3:38:19, 16.86s/it]
 61%|██████    | 1224/2000 [5:45:35<3:37:53, 16.85s/it]
                                                       

 61%|██████    | 1224/2000 [5:45:35<3:37:53, 16.85s/it]
 61%|██████▏   | 1225/2000 [5:45:52<3:37:16, 16.82s/it]
                                                       

 61%|██████▏   | 1225/2000 [5:45:52<3:37:16, 16.82s/it]
 61%|██████▏   | 1226/2000 [5:46:08<3:36:57, 16.82s/it]
                                                       

 61%|██████▏   | 1226/2000 [5:46:08<3:36:57, 16.82s/it]
 61%|██████▏   | 1227/2000 [5:46:25<3:36:42, 16.82s/it]
                                                       

 61%|██████▏   | 1227/2000 [5:46:25<3:36:42, 16.82s/it]
 61%|██████▏   | 1228/2000 [5:46:42<3:36:27, 16.82s/it]
                                                       

 61%|██████▏   | 1228/2000 [5:46:42<3:36:27, 16.82s/it]
 61%|██████▏   | 1229/2000 [5:46:59<3:36:12, 16.83s/it]
                                                       

 61%|██████▏   | 1229/2000 [5:46:59<3:36:12, 16.83s/it]
 62%|██████▏   | 1230/2000 [5:47:16<3:35:58, 16.83s/it]
                                                       

 62%|██████▏   | 1230/2000 [5:47:16<3:35:58, 16.83s/it]
 62%|██████▏   | 1231/2000 [5:47:32<3:35:36, 16.82s/it]
                                                       

 62%|██████▏   | 1231/2000 [5:47:32<3:35:36, 16.82s/it]
 62%|██████▏   | 1232/2000 [5:47:49<3:35:15, 16.82s/it]
                                                       

 62%|██████▏   | 1232/2000 [5:47:49<3:35:15, 16.82s/it]
 62%|██████▏   | 1233/2000 [5:48:06<3:35:05, 16.83s/it]
                                                       

 62%|██████▏   | 1233/2000 [5:48:06<3:35:05, 16.83s/it]
 62%|██████▏   | 1234/2000 [5:48:23<3:34:42, 16.82s/it]
                                                       

 62%|██████▏   | 1234/2000 [5:48:23<3:34:42, 16.82s/it]
 62%|██████▏   | 1235/2000 [5:48:40<3:34:23, 16.81s/it]
                                                       

 62%|██████▏   | 1235/2000 [5:48:40<3:34:23, 16.81s/it]
 62%|██████▏   | 1236/2000 [5:48:57<3:34:04, 16.81s/it]
                                                       

 62%|██████▏   | 1236/2000 [5:48:57<3:34:04, 16.81s/it]
 62%|██████▏   | 1237/2000 [5:49:13<3:33:46, 16.81s/it]
                                                       

 62%|██████▏   | 1237/2000 [5:49:13<3:33:46, 16.81s/it]
 62%|██████▏   | 1238/2000 [5:49:30<3:33:42, 16.83s/it]
                                                       

 62%|██████▏   | 1238/2000 [5:49:30<3:33:42, 16.83s/it]
 62%|██████▏   | 1239/2000 [5:49:47<3:33:19, 16.82s/it]
                                                       

 62%|██████▏   | 1239/2000 [5:49:47<3:33:19, 16.82s/it]
 62%|██████▏   | 1240/2000 [5:50:04<3:32:56, 16.81s/it]
                                                       

 62%|██████▏   | 1240/2000 [5:50:04<3:32:56, 16.81s/it]
 62%|██████▏   | 1241/2000 [5:50:21<3:32:30, 16.80s/it]
                                                       

 62%|██████▏   | 1241/2000 [5:50:21<3:32:30, 16.80s/it]
 62%|██████▏   | 1242/2000 [5:50:37<3:32:16, 16.80s/it]
                                                       

 62%|██████▏   | 1242/2000 [5:50:37<3:32:16, 16.80s/it]
 62%|██████▏   | 1243/2000 [5:50:54<3:32:03, 16.81s/it]
                                                       

 62%|██████▏   | 1243/2000 [5:50:54<3:32:03, 16.81s/it]
 62%|██████▏   | 1244/2000 [5:51:11<3:31:49, 16.81s/it]
                                                       

 62%|██████▏   | 1244/2000 [5:51:11<3:31:49, 16.81s/it]
 62%|██████▏   | 1245/2000 [5:51:28<3:31:36, 16.82s/it]
                                                       

 62%|██████▏   | 1245/2000 [5:51:28<3:31:36, 16.82s/it]
 62%|██████▏   | 1246/2000 [5:51:45<3:31:21, 16.82s/it]
                                                       

 62%|██████▏   | 1246/2000 [5:51:45<3:31:21, 16.82s/it]
 62%|██████▏   | 1247/2000 [5:52:01<3:31:03, 16.82s/it]
                                                       

 62%|██████▏   | 1247/2000 [5:52:01<3:31:03, 16.82s/it]
 62%|██████▏   | 1248/2000 [5:52:18<3:30:35, 16.80s/it]
                                                       

 62%|██████▏   | 1248/2000 [5:52:18<3:30:35, 16.80s/it]
 62%|██████▏   | 1249/2000 [5:52:35<3:30:19, 16.80s/it]
                                                       

 62%|██████▏   | 1249/2000 [5:52:35<3:30:19, 16.80s/it]
 62%|██████▎   | 1250/2000 [5:52:52<3:30:11, 16.82s/it]
                                                       

 62%|██████▎   | 1250/2000 [5:52:52<3:30:11, 16.82s/it]
 63%|██████▎   | 1251/2000 [5:53:09<3:29:47, 16.81s/it]
                                                       

 63%|██████▎   | 1251/2000 [5:53:09<3:29:47, 16.81s/it]
 63%|██████▎   | 1252/2000 [5:53:25<3:29:29, 16.80s/it]
                                                       

 63%|██████▎   | 1252/2000 [5:53:25<3:29:29, 16.80s/it]
 63%|██████▎   | 1253/2000 [5:53:42<3:29:18, 16.81s/it]
                                                       

 63%|██████▎   | 1253/2000 [5:53:42<3:29:18, 16.81s/it]
 63%|██████▎   | 1254/2000 [5:53:59<3:29:00, 16.81s/it]
                                                       

 63%|██████▎   | 1254/2000 [5:53:59<3:29:00, 16.81s/it]
 63%|██████▎   | 1255/2000 [5:54:16<3:28:49, 16.82s/it]
                                                       

 63%|██████▎   | 1255/2000 [5:54:16<3:28:49, 16.82s/it]
 63%|██████▎   | 1256/2000 [5:54:33<3:28:31, 16.82s/it]
                                                       

 63%|██████▎   | 1256/2000 [5:54:33<3:28:31, 16.82s/it]
 63%|██████▎   | 1257/2000 [5:54:50<3:28:09, 16.81s/it]
                                                       

 63%|██████▎   | 1257/2000 [5:54:50<3:28:09, 16.81s/it]
 63%|██████▎   | 1258/2000 [5:55:06<3:27:51, 16.81s/it]
                                                       

 63%|██████▎   | 1258/2000 [5:55:06<3:27:51, 16.81s/it]
 63%|██████▎   | 1259/2000 [5:55:23<3:27:42, 16.82s/it]
                                                       

 63%|██████▎   | 1259/2000 [5:55:23<3:27:42, 16.82s/it]
 63%|██████▎   | 1260/2000 [5:55:40<3:27:27, 16.82s/it]
                                                       

 63%|██████▎   | 1260/2000 [5:55:40<3:27:27, 16.82s/it]
 63%|██████▎   | 1261/2000 [5:55:57<3:27:03, 16.81s/it]
                                                       

 63%|██████▎   | 1261/2000 [5:55:57<3:27:03, 16.81s/it]
 63%|██████▎   | 1262/2000 [5:56:14<3:26:54, 16.82s/it]
                                                       

 63%|██████▎   | 1262/2000 [5:56:14<3:26:54, 16.82s/it]
 63%|██████▎   | 1263/2000 [5:56:31<3:26:41, 16.83s/it]
                                                       

 63%|██████▎   | 1263/2000 [5:56:31<3:26:41, 16.83s/it]
 63%|██████▎   | 1264/2000 [5:56:47<3:26:12, 16.81s/it]
                                                       

 63%|██████▎   | 1264/2000 [5:56:47<3:26:12, 16.81s/it]
 63%|██████▎   | 1265/2000 [5:57:04<3:25:59, 16.82s/it]
                                                       

 63%|██████▎   | 1265/2000 [5:57:04<3:25:59, 16.82s/it]
 63%|██████▎   | 1266/2000 [5:57:21<3:25:38, 16.81s/it]
                                                       

 63%|██████▎   | 1266/2000 [5:57:21<3:25:38, 16.81s/it]
 63%|██████▎   | 1267/2000 [5:57:38<3:25:24, 16.81s/it]
                                                       

 63%|██████▎   | 1267/2000 [5:57:38<3:25:24, 16.81s/it]
 63%|██████▎   | 1268/2000 [5:57:55<3:25:09, 16.82s/it]
                                                       

 63%|██████▎   | 1268/2000 [5:57:55<3:25:09, 16.82s/it]
 63%|██████▎   | 1269/2000 [5:58:11<3:24:44, 16.80s/it]
                                                       

 63%|██████▎   | 1269/2000 [5:58:11<3:24:44, 16.80s/it]
 64%|██████▎   | 1270/2000 [5:58:28<3:24:19, 16.79s/it]
                                                       

 64%|██████▎   | 1270/2000 [5:58:28<3:24:19, 16.79s/it]
 64%|██████▎   | 1271/2000 [5:58:45<3:24:11, 16.81s/it]
                                                       

 64%|██████▎   | 1271/2000 [5:58:45<3:24:11, 16.81s/it]
 64%|██████▎   | 1272/2000 [5:59:02<3:23:55, 16.81s/it]
                                                       

 64%|██████▎   | 1272/2000 [5:59:02<3:23:55, 16.81s/it]
 64%|██████▎   | 1273/2000 [5:59:19<3:23:39, 16.81s/it]
                                                       

 64%|██████▎   | 1273/2000 [5:59:19<3:23:39, 16.81s/it]
 64%|██████▎   | 1274/2000 [5:59:35<3:23:20, 16.81s/it]
                                                       

 64%|██████▎   | 1274/2000 [5:59:35<3:23:20, 16.81s/it]
 64%|██████▍   | 1275/2000 [5:59:52<3:22:57, 16.80s/it]
                                                       

 64%|██████▍   | 1275/2000 [5:59:52<3:22:57, 16.80s/it]
 64%|██████▍   | 1276/2000 [6:00:09<3:22:41, 16.80s/it]
                                                       

 64%|██████▍   | 1276/2000 [6:00:09<3:22:41, 16.80s/it]
 64%|██████▍   | 1277/2000 [6:00:26<3:22:31, 16.81s/it]
                                                       

 64%|██████▍   | 1277/2000 [6:00:26<3:22:31, 16.81s/it]
 64%|██████▍   | 1278/2000 [6:00:42<3:22:01, 16.79s/it]
                                                       

 64%|██████▍   | 1278/2000 [6:00:42<3:22:01, 16.79s/it]
 64%|██████▍   | 1279/2000 [6:00:59<3:21:46, 16.79s/it]
                                                       

 64%|██████▍   | 1279/2000 [6:00:59<3:21:46, 16.79s/it]
 64%|██████▍   | 1280/2000 [6:01:16<3:22:23, 16.87s/it]
                                                       

 64%|██████▍   | 1280/2000 [6:01:16<3:22:23, 16.87s/it]
 64%|██████▍   | 1281/2000 [6:01:36<3:31:34, 17.66s/it]
                                                       

 64%|██████▍   | 1281/2000 [6:01:36<3:31:34, 17.66s/it]
 64%|██████▍   | 1282/2000 [6:01:54<3:32:05, 17.72s/it]
                                                       

 64%|██████▍   | 1282/2000 [6:01:54<3:32:05, 17.72s/it]
 64%|██████▍   | 1283/2000 [6:02:10<3:28:10, 17.42s/it]
                                                       

 64%|██████▍   | 1283/2000 [6:02:10<3:28:10, 17.42s/it]
 64%|██████▍   | 1284/2000 [6:02:28<3:28:27, 17.47s/it]
                                                       

 64%|██████▍   | 1284/2000 [6:02:28<3:28:27, 17.47s/it]
 64%|██████▍   | 1285/2000 [6:02:45<3:25:42, 17.26s/it]
                                                       

 64%|██████▍   | 1285/2000 [6:02:45<3:25:42, 17.26s/it]
 64%|██████▍   | 1286/2000 [6:03:02<3:24:25, 17.18s/it]
                                                       

 64%|██████▍   | 1286/2000 [6:03:02<3:24:25, 17.18s/it]
 64%|██████▍   | 1287/2000 [6:03:19<3:22:50, 17.07s/it]
                                                       

 64%|██████▍   | 1287/2000 [6:03:19<3:22:50, 17.07s/it]
 64%|██████▍   | 1288/2000 [6:03:35<3:21:36, 16.99s/it]
                                                       

 64%|██████▍   | 1288/2000 [6:03:35<3:21:36, 16.99s/it]
 64%|██████▍   | 1289/2000 [6:03:52<3:20:37, 16.93s/it]
                                                       

 64%|██████▍   | 1289/2000 [6:03:52<3:20:37, 16.93s/it]
 64%|██████▍   | 1290/2000 [6:04:09<3:19:58, 16.90s/it]
                                                       

 64%|██████▍   | 1290/2000 [6:04:09<3:19:58, 16.90s/it]
 65%|██████▍   | 1291/2000 [6:04:26<3:19:28, 16.88s/it]
                                                       

 65%|██████▍   | 1291/2000 [6:04:26<3:19:28, 16.88s/it]
 65%|██████▍   | 1292/2000 [6:04:43<3:18:54, 16.86s/it]
                                                       

 65%|██████▍   | 1292/2000 [6:04:43<3:18:54, 16.86s/it]
 65%|██████▍   | 1293/2000 [6:04:59<3:18:17, 16.83s/it]
                                                       

 65%|██████▍   | 1293/2000 [6:04:59<3:18:17, 16.83s/it]
 65%|██████▍   | 1294/2000 [6:05:16<3:18:00, 16.83s/it]
                                                       

 65%|██████▍   | 1294/2000 [6:05:16<3:18:00, 16.83s/it]
 65%|██████▍   | 1295/2000 [6:05:33<3:17:44, 16.83s/it]
                                                       

 65%|██████▍   | 1295/2000 [6:05:33<3:17:44, 16.83s/it]
 65%|██████▍   | 1296/2000 [6:05:50<3:17:12, 16.81s/it]
                                                       

 65%|██████▍   | 1296/2000 [6:05:50<3:17:12, 16.81s/it]
 65%|██████▍   | 1297/2000 [6:06:07<3:16:57, 16.81s/it]
                                                       

 65%|██████▍   | 1297/2000 [6:06:07<3:16:57, 16.81s/it]
 65%|██████▍   | 1298/2000 [6:06:23<3:16:36, 16.80s/it]
                                                       

 65%|██████▍   | 1298/2000 [6:06:23<3:16:36, 16.80s/it]
 65%|██████▍   | 1299/2000 [6:06:40<3:16:26, 16.81s/it]
                                                       

 65%|██████▍   | 1299/2000 [6:06:40<3:16:26, 16.81s/it]
 65%|██████▌   | 1300/2000 [6:06:57<3:16:08, 16.81s/it]
                                                       

 65%|██████▌   | 1300/2000 [6:06:57<3:16:08, 16.81s/it]
 65%|██████▌   | 1301/2000 [6:07:14<3:15:41, 16.80s/it]
                                                       

 65%|██████▌   | 1301/2000 [6:07:14<3:15:41, 16.80s/it]
 65%|██████▌   | 1302/2000 [6:07:31<3:15:22, 16.79s/it]
                                                       

 65%|██████▌   | 1302/2000 [6:07:31<3:15:22, 16.79s/it]
 65%|██████▌   | 1303/2000 [6:07:47<3:15:15, 16.81s/it]
                                                       

 65%|██████▌   | 1303/2000 [6:07:47<3:15:15, 16.81s/it]
 65%|██████▌   | 1304/2000 [6:08:04<3:14:54, 16.80s/it]
                                                       

 65%|██████▌   | 1304/2000 [6:08:04<3:14:54, 16.80s/it]
 65%|██████▌   | 1305/2000 [6:08:21<3:14:39, 16.81s/it]
                                                       

 65%|██████▌   | 1305/2000 [6:08:21<3:14:39, 16.81s/it]
 65%|██████▌   | 1306/2000 [6:08:38<3:14:18, 16.80s/it]
                                                       

 65%|██████▌   | 1306/2000 [6:08:38<3:14:18, 16.80s/it]
 65%|██████▌   | 1307/2000 [6:08:55<3:13:59, 16.80s/it]
                                                       

 65%|██████▌   | 1307/2000 [6:08:55<3:13:59, 16.80s/it]
 65%|██████▌   | 1308/2000 [6:09:11<3:13:42, 16.80s/it]
                                                       

 65%|██████▌   | 1308/2000 [6:09:11<3:13:42, 16.80s/it]
 65%|██████▌   | 1309/2000 [6:09:28<3:13:29, 16.80s/it]
                                                       

 65%|██████▌   | 1309/2000 [6:09:28<3:13:29, 16.80s/it]
 66%|██████▌   | 1310/2000 [6:09:45<3:13:06, 16.79s/it]
                                                       

 66%|██████▌   | 1310/2000 [6:09:45<3:13:06, 16.79s/it]
 66%|██████▌   | 1311/2000 [6:10:02<3:12:50, 16.79s/it]
                                                       

 66%|██████▌   | 1311/2000 [6:10:02<3:12:50, 16.79s/it]
 66%|██████▌   | 1312/2000 [6:10:19<3:12:48, 16.82s/it]
                                                       

 66%|██████▌   | 1312/2000 [6:10:19<3:12:48, 16.82s/it]
 66%|██████▌   | 1313/2000 [6:10:35<3:12:24, 16.80s/it]
                                                       

 66%|██████▌   | 1313/2000 [6:10:35<3:12:24, 16.80s/it]
 66%|██████▌   | 1314/2000 [6:10:52<3:12:03, 16.80s/it]
                                                       

 66%|██████▌   | 1314/2000 [6:10:52<3:12:03, 16.80s/it]
 66%|██████▌   | 1315/2000 [6:11:09<3:12:00, 16.82s/it]
                                                       

 66%|██████▌   | 1315/2000 [6:11:09<3:12:00, 16.82s/it]
 66%|██████▌   | 1316/2000 [6:11:26<3:11:46, 16.82s/it]
                                                       

 66%|██████▌   | 1316/2000 [6:11:26<3:11:46, 16.82s/it]
 66%|██████▌   | 1317/2000 [6:11:43<3:11:26, 16.82s/it]
                                                       

 66%|██████▌   | 1317/2000 [6:11:43<3:11:26, 16.82s/it]
 66%|██████▌   | 1318/2000 [6:11:59<3:10:55, 16.80s/it]
                                                       

 66%|██████▌   | 1318/2000 [6:12:00<3:10:55, 16.80s/it]
 66%|██████▌   | 1319/2000 [6:12:16<3:10:36, 16.79s/it]
                                                       

 66%|██████▌   | 1319/2000 [6:12:16<3:10:36, 16.79s/it]
 66%|██████▌   | 1320/2000 [6:12:33<3:10:17, 16.79s/it]
                                                       

 66%|██████▌   | 1320/2000 [6:12:33<3:10:17, 16.79s/it]
 66%|██████▌   | 1321/2000 [6:12:50<3:09:59, 16.79s/it]
                                                       

 66%|██████▌   | 1321/2000 [6:12:50<3:09:59, 16.79s/it]
 66%|██████▌   | 1322/2000 [6:13:07<3:09:41, 16.79s/it]
                                                       

 66%|██████▌   | 1322/2000 [6:13:07<3:09:41, 16.79s/it]
 66%|██████▌   | 1323/2000 [6:13:24<3:09:40, 16.81s/it]
                                                       

 66%|██████▌   | 1323/2000 [6:13:24<3:09:40, 16.81s/it]
 66%|██████▌   | 1324/2000 [6:13:40<3:09:25, 16.81s/it]
                                                       

 66%|██████▌   | 1324/2000 [6:13:40<3:09:25, 16.81s/it]
 66%|██████▋   | 1325/2000 [6:13:57<3:09:14, 16.82s/it]
                                                       

 66%|██████▋   | 1325/2000 [6:13:57<3:09:14, 16.82s/it]
 66%|██████▋   | 1326/2000 [6:14:14<3:08:49, 16.81s/it]
                                                       

 66%|██████▋   | 1326/2000 [6:14:14<3:08:49, 16.81s/it]
 66%|██████▋   | 1327/2000 [6:14:31<3:08:31, 16.81s/it]
                                                       

 66%|██████▋   | 1327/2000 [6:14:31<3:08:31, 16.81s/it]
 66%|██████▋   | 1328/2000 [6:14:48<3:08:14, 16.81s/it]
                                                       

 66%|██████▋   | 1328/2000 [6:14:48<3:08:14, 16.81s/it]
 66%|██████▋   | 1329/2000 [6:15:04<3:07:59, 16.81s/it]
                                                       

 66%|██████▋   | 1329/2000 [6:15:04<3:07:59, 16.81s/it]
 66%|██████▋   | 1330/2000 [6:15:21<3:07:31, 16.79s/it]
                                                       

 66%|██████▋   | 1330/2000 [6:15:21<3:07:31, 16.79s/it]
 67%|██████▋   | 1331/2000 [6:15:38<3:07:16, 16.80s/it]
                                                       

 67%|██████▋   | 1331/2000 [6:15:38<3:07:16, 16.80s/it]
 67%|██████▋   | 1332/2000 [6:15:55<3:07:07, 16.81s/it]
                                                       

 67%|██████▋   | 1332/2000 [6:15:55<3:07:07, 16.81s/it]
 67%|██████▋   | 1333/2000 [6:16:12<3:06:48, 16.80s/it]
                                                       

 67%|██████▋   | 1333/2000 [6:16:12<3:06:48, 16.80s/it]
 67%|██████▋   | 1334/2000 [6:16:28<3:06:34, 16.81s/it]
                                                       

 67%|██████▋   | 1334/2000 [6:16:28<3:06:34, 16.81s/it]
 67%|██████▋   | 1335/2000 [6:16:45<3:06:08, 16.79s/it]
                                                       

 67%|██████▋   | 1335/2000 [6:16:45<3:06:08, 16.79s/it]
 67%|██████▋   | 1336/2000 [6:17:02<3:05:58, 16.81s/it]
                                                       

 67%|██████▋   | 1336/2000 [6:17:02<3:05:58, 16.81s/it]
 67%|██████▋   | 1337/2000 [6:17:19<3:05:48, 16.81s/it]
                                                       

 67%|██████▋   | 1337/2000 [6:17:19<3:05:48, 16.81s/it]
 67%|██████▋   | 1338/2000 [6:17:36<3:05:24, 16.80s/it]
                                                       

 67%|██████▋   | 1338/2000 [6:17:36<3:05:24, 16.80s/it]
 67%|██████▋   | 1339/2000 [6:17:52<3:05:06, 16.80s/it]
                                                       

 67%|██████▋   | 1339/2000 [6:17:52<3:05:06, 16.80s/it]
 67%|██████▋   | 1340/2000 [6:18:09<3:04:43, 16.79s/it]
                                                       

 67%|██████▋   | 1340/2000 [6:18:09<3:04:43, 16.79s/it]
 67%|██████▋   | 1341/2000 [6:18:26<3:04:23, 16.79s/it]
                                                       

 67%|██████▋   | 1341/2000 [6:18:26<3:04:23, 16.79s/it]
 67%|██████▋   | 1342/2000 [6:18:43<3:04:04, 16.79s/it]
                                                       

 67%|██████▋   | 1342/2000 [6:18:43<3:04:04, 16.79s/it]
 67%|██████▋   | 1343/2000 [6:18:59<3:03:37, 16.77s/it]
                                                       

 67%|██████▋   | 1343/2000 [6:18:59<3:03:37, 16.77s/it]
 67%|██████▋   | 1344/2000 [6:19:17<3:06:04, 17.02s/it]
                                                       

 67%|██████▋   | 1344/2000 [6:19:17<3:06:04, 17.02s/it]
 67%|██████▋   | 1345/2000 [6:19:34<3:04:50, 16.93s/it]
                                                       

 67%|██████▋   | 1345/2000 [6:19:34<3:04:50, 16.93s/it]
 67%|██████▋   | 1346/2000 [6:19:51<3:05:00, 16.97s/it]
                                                       

 67%|██████▋   | 1346/2000 [6:19:51<3:05:00, 16.97s/it]
 67%|██████▋   | 1347/2000 [6:20:08<3:04:22, 16.94s/it]
                                                       

 67%|██████▋   | 1347/2000 [6:20:08<3:04:22, 16.94s/it]
 67%|██████▋   | 1348/2000 [6:20:25<3:03:50, 16.92s/it]
                                                       

 67%|██████▋   | 1348/2000 [6:20:25<3:03:50, 16.92s/it]
 67%|██████▋   | 1349/2000 [6:20:42<3:04:08, 16.97s/it]
                                                       

 67%|██████▋   | 1349/2000 [6:20:42<3:04:08, 16.97s/it]
 68%|██████▊   | 1350/2000 [6:20:58<3:03:16, 16.92s/it]
                                                       

 68%|██████▊   | 1350/2000 [6:20:58<3:03:16, 16.92s/it]
 68%|██████▊   | 1351/2000 [6:21:15<3:02:50, 16.90s/it]
                                                       

 68%|██████▊   | 1351/2000 [6:21:15<3:02:50, 16.90s/it]
 68%|██████▊   | 1352/2000 [6:21:32<3:02:04, 16.86s/it]
                                                       

 68%|██████▊   | 1352/2000 [6:21:32<3:02:04, 16.86s/it]
 68%|██████▊   | 1353/2000 [6:21:49<3:01:42, 16.85s/it]
                                                       

 68%|██████▊   | 1353/2000 [6:21:49<3:01:42, 16.85s/it]
 68%|██████▊   | 1354/2000 [6:22:06<3:01:12, 16.83s/it]
                                                       

 68%|██████▊   | 1354/2000 [6:22:06<3:01:12, 16.83s/it]
 68%|██████▊   | 1355/2000 [6:22:23<3:00:54, 16.83s/it]
                                                       

 68%|██████▊   | 1355/2000 [6:22:23<3:00:54, 16.83s/it]
 68%|██████▊   | 1356/2000 [6:22:39<3:00:26, 16.81s/it]
                                                       

 68%|██████▊   | 1356/2000 [6:22:39<3:00:26, 16.81s/it]
 68%|██████▊   | 1357/2000 [6:22:56<3:00:09, 16.81s/it]
                                                       

 68%|██████▊   | 1357/2000 [6:22:56<3:00:09, 16.81s/it]
 68%|██████▊   | 1358/2000 [6:23:13<2:59:46, 16.80s/it]
                                                       

 68%|██████▊   | 1358/2000 [6:23:13<2:59:46, 16.80s/it]
 68%|██████▊   | 1359/2000 [6:23:30<2:59:36, 16.81s/it]
                                                       

 68%|██████▊   | 1359/2000 [6:23:30<2:59:36, 16.81s/it]
 68%|██████▊   | 1360/2000 [6:23:47<2:59:34, 16.83s/it]
                                                       

 68%|██████▊   | 1360/2000 [6:23:47<2:59:34, 16.83s/it]
 68%|██████▊   | 1361/2000 [6:24:06<3:07:34, 17.61s/it]
                                                       

 68%|██████▊   | 1361/2000 [6:24:06<3:07:34, 17.61s/it]
 68%|██████▊   | 1362/2000 [6:24:24<3:08:04, 17.69s/it]
                                                       

 68%|██████▊   | 1362/2000 [6:24:24<3:08:04, 17.69s/it]
 68%|██████▊   | 1363/2000 [6:24:41<3:04:53, 17.42s/it]
                                                       

 68%|██████▊   | 1363/2000 [6:24:41<3:04:53, 17.42s/it]
 68%|██████▊   | 1364/2000 [6:24:58<3:03:39, 17.33s/it]
                                                       

 68%|██████▊   | 1364/2000 [6:24:58<3:03:39, 17.33s/it]
 68%|██████▊   | 1365/2000 [6:25:15<3:01:37, 17.16s/it]
                                                       

 68%|██████▊   | 1365/2000 [6:25:15<3:01:37, 17.16s/it]
 68%|██████▊   | 1366/2000 [6:25:31<3:00:19, 17.07s/it]
                                                       

 68%|██████▊   | 1366/2000 [6:25:31<3:00:19, 17.07s/it]
 68%|██████▊   | 1367/2000 [6:25:48<2:59:14, 16.99s/it]
                                                       

 68%|██████▊   | 1367/2000 [6:25:48<2:59:14, 16.99s/it]
 68%|██████▊   | 1368/2000 [6:26:05<2:58:21, 16.93s/it]
                                                       

 68%|██████▊   | 1368/2000 [6:26:05<2:58:21, 16.93s/it]
 68%|██████▊   | 1369/2000 [6:26:22<2:57:48, 16.91s/it]
                                                       

 68%|██████▊   | 1369/2000 [6:26:22<2:57:48, 16.91s/it]
 68%|██████▊   | 1370/2000 [6:26:39<2:57:10, 16.87s/it]
                                                       

 68%|██████▊   | 1370/2000 [6:26:39<2:57:10, 16.87s/it]
 69%|██████▊   | 1371/2000 [6:26:56<2:56:51, 16.87s/it]
                                                       

 69%|██████▊   | 1371/2000 [6:26:56<2:56:51, 16.87s/it]
 69%|██████▊   | 1372/2000 [6:27:12<2:56:29, 16.86s/it]
                                                       

 69%|██████▊   | 1372/2000 [6:27:12<2:56:29, 16.86s/it]
 69%|██████▊   | 1373/2000 [6:27:29<2:56:05, 16.85s/it]
                                                       

 69%|██████▊   | 1373/2000 [6:27:29<2:56:05, 16.85s/it]
 69%|██████▊   | 1374/2000 [6:27:46<2:55:42, 16.84s/it]
                                                       

 69%|██████▊   | 1374/2000 [6:27:46<2:55:42, 16.84s/it]
 69%|██████▉   | 1375/2000 [6:28:03<2:55:16, 16.83s/it]
                                                       

 69%|██████▉   | 1375/2000 [6:28:03<2:55:16, 16.83s/it]
 69%|██████▉   | 1376/2000 [6:28:20<2:54:55, 16.82s/it]
                                                       

 69%|██████▉   | 1376/2000 [6:28:20<2:54:55, 16.82s/it]
 69%|██████▉   | 1377/2000 [6:28:36<2:54:41, 16.82s/it]
                                                       

 69%|██████▉   | 1377/2000 [6:28:36<2:54:41, 16.82s/it]
 69%|██████▉   | 1378/2000 [6:28:53<2:54:22, 16.82s/it]
                                                       

 69%|██████▉   | 1378/2000 [6:28:53<2:54:22, 16.82s/it]
 69%|██████▉   | 1379/2000 [6:29:10<2:54:01, 16.81s/it]
                                                       

 69%|██████▉   | 1379/2000 [6:29:10<2:54:01, 16.81s/it]
 69%|██████▉   | 1380/2000 [6:29:27<2:53:42, 16.81s/it]
                                                       

 69%|██████▉   | 1380/2000 [6:29:27<2:53:42, 16.81s/it]
 69%|██████▉   | 1381/2000 [6:29:44<2:53:24, 16.81s/it]
                                                       

 69%|██████▉   | 1381/2000 [6:29:44<2:53:24, 16.81s/it]
 69%|██████▉   | 1382/2000 [6:30:01<2:53:18, 16.83s/it]
                                                       

 69%|██████▉   | 1382/2000 [6:30:01<2:53:18, 16.83s/it]
 69%|██████▉   | 1383/2000 [6:30:17<2:53:02, 16.83s/it]
                                                       

 69%|██████▉   | 1383/2000 [6:30:17<2:53:02, 16.83s/it]
 69%|██████▉   | 1384/2000 [6:30:34<2:52:35, 16.81s/it]
                                                       

 69%|██████▉   | 1384/2000 [6:30:34<2:52:35, 16.81s/it]
 69%|██████▉   | 1385/2000 [6:30:51<2:52:20, 16.81s/it]
                                                       

 69%|██████▉   | 1385/2000 [6:30:51<2:52:20, 16.81s/it]
 69%|██████▉   | 1386/2000 [6:31:08<2:52:10, 16.82s/it]
                                                       

 69%|██████▉   | 1386/2000 [6:31:08<2:52:10, 16.82s/it]
 69%|██████▉   | 1387/2000 [6:31:25<2:51:47, 16.82s/it]
                                                       

 69%|██████▉   | 1387/2000 [6:31:25<2:51:47, 16.82s/it]
 69%|██████▉   | 1388/2000 [6:31:41<2:51:30, 16.81s/it]
                                                       

 69%|██████▉   | 1388/2000 [6:31:41<2:51:30, 16.81s/it]
 69%|██████▉   | 1389/2000 [6:31:58<2:51:07, 16.81s/it]
                                                       

 69%|██████▉   | 1389/2000 [6:31:58<2:51:07, 16.81s/it]
 70%|██████▉   | 1390/2000 [6:32:15<2:50:42, 16.79s/it]
                                                       

 70%|██████▉   | 1390/2000 [6:32:15<2:50:42, 16.79s/it]
 70%|██████▉   | 1391/2000 [6:32:32<2:50:25, 16.79s/it]
                                                       

 70%|██████▉   | 1391/2000 [6:32:32<2:50:25, 16.79s/it]
 70%|██████▉   | 1392/2000 [6:32:49<2:50:10, 16.79s/it]
                                                       

 70%|██████▉   | 1392/2000 [6:32:49<2:50:10, 16.79s/it]
 70%|██████▉   | 1393/2000 [6:33:05<2:49:50, 16.79s/it]
                                                       

 70%|██████▉   | 1393/2000 [6:33:05<2:49:50, 16.79s/it]
 70%|██████▉   | 1394/2000 [6:33:22<2:49:36, 16.79s/it]
                                                       

 70%|██████▉   | 1394/2000 [6:33:22<2:49:36, 16.79s/it]
 70%|██████▉   | 1395/2000 [6:33:39<2:49:17, 16.79s/it]
                                                       

 70%|██████▉   | 1395/2000 [6:33:39<2:49:17, 16.79s/it]
 70%|██████▉   | 1396/2000 [6:33:56<2:49:08, 16.80s/it]
                                                       

 70%|██████▉   | 1396/2000 [6:33:56<2:49:08, 16.80s/it]
 70%|██████▉   | 1397/2000 [6:34:13<2:48:52, 16.80s/it]
                                                       

 70%|██████▉   | 1397/2000 [6:34:13<2:48:52, 16.80s/it]
 70%|██████▉   | 1398/2000 [6:34:29<2:48:30, 16.79s/it]
                                                       

 70%|██████▉   | 1398/2000 [6:34:29<2:48:30, 16.79s/it]
 70%|██████▉   | 1399/2000 [6:34:46<2:48:17, 16.80s/it]
                                                       

 70%|██████▉   | 1399/2000 [6:34:46<2:48:17, 16.80s/it]
 70%|███████   | 1400/2000 [6:35:03<2:47:50, 16.78s/it]
                                                       

 70%|███████   | 1400/2000 [6:35:03<2:47:50, 16.78s/it]
 70%|███████   | 1401/2000 [6:35:20<2:47:39, 16.79s/it]
                                                       

 70%|███████   | 1401/2000 [6:35:20<2:47:39, 16.79s/it]
 70%|███████   | 1402/2000 [6:35:36<2:47:21, 16.79s/it]
                                                       

 70%|███████   | 1402/2000 [6:35:36<2:47:21, 16.79s/it]
 70%|███████   | 1403/2000 [6:35:53<2:47:03, 16.79s/it]
                                                       

 70%|███████   | 1403/2000 [6:35:53<2:47:03, 16.79s/it]
 70%|███████   | 1404/2000 [6:36:10<2:46:45, 16.79s/it]
                                                       

 70%|███████   | 1404/2000 [6:36:10<2:46:45, 16.79s/it]
 70%|███████   | 1405/2000 [6:36:27<2:46:24, 16.78s/it]
                                                       

 70%|███████   | 1405/2000 [6:36:27<2:46:24, 16.78s/it]
 70%|███████   | 1406/2000 [6:36:44<2:46:20, 16.80s/it]
                                                       

 70%|███████   | 1406/2000 [6:36:44<2:46:20, 16.80s/it]
 70%|███████   | 1407/2000 [6:37:00<2:46:07, 16.81s/it]
                                                       

 70%|███████   | 1407/2000 [6:37:00<2:46:07, 16.81s/it]
 70%|███████   | 1408/2000 [6:37:17<2:45:52, 16.81s/it]
                                                       

 70%|███████   | 1408/2000 [6:37:17<2:45:52, 16.81s/it]
 70%|███████   | 1409/2000 [6:37:34<2:45:34, 16.81s/it]
                                                       

 70%|███████   | 1409/2000 [6:37:34<2:45:34, 16.81s/it]
 70%|███████   | 1410/2000 [6:37:51<2:45:13, 16.80s/it]
                                                       

 70%|███████   | 1410/2000 [6:37:51<2:45:13, 16.80s/it]
 71%|███████   | 1411/2000 [6:38:08<2:44:56, 16.80s/it]
                                                       

 71%|███████   | 1411/2000 [6:38:08<2:44:56, 16.80s/it]
 71%|███████   | 1412/2000 [6:38:25<2:44:47, 16.82s/it]
                                                       

 71%|███████   | 1412/2000 [6:38:25<2:44:47, 16.82s/it]
 71%|███████   | 1413/2000 [6:38:41<2:44:26, 16.81s/it]
                                                       

 71%|███████   | 1413/2000 [6:38:41<2:44:26, 16.81s/it]
 71%|███████   | 1414/2000 [6:38:58<2:44:13, 16.82s/it]
                                                       

 71%|███████   | 1414/2000 [6:38:58<2:44:13, 16.82s/it]
 71%|███████   | 1415/2000 [6:39:15<2:43:45, 16.80s/it]
                                                       

 71%|███████   | 1415/2000 [6:39:15<2:43:45, 16.80s/it]
 71%|███████   | 1416/2000 [6:39:32<2:43:29, 16.80s/it]
                                                       

 71%|███████   | 1416/2000 [6:39:32<2:43:29, 16.80s/it]
 71%|███████   | 1417/2000 [6:39:49<2:43:11, 16.79s/it]
                                                       

 71%|███████   | 1417/2000 [6:39:49<2:43:11, 16.79s/it]
 71%|███████   | 1418/2000 [6:40:05<2:42:50, 16.79s/it]
                                                       

 71%|███████   | 1418/2000 [6:40:05<2:42:50, 16.79s/it]
 71%|███████   | 1419/2000 [6:40:22<2:43:32, 16.89s/it]
                                                       

 71%|███████   | 1419/2000 [6:40:22<2:43:32, 16.89s/it]
 71%|███████   | 1420/2000 [6:40:39<2:43:00, 16.86s/it]
                                                       

 71%|███████   | 1420/2000 [6:40:39<2:43:00, 16.86s/it]
 71%|███████   | 1421/2000 [6:40:56<2:42:37, 16.85s/it]
                                                       

 71%|███████   | 1421/2000 [6:40:56<2:42:37, 16.85s/it]
 71%|███████   | 1422/2000 [6:41:13<2:42:09, 16.83s/it]
                                                       

 71%|███████   | 1422/2000 [6:41:13<2:42:09, 16.83s/it]
 71%|███████   | 1423/2000 [6:41:30<2:41:49, 16.83s/it]
                                                       

 71%|███████   | 1423/2000 [6:41:30<2:41:49, 16.83s/it]
 71%|███████   | 1424/2000 [6:41:46<2:41:28, 16.82s/it]
                                                       

 71%|███████   | 1424/2000 [6:41:46<2:41:28, 16.82s/it]
 71%|███████▏  | 1425/2000 [6:42:03<2:41:09, 16.82s/it]
                                                       

 71%|███████▏  | 1425/2000 [6:42:03<2:41:09, 16.82s/it]
 71%|███████▏  | 1426/2000 [6:42:20<2:40:51, 16.81s/it]
                                                       

 71%|███████▏  | 1426/2000 [6:42:20<2:40:51, 16.81s/it]
 71%|███████▏  | 1427/2000 [6:42:37<2:40:29, 16.80s/it]
                                                       

 71%|███████▏  | 1427/2000 [6:42:37<2:40:29, 16.80s/it]
 71%|███████▏  | 1428/2000 [6:42:54<2:40:11, 16.80s/it]
                                                       

 71%|███████▏  | 1428/2000 [6:42:54<2:40:11, 16.80s/it]
 71%|███████▏  | 1429/2000 [6:43:10<2:39:56, 16.81s/it]
                                                       

 71%|███████▏  | 1429/2000 [6:43:10<2:39:56, 16.81s/it]
 72%|███████▏  | 1430/2000 [6:43:27<2:39:32, 16.79s/it]
                                                       

 72%|███████▏  | 1430/2000 [6:43:27<2:39:32, 16.79s/it]
 72%|███████▏  | 1431/2000 [6:43:44<2:39:15, 16.79s/it]
                                                       

 72%|███████▏  | 1431/2000 [6:43:44<2:39:15, 16.79s/it]
 72%|███████▏  | 1432/2000 [6:44:01<2:39:03, 16.80s/it]
                                                       

 72%|███████▏  | 1432/2000 [6:44:01<2:39:03, 16.80s/it]
 72%|███████▏  | 1433/2000 [6:44:18<2:38:43, 16.80s/it]
                                                       

 72%|███████▏  | 1433/2000 [6:44:18<2:38:43, 16.80s/it]
 72%|███████▏  | 1434/2000 [6:44:34<2:38:26, 16.80s/it]
                                                       

 72%|███████▏  | 1434/2000 [6:44:34<2:38:26, 16.80s/it]
 72%|███████▏  | 1435/2000 [6:44:52<2:38:59, 16.88s/it]
                                                       

 72%|███████▏  | 1435/2000 [6:44:52<2:38:59, 16.88s/it]
 72%|███████▏  | 1436/2000 [6:45:08<2:38:32, 16.87s/it]
                                                       

 72%|███████▏  | 1436/2000 [6:45:08<2:38:32, 16.87s/it]
 72%|███████▏  | 1437/2000 [6:45:25<2:38:19, 16.87s/it]
                                                       

 72%|███████▏  | 1437/2000 [6:45:25<2:38:19, 16.87s/it]
 72%|███████▏  | 1438/2000 [6:45:42<2:38:41, 16.94s/it]
                                                       

 72%|███████▏  | 1438/2000 [6:45:42<2:38:41, 16.94s/it]
 72%|███████▏  | 1439/2000 [6:45:59<2:37:50, 16.88s/it]
                                                       

 72%|███████▏  | 1439/2000 [6:45:59<2:37:50, 16.88s/it]
 72%|███████▏  | 1440/2000 [6:46:16<2:37:47, 16.91s/it]
                                                       

 72%|███████▏  | 1440/2000 [6:46:16<2:37:47, 16.91s/it]
 72%|███████▏  | 1441/2000 [6:46:35<2:44:33, 17.66s/it]
                                                       

 72%|███████▏  | 1441/2000 [6:46:35<2:44:33, 17.66s/it]
 72%|███████▏  | 1442/2000 [6:46:53<2:44:36, 17.70s/it]
                                                       

 72%|███████▏  | 1442/2000 [6:46:53<2:44:36, 17.70s/it]
 72%|███████▏  | 1443/2000 [6:47:10<2:42:00, 17.45s/it]
                                                       

 72%|███████▏  | 1443/2000 [6:47:10<2:42:00, 17.45s/it]
 72%|███████▏  | 1444/2000 [6:47:27<2:41:00, 17.38s/it]
                                                       

 72%|███████▏  | 1444/2000 [6:47:27<2:41:00, 17.38s/it]
 72%|███████▏  | 1445/2000 [6:47:44<2:39:24, 17.23s/it]
                                                       

 72%|███████▏  | 1445/2000 [6:47:44<2:39:24, 17.23s/it]
 72%|███████▏  | 1446/2000 [6:48:01<2:38:23, 17.15s/it]
                                                       

 72%|███████▏  | 1446/2000 [6:48:01<2:38:23, 17.15s/it]
 72%|███████▏  | 1447/2000 [6:48:18<2:37:15, 17.06s/it]
                                                       

 72%|███████▏  | 1447/2000 [6:48:18<2:37:15, 17.06s/it]
 72%|███████▏  | 1448/2000 [6:48:35<2:36:16, 16.99s/it]
                                                       

 72%|███████▏  | 1448/2000 [6:48:35<2:36:16, 16.99s/it]
 72%|███████▏  | 1449/2000 [6:48:52<2:35:36, 16.95s/it]
                                                       

 72%|███████▏  | 1449/2000 [6:48:52<2:35:36, 16.95s/it]
 72%|███████▎  | 1450/2000 [6:49:09<2:35:13, 16.93s/it]
                                                       

 72%|███████▎  | 1450/2000 [6:49:09<2:35:13, 16.93s/it]
 73%|███████▎  | 1451/2000 [6:49:26<2:34:52, 16.93s/it]
                                                       

 73%|███████▎  | 1451/2000 [6:49:26<2:34:52, 16.93s/it]
 73%|███████▎  | 1452/2000 [6:49:42<2:34:30, 16.92s/it]
                                                       

 73%|███████▎  | 1452/2000 [6:49:42<2:34:30, 16.92s/it]
 73%|███████▎  | 1453/2000 [6:49:59<2:33:57, 16.89s/it]
                                                       

 73%|███████▎  | 1453/2000 [6:49:59<2:33:57, 16.89s/it]
 73%|███████▎  | 1454/2000 [6:50:16<2:33:28, 16.87s/it]
                                                       

 73%|███████▎  | 1454/2000 [6:50:16<2:33:28, 16.87s/it]
 73%|███████▎  | 1455/2000 [6:50:33<2:33:01, 16.85s/it]
                                                       

 73%|███████▎  | 1455/2000 [6:50:33<2:33:01, 16.85s/it]
 73%|███████▎  | 1456/2000 [6:50:50<2:32:54, 16.87s/it]
                                                       

 73%|███████▎  | 1456/2000 [6:50:50<2:32:54, 16.87s/it]
 73%|███████▎  | 1457/2000 [6:51:07<2:32:46, 16.88s/it]
                                                       

 73%|███████▎  | 1457/2000 [6:51:07<2:32:46, 16.88s/it]
 73%|███████▎  | 1458/2000 [6:51:23<2:32:18, 16.86s/it]
                                                       

 73%|███████▎  | 1458/2000 [6:51:23<2:32:18, 16.86s/it]
 73%|███████▎  | 1459/2000 [6:51:40<2:31:51, 16.84s/it]
                                                       

 73%|███████▎  | 1459/2000 [6:51:40<2:31:51, 16.84s/it]
 73%|███████▎  | 1460/2000 [6:51:57<2:31:28, 16.83s/it]
                                                       

 73%|███████▎  | 1460/2000 [6:51:57<2:31:28, 16.83s/it]
 73%|███████▎  | 1461/2000 [6:52:14<2:31:12, 16.83s/it]
                                                       

 73%|███████▎  | 1461/2000 [6:52:14<2:31:12, 16.83s/it]
 73%|███████▎  | 1462/2000 [6:52:31<2:31:00, 16.84s/it]
                                                       

 73%|███████▎  | 1462/2000 [6:52:31<2:31:00, 16.84s/it]
 73%|███████▎  | 1463/2000 [6:52:48<2:31:01, 16.88s/it]
                                                       

 73%|███████▎  | 1463/2000 [6:52:48<2:31:01, 16.88s/it]
 73%|███████▎  | 1464/2000 [6:53:05<2:30:34, 16.86s/it]
                                                       

 73%|███████▎  | 1464/2000 [6:53:05<2:30:34, 16.86s/it]
 73%|███████▎  | 1465/2000 [6:53:21<2:30:12, 16.85s/it]
                                                       

 73%|███████▎  | 1465/2000 [6:53:21<2:30:12, 16.85s/it]
 73%|███████▎  | 1466/2000 [6:53:38<2:29:50, 16.84s/it]
                                                       

 73%|███████▎  | 1466/2000 [6:53:38<2:29:50, 16.84s/it]
 73%|███████▎  | 1467/2000 [6:53:55<2:29:31, 16.83s/it]
                                                       

 73%|███████▎  | 1467/2000 [6:53:55<2:29:31, 16.83s/it]
 73%|███████▎  | 1468/2000 [6:54:12<2:29:27, 16.86s/it]
                                                       

 73%|███████▎  | 1468/2000 [6:54:12<2:29:27, 16.86s/it]
 73%|███████▎  | 1469/2000 [6:54:29<2:29:22, 16.88s/it]
                                                       

 73%|███████▎  | 1469/2000 [6:54:29<2:29:22, 16.88s/it]
 74%|███████▎  | 1470/2000 [6:54:46<2:28:49, 16.85s/it]
                                                       

 74%|███████▎  | 1470/2000 [6:54:46<2:28:49, 16.85s/it]
 74%|███████▎  | 1471/2000 [6:55:02<2:28:20, 16.83s/it]
                                                       

 74%|███████▎  | 1471/2000 [6:55:02<2:28:20, 16.83s/it]
 74%|███████▎  | 1472/2000 [6:55:19<2:27:58, 16.82s/it]
                                                       

 74%|███████▎  | 1472/2000 [6:55:19<2:27:58, 16.82s/it]
 74%|███████▎  | 1473/2000 [6:55:36<2:27:40, 16.81s/it]
                                                       

 74%|███████▎  | 1473/2000 [6:55:36<2:27:40, 16.81s/it]
 74%|███████▎  | 1474/2000 [6:55:53<2:27:29, 16.82s/it]
                                                       

 74%|███████▎  | 1474/2000 [6:55:53<2:27:29, 16.82s/it]
 74%|███████▍  | 1475/2000 [6:56:10<2:27:02, 16.80s/it]
                                                       

 74%|███████▍  | 1475/2000 [6:56:10<2:27:02, 16.80s/it]
 74%|███████▍  | 1476/2000 [6:56:26<2:26:46, 16.81s/it]
                                                       

 74%|███████▍  | 1476/2000 [6:56:26<2:26:46, 16.81s/it]
 74%|███████▍  | 1477/2000 [6:56:43<2:26:29, 16.81s/it]
                                                       

 74%|███████▍  | 1477/2000 [6:56:43<2:26:29, 16.81s/it]
 74%|███████▍  | 1478/2000 [6:57:00<2:26:18, 16.82s/it]
                                                       

 74%|███████▍  | 1478/2000 [6:57:00<2:26:18, 16.82s/it]
 74%|███████▍  | 1479/2000 [6:57:17<2:26:03, 16.82s/it]
                                                       

 74%|███████▍  | 1479/2000 [6:57:17<2:26:03, 16.82s/it]
 74%|███████▍  | 1480/2000 [6:57:34<2:25:54, 16.84s/it]
                                                       

 74%|███████▍  | 1480/2000 [6:57:34<2:25:54, 16.84s/it]
 74%|███████▍  | 1481/2000 [6:57:51<2:26:03, 16.89s/it]
                                                       

 74%|███████▍  | 1481/2000 [6:57:51<2:26:03, 16.89s/it]
 74%|███████▍  | 1482/2000 [6:58:08<2:25:39, 16.87s/it]
                                                       

 74%|███████▍  | 1482/2000 [6:58:08<2:25:39, 16.87s/it]
 74%|███████▍  | 1483/2000 [6:58:24<2:25:21, 16.87s/it]
                                                       

 74%|███████▍  | 1483/2000 [6:58:24<2:25:21, 16.87s/it]
 74%|███████▍  | 1484/2000 [6:58:41<2:25:00, 16.86s/it]
                                                       

 74%|███████▍  | 1484/2000 [6:58:41<2:25:00, 16.86s/it]
 74%|███████▍  | 1485/2000 [6:58:58<2:24:30, 16.83s/it]
                                                       

 74%|███████▍  | 1485/2000 [6:58:58<2:24:30, 16.83s/it]
 74%|███████▍  | 1486/2000 [6:59:15<2:24:26, 16.86s/it]
                                                       

 74%|███████▍  | 1486/2000 [6:59:15<2:24:26, 16.86s/it]
 74%|███████▍  | 1487/2000 [6:59:32<2:24:23, 16.89s/it]
                                                       

 74%|███████▍  | 1487/2000 [6:59:32<2:24:23, 16.89s/it]
 74%|███████▍  | 1488/2000 [6:59:49<2:23:56, 16.87s/it]
                                                       

 74%|███████▍  | 1488/2000 [6:59:49<2:23:56, 16.87s/it]
 74%|███████▍  | 1489/2000 [7:00:06<2:23:37, 16.86s/it]
                                                       

 74%|███████▍  | 1489/2000 [7:00:06<2:23:37, 16.86s/it]
 74%|███████▍  | 1490/2000 [7:00:23<2:23:22, 16.87s/it]
                                                       

 74%|███████▍  | 1490/2000 [7:00:23<2:23:22, 16.87s/it]
 75%|███████▍  | 1491/2000 [7:00:39<2:23:11, 16.88s/it]
                                                       

 75%|███████▍  | 1491/2000 [7:00:39<2:23:11, 16.88s/it]
 75%|███████▍  | 1492/2000 [7:00:56<2:22:45, 16.86s/it]
                                                       

 75%|███████▍  | 1492/2000 [7:00:56<2:22:45, 16.86s/it]
 75%|███████▍  | 1493/2000 [7:01:13<2:22:17, 16.84s/it]
                                                       

 75%|███████▍  | 1493/2000 [7:01:13<2:22:17, 16.84s/it]
 75%|███████▍  | 1494/2000 [7:01:30<2:22:06, 16.85s/it]
                                                       

 75%|███████▍  | 1494/2000 [7:01:30<2:22:06, 16.85s/it]
 75%|███████▍  | 1495/2000 [7:01:47<2:22:00, 16.87s/it]
                                                       

 75%|███████▍  | 1495/2000 [7:01:47<2:22:00, 16.87s/it]
 75%|███████▍  | 1496/2000 [7:02:04<2:21:44, 16.87s/it]
                                                       

 75%|███████▍  | 1496/2000 [7:02:04<2:21:44, 16.87s/it]
 75%|███████▍  | 1497/2000 [7:02:21<2:21:24, 16.87s/it]
                                                       

 75%|███████▍  | 1497/2000 [7:02:21<2:21:24, 16.87s/it]
 75%|███████▍  | 1498/2000 [7:02:38<2:21:20, 16.89s/it]
                                                       

 75%|███████▍  | 1498/2000 [7:02:38<2:21:20, 16.89s/it]
 75%|███████▍  | 1499/2000 [7:02:54<2:20:47, 16.86s/it]
                                                       

 75%|███████▍  | 1499/2000 [7:02:54<2:20:47, 16.86s/it]
 75%|███████▌  | 1500/2000 [7:03:11<2:20:23, 16.85s/it]
                                                       

 75%|███████▌  | 1500/2000 [7:03:11<2:20:23, 16.85s/it]
 75%|███████▌  | 1501/2000 [7:03:28<2:20:05, 16.84s/it]
                                                       

 75%|███████▌  | 1501/2000 [7:03:28<2:20:05, 16.84s/it]
 75%|███████▌  | 1502/2000 [7:03:45<2:20:01, 16.87s/it]
                                                       

 75%|███████▌  | 1502/2000 [7:03:45<2:20:01, 16.87s/it]
 75%|███████▌  | 1503/2000 [7:04:02<2:19:33, 16.85s/it]
                                                       

 75%|███████▌  | 1503/2000 [7:04:02<2:19:33, 16.85s/it]
 75%|███████▌  | 1504/2000 [7:04:19<2:19:19, 16.85s/it]
                                                       

 75%|███████▌  | 1504/2000 [7:04:19<2:19:19, 16.85s/it]
 75%|███████▌  | 1505/2000 [7:04:35<2:19:15, 16.88s/it]
                                                       

 75%|███████▌  | 1505/2000 [7:04:35<2:19:15, 16.88s/it]
 75%|███████▌  | 1506/2000 [7:04:52<2:18:51, 16.86s/it]
                                                       

 75%|███████▌  | 1506/2000 [7:04:52<2:18:51, 16.86s/it]
 75%|███████▌  | 1507/2000 [7:05:09<2:18:30, 16.86s/it]
                                                       

 75%|███████▌  | 1507/2000 [7:05:09<2:18:30, 16.86s/it]
 75%|███████▌  | 1508/2000 [7:05:26<2:18:26, 16.88s/it]
                                                       

 75%|███████▌  | 1508/2000 [7:05:26<2:18:26, 16.88s/it]
 75%|███████▌  | 1509/2000 [7:05:43<2:18:00, 16.86s/it]
                                                       

 75%|███████▌  | 1509/2000 [7:05:43<2:18:00, 16.86s/it]
 76%|███████▌  | 1510/2000 [7:06:00<2:17:49, 16.88s/it]
                                                       

 76%|███████▌  | 1510/2000 [7:06:00<2:17:49, 16.88s/it]
 76%|███████▌  | 1511/2000 [7:06:17<2:17:41, 16.89s/it]
                                                       

 76%|███████▌  | 1511/2000 [7:06:17<2:17:41, 16.89s/it]
 76%|███████▌  | 1512/2000 [7:06:34<2:17:21, 16.89s/it]
                                                       

 76%|███████▌  | 1512/2000 [7:06:34<2:17:21, 16.89s/it]
 76%|███████▌  | 1513/2000 [7:06:50<2:16:44, 16.85s/it]
                                                       

 76%|███████▌  | 1513/2000 [7:06:50<2:16:44, 16.85s/it]
 76%|███████▌  | 1514/2000 [7:07:07<2:16:32, 16.86s/it]
                                                       

 76%|███████▌  | 1514/2000 [7:07:07<2:16:32, 16.86s/it]
 76%|███████▌  | 1515/2000 [7:07:24<2:16:26, 16.88s/it]
                                                       

 76%|███████▌  | 1515/2000 [7:07:24<2:16:26, 16.88s/it]
 76%|███████▌  | 1516/2000 [7:07:41<2:16:03, 16.87s/it]
                                                       

 76%|███████▌  | 1516/2000 [7:07:41<2:16:03, 16.87s/it]
 76%|███████▌  | 1517/2000 [7:07:58<2:15:35, 16.84s/it]
                                                       

 76%|███████▌  | 1517/2000 [7:07:58<2:15:35, 16.84s/it]
 76%|███████▌  | 1518/2000 [7:08:15<2:15:15, 16.84s/it]
                                                       

 76%|███████▌  | 1518/2000 [7:08:15<2:15:15, 16.84s/it]
 76%|███████▌  | 1519/2000 [7:08:32<2:15:12, 16.87s/it]
                                                       

 76%|███████▌  | 1519/2000 [7:08:32<2:15:12, 16.87s/it]
 76%|███████▌  | 1520/2000 [7:08:48<2:15:01, 16.88s/it]
                                                       

 76%|███████▌  | 1520/2000 [7:08:48<2:15:01, 16.88s/it]
 76%|███████▌  | 1521/2000 [7:09:08<2:21:30, 17.73s/it]
                                                       

 76%|███████▌  | 1521/2000 [7:09:08<2:21:30, 17.73s/it]
 76%|███████▌  | 1522/2000 [7:09:25<2:20:02, 17.58s/it]
                                                       

 76%|███████▌  | 1522/2000 [7:09:25<2:20:02, 17.58s/it]
 76%|███████▌  | 1523/2000 [7:09:42<2:17:57, 17.35s/it]
                                                       

 76%|███████▌  | 1523/2000 [7:09:42<2:17:57, 17.35s/it]
 76%|███████▌  | 1524/2000 [7:09:59<2:16:33, 17.21s/it]
                                                       

 76%|███████▌  | 1524/2000 [7:09:59<2:16:33, 17.21s/it]
 76%|███████▋  | 1525/2000 [7:10:16<2:15:40, 17.14s/it]
                                                       

 76%|███████▋  | 1525/2000 [7:10:16<2:15:40, 17.14s/it]
 76%|███████▋  | 1526/2000 [7:10:33<2:14:45, 17.06s/it]
                                                       

 76%|███████▋  | 1526/2000 [7:10:33<2:14:45, 17.06s/it]
 76%|███████▋  | 1527/2000 [7:10:50<2:14:12, 17.02s/it]
                                                       

 76%|███████▋  | 1527/2000 [7:10:50<2:14:12, 17.02s/it]
 76%|███████▋  | 1528/2000 [7:11:07<2:13:42, 17.00s/it]
                                                       

 76%|███████▋  | 1528/2000 [7:11:07<2:13:42, 17.00s/it]
 76%|███████▋  | 1529/2000 [7:11:24<2:13:02, 16.95s/it]
                                                       

 76%|███████▋  | 1529/2000 [7:11:24<2:13:02, 16.95s/it]
 76%|███████▋  | 1530/2000 [7:11:41<2:12:28, 16.91s/it]
                                                       

 76%|███████▋  | 1530/2000 [7:11:41<2:12:28, 16.91s/it]
 77%|███████▋  | 1531/2000 [7:11:57<2:12:10, 16.91s/it]
                                                       

 77%|███████▋  | 1531/2000 [7:11:57<2:12:10, 16.91s/it]
 77%|███████▋  | 1532/2000 [7:12:14<2:12:03, 16.93s/it]
                                                       

 77%|███████▋  | 1532/2000 [7:12:14<2:12:03, 16.93s/it]
 77%|███████▋  | 1533/2000 [7:12:31<2:11:31, 16.90s/it]
                                                       

 77%|███████▋  | 1533/2000 [7:12:31<2:11:31, 16.90s/it]
 77%|███████▋  | 1534/2000 [7:12:48<2:11:17, 16.90s/it]
                                                       

 77%|███████▋  | 1534/2000 [7:12:48<2:11:17, 16.90s/it]
 77%|███████▋  | 1535/2000 [7:13:05<2:11:05, 16.91s/it]
                                                       

 77%|███████▋  | 1535/2000 [7:13:05<2:11:05, 16.91s/it]
 77%|███████▋  | 1536/2000 [7:13:22<2:10:32, 16.88s/it]
                                                       

 77%|███████▋  | 1536/2000 [7:13:22<2:10:32, 16.88s/it]
 77%|███████▋  | 1537/2000 [7:13:39<2:10:02, 16.85s/it]
                                                       

 77%|███████▋  | 1537/2000 [7:13:39<2:10:02, 16.85s/it]
 77%|███████▋  | 1538/2000 [7:13:55<2:09:37, 16.83s/it]
                                                       

 77%|███████▋  | 1538/2000 [7:13:55<2:09:37, 16.83s/it]
 77%|███████▋  | 1539/2000 [7:14:12<2:09:12, 16.82s/it]
                                                       

 77%|███████▋  | 1539/2000 [7:14:12<2:09:12, 16.82s/it]
 77%|███████▋  | 1540/2000 [7:14:29<2:08:52, 16.81s/it]
                                                       

 77%|███████▋  | 1540/2000 [7:14:29<2:08:52, 16.81s/it]
 77%|███████▋  | 1541/2000 [7:14:46<2:08:33, 16.81s/it]
                                                       

 77%|███████▋  | 1541/2000 [7:14:46<2:08:33, 16.81s/it]
 77%|███████▋  | 1542/2000 [7:15:03<2:08:10, 16.79s/it]
                                                       

 77%|███████▋  | 1542/2000 [7:15:03<2:08:10, 16.79s/it]
 77%|███████▋  | 1543/2000 [7:15:19<2:07:52, 16.79s/it]
                                                       

 77%|███████▋  | 1543/2000 [7:15:19<2:07:52, 16.79s/it]
 77%|███████▋  | 1544/2000 [7:15:36<2:07:34, 16.79s/it]
                                                       

 77%|███████▋  | 1544/2000 [7:15:36<2:07:34, 16.79s/it]
 77%|███████▋  | 1545/2000 [7:15:53<2:07:14, 16.78s/it]
                                                       

 77%|███████▋  | 1545/2000 [7:15:53<2:07:14, 16.78s/it]
 77%|███████▋  | 1546/2000 [7:16:10<2:07:05, 16.80s/it]
                                                       

 77%|███████▋  | 1546/2000 [7:16:10<2:07:05, 16.80s/it]
 77%|███████▋  | 1547/2000 [7:16:27<2:06:53, 16.81s/it]
                                                       

 77%|███████▋  | 1547/2000 [7:16:27<2:06:53, 16.81s/it]
 77%|███████▋  | 1548/2000 [7:16:43<2:06:44, 16.82s/it]
                                                       

 77%|███████▋  | 1548/2000 [7:16:43<2:06:44, 16.82s/it]
 77%|███████▋  | 1549/2000 [7:17:00<2:06:43, 16.86s/it]
                                                       

 77%|███████▋  | 1549/2000 [7:17:00<2:06:43, 16.86s/it]
 78%|███████▊  | 1550/2000 [7:17:17<2:06:24, 16.85s/it]
                                                       

 78%|███████▊  | 1550/2000 [7:17:17<2:06:24, 16.85s/it]
 78%|███████▊  | 1551/2000 [7:17:34<2:05:58, 16.83s/it]
                                                       

 78%|███████▊  | 1551/2000 [7:17:34<2:05:58, 16.83s/it]
 78%|███████▊  | 1552/2000 [7:17:51<2:05:37, 16.82s/it]
                                                       

 78%|███████▊  | 1552/2000 [7:17:51<2:05:37, 16.82s/it]
 78%|███████▊  | 1553/2000 [7:18:08<2:05:16, 16.81s/it]
                                                       

 78%|███████▊  | 1553/2000 [7:18:08<2:05:16, 16.81s/it]
 78%|███████▊  | 1554/2000 [7:18:25<2:05:14, 16.85s/it]
                                                       

 78%|███████▊  | 1554/2000 [7:18:25<2:05:14, 16.85s/it]
 78%|███████▊  | 1555/2000 [7:18:41<2:05:05, 16.87s/it]
                                                       

 78%|███████▊  | 1555/2000 [7:18:41<2:05:05, 16.87s/it]
 78%|███████▊  | 1556/2000 [7:18:58<2:04:46, 16.86s/it]
                                                       

 78%|███████▊  | 1556/2000 [7:18:58<2:04:46, 16.86s/it]
 78%|███████▊  | 1557/2000 [7:19:15<2:04:35, 16.87s/it]
                                                       

 78%|███████▊  | 1557/2000 [7:19:15<2:04:35, 16.87s/it]
 78%|███████▊  | 1558/2000 [7:19:32<2:04:27, 16.89s/it]
                                                       

 78%|███████▊  | 1558/2000 [7:19:32<2:04:27, 16.89s/it]
 78%|███████▊  | 1559/2000 [7:19:49<2:04:02, 16.88s/it]
                                                       

 78%|███████▊  | 1559/2000 [7:19:49<2:04:02, 16.88s/it]
 78%|███████▊  | 1560/2000 [7:20:06<2:03:35, 16.85s/it]
                                                       

 78%|███████▊  | 1560/2000 [7:20:06<2:03:35, 16.85s/it]
 78%|███████▊  | 1561/2000 [7:20:23<2:03:18, 16.85s/it]
                                                       

 78%|███████▊  | 1561/2000 [7:20:23<2:03:18, 16.85s/it]
 78%|███████▊  | 1562/2000 [7:20:40<2:04:45, 17.09s/it]
                                                       

 78%|███████▊  | 1562/2000 [7:20:40<2:04:45, 17.09s/it]
 78%|███████▊  | 1563/2000 [7:20:57<2:03:48, 17.00s/it]
                                                       

 78%|███████▊  | 1563/2000 [7:20:57<2:03:48, 17.00s/it]
 78%|███████▊  | 1564/2000 [7:21:14<2:03:16, 16.96s/it]
                                                       

 78%|███████▊  | 1564/2000 [7:21:14<2:03:16, 16.96s/it]
 78%|███████▊  | 1565/2000 [7:21:31<2:02:49, 16.94s/it]
                                                       

 78%|███████▊  | 1565/2000 [7:21:31<2:02:49, 16.94s/it]
 78%|███████▊  | 1566/2000 [7:21:48<2:02:35, 16.95s/it]
                                                       

 78%|███████▊  | 1566/2000 [7:21:48<2:02:35, 16.95s/it]
 78%|███████▊  | 1567/2000 [7:22:05<2:02:02, 16.91s/it]
                                                       

 78%|███████▊  | 1567/2000 [7:22:05<2:02:02, 16.91s/it]
 78%|███████▊  | 1568/2000 [7:22:21<2:01:43, 16.91s/it]
                                                       

 78%|███████▊  | 1568/2000 [7:22:21<2:01:43, 16.91s/it]
 78%|███████▊  | 1569/2000 [7:22:38<2:01:36, 16.93s/it]
                                                       

 78%|███████▊  | 1569/2000 [7:22:38<2:01:36, 16.93s/it]
 78%|███████▊  | 1570/2000 [7:22:55<2:01:07, 16.90s/it]
                                                       

 78%|███████▊  | 1570/2000 [7:22:55<2:01:07, 16.90s/it]
 79%|███████▊  | 1571/2000 [7:23:12<2:00:41, 16.88s/it]
                                                       

 79%|███████▊  | 1571/2000 [7:23:12<2:00:41, 16.88s/it]
 79%|███████▊  | 1572/2000 [7:23:29<2:00:14, 16.86s/it]
                                                       

 79%|███████▊  | 1572/2000 [7:23:29<2:00:14, 16.86s/it]
 79%|███████▊  | 1573/2000 [7:23:46<1:59:55, 16.85s/it]
                                                       

 79%|███████▊  | 1573/2000 [7:23:46<1:59:55, 16.85s/it]
 79%|███████▊  | 1574/2000 [7:24:03<1:59:39, 16.85s/it]
                                                       

 79%|███████▊  | 1574/2000 [7:24:03<1:59:39, 16.85s/it]
 79%|███████▉  | 1575/2000 [7:24:20<1:59:24, 16.86s/it]
                                                       

 79%|███████▉  | 1575/2000 [7:24:20<1:59:24, 16.86s/it]
 79%|███████▉  | 1576/2000 [7:24:36<1:59:17, 16.88s/it]
                                                       

 79%|███████▉  | 1576/2000 [7:24:36<1:59:17, 16.88s/it]
 79%|███████▉  | 1577/2000 [7:24:53<1:58:50, 16.86s/it]
                                                       

 79%|███████▉  | 1577/2000 [7:24:53<1:58:50, 16.86s/it]
 79%|███████▉  | 1578/2000 [7:25:10<1:58:27, 16.84s/it]
                                                       

 79%|███████▉  | 1578/2000 [7:25:10<1:58:27, 16.84s/it]
 79%|███████▉  | 1579/2000 [7:25:27<1:58:13, 16.85s/it]
                                                       

 79%|███████▉  | 1579/2000 [7:25:27<1:58:13, 16.85s/it]
 79%|███████▉  | 1580/2000 [7:25:44<1:58:10, 16.88s/it]
                                                       

 79%|███████▉  | 1580/2000 [7:25:44<1:58:10, 16.88s/it]
 79%|███████▉  | 1581/2000 [7:26:01<1:57:42, 16.86s/it]
                                                       

 79%|███████▉  | 1581/2000 [7:26:01<1:57:42, 16.86s/it]
 79%|███████▉  | 1582/2000 [7:26:18<1:57:31, 16.87s/it]
                                                       

 79%|███████▉  | 1582/2000 [7:26:18<1:57:31, 16.87s/it]
 79%|███████▉  | 1583/2000 [7:26:35<1:57:22, 16.89s/it]
                                                       

 79%|███████▉  | 1583/2000 [7:26:35<1:57:22, 16.89s/it]
 79%|███████▉  | 1584/2000 [7:26:51<1:56:52, 16.86s/it]
                                                       

 79%|███████▉  | 1584/2000 [7:26:51<1:56:52, 16.86s/it]
 79%|███████▉  | 1585/2000 [7:27:08<1:56:23, 16.83s/it]
                                                       

 79%|███████▉  | 1585/2000 [7:27:08<1:56:23, 16.83s/it]
 79%|███████▉  | 1586/2000 [7:27:25<1:56:08, 16.83s/it]
                                                       

 79%|███████▉  | 1586/2000 [7:27:25<1:56:08, 16.83s/it]
 79%|███████▉  | 1587/2000 [7:27:42<1:55:51, 16.83s/it]
                                                       

 79%|███████▉  | 1587/2000 [7:27:42<1:55:51, 16.83s/it]
 79%|███████▉  | 1588/2000 [7:27:58<1:55:26, 16.81s/it]
                                                       

 79%|███████▉  | 1588/2000 [7:27:58<1:55:26, 16.81s/it]
 79%|███████▉  | 1589/2000 [7:28:15<1:55:03, 16.80s/it]
                                                       

 79%|███████▉  | 1589/2000 [7:28:15<1:55:03, 16.80s/it]
 80%|███████▉  | 1590/2000 [7:28:32<1:54:49, 16.80s/it]
                                                       

 80%|███████▉  | 1590/2000 [7:28:32<1:54:49, 16.80s/it]
 80%|███████▉  | 1591/2000 [7:28:49<1:54:37, 16.82s/it]
                                                       

 80%|███████▉  | 1591/2000 [7:28:49<1:54:37, 16.82s/it]
 80%|███████▉  | 1592/2000 [7:29:06<1:54:14, 16.80s/it]
                                                       

 80%|███████▉  | 1592/2000 [7:29:06<1:54:14, 16.80s/it]
 80%|███████▉  | 1593/2000 [7:29:22<1:53:54, 16.79s/it]
                                                       

 80%|███████▉  | 1593/2000 [7:29:22<1:53:54, 16.79s/it]
 80%|███████▉  | 1594/2000 [7:29:39<1:53:49, 16.82s/it]
                                                       

 80%|███████▉  | 1594/2000 [7:29:39<1:53:49, 16.82s/it]
 80%|███████▉  | 1595/2000 [7:29:56<1:53:49, 16.86s/it]
                                                       

 80%|███████▉  | 1595/2000 [7:29:56<1:53:49, 16.86s/it]
 80%|███████▉  | 1596/2000 [7:30:13<1:53:29, 16.85s/it]
                                                       

 80%|███████▉  | 1596/2000 [7:30:13<1:53:29, 16.85s/it]
 80%|███████▉  | 1597/2000 [7:30:30<1:53:01, 16.83s/it]
                                                       

 80%|███████▉  | 1597/2000 [7:30:30<1:53:01, 16.83s/it]
 80%|███████▉  | 1598/2000 [7:30:47<1:52:35, 16.81s/it]
                                                       

 80%|███████▉  | 1598/2000 [7:30:47<1:52:35, 16.81s/it]
 80%|███████▉  | 1599/2000 [7:31:03<1:52:13, 16.79s/it]
                                                       

 80%|███████▉  | 1599/2000 [7:31:03<1:52:13, 16.79s/it]
 80%|████████  | 1600/2000 [7:31:20<1:52:07, 16.82s/it]
                                                       

 80%|████████  | 1600/2000 [7:31:20<1:52:07, 16.82s/it]
 80%|████████  | 1601/2000 [7:31:40<1:57:37, 17.69s/it]
                                                       

 80%|████████  | 1601/2000 [7:31:40<1:57:37, 17.69s/it]
 80%|████████  | 1602/2000 [7:31:57<1:55:25, 17.40s/it]
                                                       

 80%|████████  | 1602/2000 [7:31:57<1:55:25, 17.40s/it]
 80%|████████  | 1603/2000 [7:32:14<1:54:33, 17.31s/it]
                                                       

 80%|████████  | 1603/2000 [7:32:14<1:54:33, 17.31s/it]
 80%|████████  | 1604/2000 [7:32:31<1:53:13, 17.16s/it]
                                                       

 80%|████████  | 1604/2000 [7:32:31<1:53:13, 17.16s/it]
 80%|████████  | 1605/2000 [7:32:47<1:52:12, 17.05s/it]
                                                       

 80%|████████  | 1605/2000 [7:32:47<1:52:12, 17.05s/it]
 80%|████████  | 1606/2000 [7:33:04<1:51:26, 16.97s/it]
                                                       

 80%|████████  | 1606/2000 [7:33:04<1:51:26, 16.97s/it]
 80%|████████  | 1607/2000 [7:33:21<1:50:52, 16.93s/it]
                                                       

 80%|████████  | 1607/2000 [7:33:21<1:50:52, 16.93s/it]
 80%|████████  | 1608/2000 [7:33:38<1:50:26, 16.90s/it]
                                                       

 80%|████████  | 1608/2000 [7:33:38<1:50:26, 16.90s/it]
 80%|████████  | 1609/2000 [7:33:55<1:49:53, 16.86s/it]
                                                       

 80%|████████  | 1609/2000 [7:33:55<1:49:53, 16.86s/it]
 80%|████████  | 1610/2000 [7:34:11<1:49:28, 16.84s/it]
                                                       

 80%|████████  | 1610/2000 [7:34:11<1:49:28, 16.84s/it]
 81%|████████  | 1611/2000 [7:34:28<1:49:12, 16.84s/it]
                                                       

 81%|████████  | 1611/2000 [7:34:28<1:49:12, 16.84s/it]
 81%|████████  | 1612/2000 [7:34:45<1:48:49, 16.83s/it]
                                                       

 81%|████████  | 1612/2000 [7:34:45<1:48:49, 16.83s/it]
 81%|████████  | 1613/2000 [7:35:02<1:48:32, 16.83s/it]
                                                       

 81%|████████  | 1613/2000 [7:35:02<1:48:32, 16.83s/it]
 81%|████████  | 1614/2000 [7:35:19<1:48:14, 16.83s/it]
                                                       

 81%|████████  | 1614/2000 [7:35:19<1:48:14, 16.83s/it]
 81%|████████  | 1615/2000 [7:35:36<1:47:51, 16.81s/it]
                                                       

 81%|████████  | 1615/2000 [7:35:36<1:47:51, 16.81s/it]
 81%|████████  | 1616/2000 [7:35:52<1:47:36, 16.81s/it]
                                                       

 81%|████████  | 1616/2000 [7:35:52<1:47:36, 16.81s/it]
 81%|████████  | 1617/2000 [7:36:09<1:47:20, 16.81s/it]
                                                       

 81%|████████  | 1617/2000 [7:36:09<1:47:20, 16.81s/it]
 81%|████████  | 1618/2000 [7:36:26<1:47:08, 16.83s/it]
                                                       

 81%|████████  | 1618/2000 [7:36:26<1:47:08, 16.83s/it]
 81%|████████  | 1619/2000 [7:36:43<1:46:47, 16.82s/it]
                                                       

 81%|████████  | 1619/2000 [7:36:43<1:46:47, 16.82s/it]
 81%|████████  | 1620/2000 [7:37:00<1:46:26, 16.81s/it]
                                                       

 81%|████████  | 1620/2000 [7:37:00<1:46:26, 16.81s/it]
 81%|████████  | 1621/2000 [7:37:16<1:46:10, 16.81s/it]
                                                       

 81%|████████  | 1621/2000 [7:37:16<1:46:10, 16.81s/it]
 81%|████████  | 1622/2000 [7:37:33<1:45:53, 16.81s/it]
                                                       

 81%|████████  | 1622/2000 [7:37:33<1:45:53, 16.81s/it]
 81%|████████  | 1623/2000 [7:37:50<1:45:35, 16.81s/it]
                                                       

 81%|████████  | 1623/2000 [7:37:50<1:45:35, 16.81s/it]
 81%|████████  | 1624/2000 [7:38:07<1:45:24, 16.82s/it]
                                                       

 81%|████████  | 1624/2000 [7:38:07<1:45:24, 16.82s/it]
 81%|████████▏ | 1625/2000 [7:38:24<1:45:02, 16.81s/it]
                                                       

 81%|████████▏ | 1625/2000 [7:38:24<1:45:02, 16.81s/it]
 81%|████████▏ | 1626/2000 [7:38:40<1:44:47, 16.81s/it]
                                                       

 81%|████████▏ | 1626/2000 [7:38:40<1:44:47, 16.81s/it]
 81%|████████▏ | 1627/2000 [7:38:57<1:44:26, 16.80s/it]
                                                       

 81%|████████▏ | 1627/2000 [7:38:57<1:44:26, 16.80s/it]
 81%|████████▏ | 1628/2000 [7:39:14<1:44:10, 16.80s/it]
                                                       

 81%|████████▏ | 1628/2000 [7:39:14<1:44:10, 16.80s/it]
 81%|████████▏ | 1629/2000 [7:39:31<1:43:50, 16.79s/it]
                                                       

 81%|████████▏ | 1629/2000 [7:39:31<1:43:50, 16.79s/it]
 82%|████████▏ | 1630/2000 [7:39:48<1:43:29, 16.78s/it]
                                                       

 82%|████████▏ | 1630/2000 [7:39:48<1:43:29, 16.78s/it]
 82%|████████▏ | 1631/2000 [7:40:04<1:43:14, 16.79s/it]
                                                       

 82%|████████▏ | 1631/2000 [7:40:04<1:43:14, 16.79s/it]
 82%|████████▏ | 1632/2000 [7:40:21<1:42:59, 16.79s/it]
                                                       

 82%|████████▏ | 1632/2000 [7:40:21<1:42:59, 16.79s/it]
 82%|████████▏ | 1633/2000 [7:40:38<1:42:48, 16.81s/it]
                                                       

 82%|████████▏ | 1633/2000 [7:40:38<1:42:48, 16.81s/it]
 82%|████████▏ | 1634/2000 [7:40:55<1:42:26, 16.79s/it]
                                                       

 82%|████████▏ | 1634/2000 [7:40:55<1:42:26, 16.79s/it]
 82%|████████▏ | 1635/2000 [7:41:12<1:42:15, 16.81s/it]
                                                       

 82%|████████▏ | 1635/2000 [7:41:12<1:42:15, 16.81s/it]
 82%|████████▏ | 1636/2000 [7:41:28<1:41:57, 16.81s/it]
                                                       

 82%|████████▏ | 1636/2000 [7:41:28<1:41:57, 16.81s/it]
 82%|████████▏ | 1637/2000 [7:41:45<1:41:43, 16.81s/it]
                                                       

 82%|████████▏ | 1637/2000 [7:41:45<1:41:43, 16.81s/it]
 82%|████████▏ | 1638/2000 [7:42:02<1:41:27, 16.82s/it]
                                                       

 82%|████████▏ | 1638/2000 [7:42:02<1:41:27, 16.82s/it]
 82%|████████▏ | 1639/2000 [7:42:19<1:41:14, 16.83s/it]
                                                       

 82%|████████▏ | 1639/2000 [7:42:19<1:41:14, 16.83s/it]
 82%|████████▏ | 1640/2000 [7:42:36<1:41:00, 16.83s/it]
                                                       

 82%|████████▏ | 1640/2000 [7:42:36<1:41:00, 16.83s/it]
 82%|████████▏ | 1641/2000 [7:42:53<1:40:35, 16.81s/it]
                                                       

 82%|████████▏ | 1641/2000 [7:42:53<1:40:35, 16.81s/it]
 82%|████████▏ | 1642/2000 [7:43:09<1:40:14, 16.80s/it]
                                                       

 82%|████████▏ | 1642/2000 [7:43:09<1:40:14, 16.80s/it]
 82%|████████▏ | 1643/2000 [7:43:26<1:39:58, 16.80s/it]
                                                       

 82%|████████▏ | 1643/2000 [7:43:26<1:39:58, 16.80s/it]
 82%|████████▏ | 1644/2000 [7:43:43<1:39:39, 16.80s/it]
                                                       

 82%|████████▏ | 1644/2000 [7:43:43<1:39:39, 16.80s/it]
 82%|████████▏ | 1645/2000 [7:44:00<1:39:21, 16.79s/it]
                                                       

 82%|████████▏ | 1645/2000 [7:44:00<1:39:21, 16.79s/it]
 82%|████████▏ | 1646/2000 [7:44:16<1:39:05, 16.80s/it]
                                                       

 82%|████████▏ | 1646/2000 [7:44:17<1:39:05, 16.80s/it]
 82%|████████▏ | 1647/2000 [7:44:33<1:38:51, 16.80s/it]
                                                       

 82%|████████▏ | 1647/2000 [7:44:33<1:38:51, 16.80s/it]
 82%|████████▏ | 1648/2000 [7:44:50<1:38:24, 16.77s/it]
                                                       

 82%|████████▏ | 1648/2000 [7:44:50<1:38:24, 16.77s/it]
 82%|████████▏ | 1649/2000 [7:45:07<1:38:03, 16.76s/it]
                                                       

 82%|████████▏ | 1649/2000 [7:45:07<1:38:03, 16.76s/it]
 82%|████████▎ | 1650/2000 [7:45:24<1:38:14, 16.84s/it]
                                                       

 82%|████████▎ | 1650/2000 [7:45:24<1:38:14, 16.84s/it]
 83%|████████▎ | 1651/2000 [7:45:41<1:37:51, 16.82s/it]
                                                       

 83%|████████▎ | 1651/2000 [7:45:41<1:37:51, 16.82s/it]
 83%|████████▎ | 1652/2000 [7:45:57<1:37:34, 16.82s/it]
                                                       

 83%|████████▎ | 1652/2000 [7:45:57<1:37:34, 16.82s/it]
 83%|████████▎ | 1653/2000 [7:46:14<1:37:17, 16.82s/it]
                                                       

 83%|████████▎ | 1653/2000 [7:46:14<1:37:17, 16.82s/it]
 83%|████████▎ | 1654/2000 [7:46:31<1:37:05, 16.84s/it]
                                                       

 83%|████████▎ | 1654/2000 [7:46:31<1:37:05, 16.84s/it]
 83%|████████▎ | 1655/2000 [7:46:48<1:36:46, 16.83s/it]
                                                       

 83%|████████▎ | 1655/2000 [7:46:48<1:36:46, 16.83s/it]
 83%|████████▎ | 1656/2000 [7:47:05<1:36:26, 16.82s/it]
                                                       

 83%|████████▎ | 1656/2000 [7:47:05<1:36:26, 16.82s/it]
 83%|████████▎ | 1657/2000 [7:47:21<1:36:05, 16.81s/it]
                                                       

 83%|████████▎ | 1657/2000 [7:47:21<1:36:05, 16.81s/it]
 83%|████████▎ | 1658/2000 [7:47:38<1:35:46, 16.80s/it]
                                                       

 83%|████████▎ | 1658/2000 [7:47:38<1:35:46, 16.80s/it]
 83%|████████▎ | 1659/2000 [7:47:55<1:35:30, 16.81s/it]
                                                       

 83%|████████▎ | 1659/2000 [7:47:55<1:35:30, 16.81s/it]
 83%|████████▎ | 1660/2000 [7:48:12<1:35:10, 16.80s/it]
                                                       

 83%|████████▎ | 1660/2000 [7:48:12<1:35:10, 16.80s/it]
 83%|████████▎ | 1661/2000 [7:48:29<1:34:50, 16.79s/it]
                                                       

 83%|████████▎ | 1661/2000 [7:48:29<1:34:50, 16.79s/it]
 83%|████████▎ | 1662/2000 [7:48:45<1:34:35, 16.79s/it]
                                                       

 83%|████████▎ | 1662/2000 [7:48:45<1:34:35, 16.79s/it]
 83%|████████▎ | 1663/2000 [7:49:02<1:34:18, 16.79s/it]
                                                       

 83%|████████▎ | 1663/2000 [7:49:02<1:34:18, 16.79s/it]
 83%|████████▎ | 1664/2000 [7:49:19<1:34:03, 16.80s/it]
                                                       

 83%|████████▎ | 1664/2000 [7:49:19<1:34:03, 16.80s/it]
 83%|████████▎ | 1665/2000 [7:49:36<1:33:50, 16.81s/it]
                                                       

 83%|████████▎ | 1665/2000 [7:49:36<1:33:50, 16.81s/it]
 83%|████████▎ | 1666/2000 [7:49:53<1:33:34, 16.81s/it]
                                                       

 83%|████████▎ | 1666/2000 [7:49:53<1:33:34, 16.81s/it]
 83%|████████▎ | 1667/2000 [7:50:09<1:33:16, 16.81s/it]
                                                       

 83%|████████▎ | 1667/2000 [7:50:09<1:33:16, 16.81s/it]
 83%|████████▎ | 1668/2000 [7:50:26<1:32:52, 16.79s/it]
                                                       

 83%|████████▎ | 1668/2000 [7:50:26<1:32:52, 16.79s/it]
 83%|████████▎ | 1669/2000 [7:50:43<1:32:37, 16.79s/it]
                                                       

 83%|████████▎ | 1669/2000 [7:50:43<1:32:37, 16.79s/it]
 84%|████████▎ | 1670/2000 [7:51:00<1:32:21, 16.79s/it]
                                                       

 84%|████████▎ | 1670/2000 [7:51:00<1:32:21, 16.79s/it]
 84%|████████▎ | 1671/2000 [7:51:17<1:32:08, 16.80s/it]
                                                       

 84%|████████▎ | 1671/2000 [7:51:17<1:32:08, 16.80s/it]
 84%|████████▎ | 1672/2000 [7:51:33<1:31:49, 16.80s/it]
                                                       

 84%|████████▎ | 1672/2000 [7:51:33<1:31:49, 16.80s/it]
 84%|████████▎ | 1673/2000 [7:51:50<1:31:30, 16.79s/it]
                                                       

 84%|████████▎ | 1673/2000 [7:51:50<1:31:30, 16.79s/it]
 84%|████████▎ | 1674/2000 [7:52:07<1:31:10, 16.78s/it]
                                                       

 84%|████████▎ | 1674/2000 [7:52:07<1:31:10, 16.78s/it]
 84%|████████▍ | 1675/2000 [7:52:24<1:30:53, 16.78s/it]
                                                       

 84%|████████▍ | 1675/2000 [7:52:24<1:30:53, 16.78s/it]
 84%|████████▍ | 1676/2000 [7:52:40<1:30:35, 16.78s/it]
                                                       

 84%|████████▍ | 1676/2000 [7:52:41<1:30:35, 16.78s/it]
 84%|████████▍ | 1677/2000 [7:52:57<1:30:19, 16.78s/it]
                                                       

 84%|████████▍ | 1677/2000 [7:52:57<1:30:19, 16.78s/it]
 84%|████████▍ | 1678/2000 [7:53:14<1:29:58, 16.77s/it]
                                                       

 84%|████████▍ | 1678/2000 [7:53:14<1:29:58, 16.77s/it]
 84%|████████▍ | 1679/2000 [7:53:31<1:29:43, 16.77s/it]
                                                       

 84%|████████▍ | 1679/2000 [7:53:31<1:29:43, 16.77s/it]
 84%|████████▍ | 1680/2000 [7:53:48<1:29:36, 16.80s/it]
                                                       

 84%|████████▍ | 1680/2000 [7:53:48<1:29:36, 16.80s/it]
 84%|████████▍ | 1681/2000 [7:54:07<1:33:51, 17.65s/it]
                                                       

 84%|████████▍ | 1681/2000 [7:54:07<1:33:51, 17.65s/it]
 84%|████████▍ | 1682/2000 [7:54:24<1:32:10, 17.39s/it]
                                                       

 84%|████████▍ | 1682/2000 [7:54:24<1:32:10, 17.39s/it]
 84%|████████▍ | 1683/2000 [7:54:41<1:30:57, 17.22s/it]
                                                       

 84%|████████▍ | 1683/2000 [7:54:41<1:30:57, 17.22s/it]
 84%|████████▍ | 1684/2000 [7:54:58<1:30:03, 17.10s/it]
                                                       

 84%|████████▍ | 1684/2000 [7:54:58<1:30:03, 17.10s/it]
 84%|████████▍ | 1685/2000 [7:55:15<1:29:18, 17.01s/it]
                                                       

 84%|████████▍ | 1685/2000 [7:55:15<1:29:18, 17.01s/it]
 84%|████████▍ | 1686/2000 [7:55:31<1:28:41, 16.95s/it]
                                                       

 84%|████████▍ | 1686/2000 [7:55:31<1:28:41, 16.95s/it]
 84%|████████▍ | 1687/2000 [7:55:48<1:28:15, 16.92s/it]
                                                       

 84%|████████▍ | 1687/2000 [7:55:48<1:28:15, 16.92s/it]
 84%|████████▍ | 1688/2000 [7:56:05<1:27:50, 16.89s/it]
                                                       

 84%|████████▍ | 1688/2000 [7:56:05<1:27:50, 16.89s/it]
 84%|████████▍ | 1689/2000 [7:56:22<1:27:25, 16.87s/it]
                                                       

 84%|████████▍ | 1689/2000 [7:56:22<1:27:25, 16.87s/it]
 84%|████████▍ | 1690/2000 [7:56:39<1:27:01, 16.84s/it]
                                                       

 84%|████████▍ | 1690/2000 [7:56:39<1:27:01, 16.84s/it]
 85%|████████▍ | 1691/2000 [7:56:55<1:26:41, 16.83s/it]
                                                       

 85%|████████▍ | 1691/2000 [7:56:55<1:26:41, 16.83s/it]
 85%|████████▍ | 1692/2000 [7:57:12<1:26:18, 16.81s/it]
                                                       

 85%|████████▍ | 1692/2000 [7:57:12<1:26:18, 16.81s/it]
 85%|████████▍ | 1693/2000 [7:57:29<1:25:57, 16.80s/it]
                                                       

 85%|████████▍ | 1693/2000 [7:57:29<1:25:57, 16.80s/it]
 85%|████████▍ | 1694/2000 [7:57:46<1:25:38, 16.79s/it]
                                                       

 85%|████████▍ | 1694/2000 [7:57:46<1:25:38, 16.79s/it]
 85%|████████▍ | 1695/2000 [7:58:02<1:25:19, 16.79s/it]
                                                       

 85%|████████▍ | 1695/2000 [7:58:03<1:25:19, 16.79s/it]
 85%|████████▍ | 1696/2000 [7:58:19<1:25:00, 16.78s/it]
                                                       

 85%|████████▍ | 1696/2000 [7:58:19<1:25:00, 16.78s/it]
 85%|████████▍ | 1697/2000 [7:58:36<1:24:41, 16.77s/it]
                                                       

 85%|████████▍ | 1697/2000 [7:58:36<1:24:41, 16.77s/it]
 85%|████████▍ | 1698/2000 [7:58:53<1:24:29, 16.79s/it]
                                                       

 85%|████████▍ | 1698/2000 [7:58:53<1:24:29, 16.79s/it]
 85%|████████▍ | 1699/2000 [7:59:10<1:24:18, 16.81s/it]
                                                       

 85%|████████▍ | 1699/2000 [7:59:10<1:24:18, 16.81s/it]
 85%|████████▌ | 1700/2000 [7:59:26<1:23:58, 16.79s/it]
                                                       

 85%|████████▌ | 1700/2000 [7:59:26<1:23:58, 16.79s/it]
 85%|████████▌ | 1701/2000 [7:59:43<1:23:41, 16.79s/it]
                                                       

 85%|████████▌ | 1701/2000 [7:59:43<1:23:41, 16.79s/it]
 85%|████████▌ | 1702/2000 [8:00:00<1:23:20, 16.78s/it]
                                                       

 85%|████████▌ | 1702/2000 [8:00:00<1:23:20, 16.78s/it]
 85%|████████▌ | 1703/2000 [8:00:17<1:23:05, 16.78s/it]
                                                       

 85%|████████▌ | 1703/2000 [8:00:17<1:23:05, 16.78s/it]
 85%|████████▌ | 1704/2000 [8:00:34<1:22:50, 16.79s/it]
                                                       

 85%|████████▌ | 1704/2000 [8:00:34<1:22:50, 16.79s/it]
 85%|████████▌ | 1705/2000 [8:00:50<1:22:33, 16.79s/it]
                                                       

 85%|████████▌ | 1705/2000 [8:00:50<1:22:33, 16.79s/it]
 85%|████████▌ | 1706/2000 [8:01:07<1:22:20, 16.80s/it]
                                                       

 85%|████████▌ | 1706/2000 [8:01:07<1:22:20, 16.80s/it]
 85%|████████▌ | 1707/2000 [8:01:24<1:21:59, 16.79s/it]
                                                       

 85%|████████▌ | 1707/2000 [8:01:24<1:21:59, 16.79s/it]
 85%|████████▌ | 1708/2000 [8:01:41<1:21:45, 16.80s/it]
                                                       

 85%|████████▌ | 1708/2000 [8:01:41<1:21:45, 16.80s/it]
 85%|████████▌ | 1709/2000 [8:01:58<1:21:27, 16.80s/it]
                                                       

 85%|████████▌ | 1709/2000 [8:01:58<1:21:27, 16.80s/it]
 86%|████████▌ | 1710/2000 [8:02:14<1:21:12, 16.80s/it]
                                                       

 86%|████████▌ | 1710/2000 [8:02:14<1:21:12, 16.80s/it]
 86%|████████▌ | 1711/2000 [8:02:31<1:20:52, 16.79s/it]
                                                       

 86%|████████▌ | 1711/2000 [8:02:31<1:20:52, 16.79s/it]
 86%|████████▌ | 1712/2000 [8:02:48<1:20:37, 16.80s/it]
                                                       

 86%|████████▌ | 1712/2000 [8:02:48<1:20:37, 16.80s/it]
 86%|████████▌ | 1713/2000 [8:03:05<1:20:18, 16.79s/it]
                                                       

 86%|████████▌ | 1713/2000 [8:03:05<1:20:18, 16.79s/it]
 86%|████████▌ | 1714/2000 [8:03:22<1:20:15, 16.84s/it]
                                                       

 86%|████████▌ | 1714/2000 [8:03:22<1:20:15, 16.84s/it]
 86%|████████▌ | 1715/2000 [8:03:39<1:20:03, 16.86s/it]
                                                       

 86%|████████▌ | 1715/2000 [8:03:39<1:20:03, 16.86s/it]
 86%|████████▌ | 1716/2000 [8:03:55<1:19:43, 16.84s/it]
                                                       

 86%|████████▌ | 1716/2000 [8:03:55<1:19:43, 16.84s/it]
 86%|████████▌ | 1717/2000 [8:04:12<1:19:23, 16.83s/it]
                                                       

 86%|████████▌ | 1717/2000 [8:04:12<1:19:23, 16.83s/it]
 86%|████████▌ | 1718/2000 [8:04:29<1:19:01, 16.81s/it]
                                                       

 86%|████████▌ | 1718/2000 [8:04:29<1:19:01, 16.81s/it]
 86%|████████▌ | 1719/2000 [8:04:46<1:18:43, 16.81s/it]
                                                       

 86%|████████▌ | 1719/2000 [8:04:46<1:18:43, 16.81s/it]
 86%|████████▌ | 1720/2000 [8:05:03<1:18:27, 16.81s/it]
                                                       

 86%|████████▌ | 1720/2000 [8:05:03<1:18:27, 16.81s/it]
 86%|████████▌ | 1721/2000 [8:05:20<1:18:17, 16.84s/it]
                                                       

 86%|████████▌ | 1721/2000 [8:05:20<1:18:17, 16.84s/it]
 86%|████████▌ | 1722/2000 [8:05:36<1:17:56, 16.82s/it]
                                                       

 86%|████████▌ | 1722/2000 [8:05:36<1:17:56, 16.82s/it]
 86%|████████▌ | 1723/2000 [8:05:53<1:17:45, 16.84s/it]
                                                       

 86%|████████▌ | 1723/2000 [8:05:53<1:17:45, 16.84s/it]
 86%|████████▌ | 1724/2000 [8:06:10<1:17:35, 16.87s/it]
                                                       

 86%|████████▌ | 1724/2000 [8:06:10<1:17:35, 16.87s/it]
 86%|████████▋ | 1725/2000 [8:06:27<1:17:14, 16.85s/it]
                                                       

 86%|████████▋ | 1725/2000 [8:06:27<1:17:14, 16.85s/it]
 86%|████████▋ | 1726/2000 [8:06:44<1:17:29, 16.97s/it]
                                                       

 86%|████████▋ | 1726/2000 [8:06:44<1:17:29, 16.97s/it]
 86%|████████▋ | 1727/2000 [8:07:01<1:17:06, 16.95s/it]
                                                       

 86%|████████▋ | 1727/2000 [8:07:01<1:17:06, 16.95s/it]
 86%|████████▋ | 1728/2000 [8:07:18<1:16:36, 16.90s/it]
                                                       

 86%|████████▋ | 1728/2000 [8:07:18<1:16:36, 16.90s/it]
 86%|████████▋ | 1729/2000 [8:07:35<1:16:16, 16.89s/it]
                                                       

 86%|████████▋ | 1729/2000 [8:07:35<1:16:16, 16.89s/it]
 86%|████████▋ | 1730/2000 [8:07:52<1:16:01, 16.89s/it]
                                                       

 86%|████████▋ | 1730/2000 [8:07:52<1:16:01, 16.89s/it]
 87%|████████▋ | 1731/2000 [8:08:08<1:15:35, 16.86s/it]
                                                       

 87%|████████▋ | 1731/2000 [8:08:08<1:15:35, 16.86s/it]
 87%|████████▋ | 1732/2000 [8:08:25<1:15:08, 16.82s/it]
                                                       

 87%|████████▋ | 1732/2000 [8:08:25<1:15:08, 16.82s/it]
 87%|████████▋ | 1733/2000 [8:08:42<1:14:52, 16.82s/it]
                                                       

 87%|████████▋ | 1733/2000 [8:08:42<1:14:52, 16.82s/it]
 87%|████████▋ | 1734/2000 [8:08:59<1:14:43, 16.85s/it]
                                                       

 87%|████████▋ | 1734/2000 [8:08:59<1:14:43, 16.85s/it]
 87%|████████▋ | 1735/2000 [8:09:16<1:14:36, 16.89s/it]
                                                       

 87%|████████▋ | 1735/2000 [8:09:16<1:14:36, 16.89s/it]
 87%|████████▋ | 1736/2000 [8:09:33<1:14:12, 16.87s/it]
                                                       

 87%|████████▋ | 1736/2000 [8:09:33<1:14:12, 16.87s/it]
 87%|████████▋ | 1737/2000 [8:09:50<1:13:53, 16.86s/it]
                                                       

 87%|████████▋ | 1737/2000 [8:09:50<1:13:53, 16.86s/it]
 87%|████████▋ | 1738/2000 [8:10:06<1:13:41, 16.88s/it]
                                                       

 87%|████████▋ | 1738/2000 [8:10:06<1:13:41, 16.88s/it]
 87%|████████▋ | 1739/2000 [8:10:23<1:13:22, 16.87s/it]
                                                       

 87%|████████▋ | 1739/2000 [8:10:23<1:13:22, 16.87s/it]
 87%|████████▋ | 1740/2000 [8:10:40<1:13:01, 16.85s/it]
                                                       

 87%|████████▋ | 1740/2000 [8:10:40<1:13:01, 16.85s/it]
 87%|████████▋ | 1741/2000 [8:10:57<1:12:40, 16.84s/it]
                                                       

 87%|████████▋ | 1741/2000 [8:10:57<1:12:40, 16.84s/it]
 87%|████████▋ | 1742/2000 [8:11:14<1:12:18, 16.82s/it]
                                                       

 87%|████████▋ | 1742/2000 [8:11:14<1:12:18, 16.82s/it]
 87%|████████▋ | 1743/2000 [8:11:30<1:11:57, 16.80s/it]
                                                       

 87%|████████▋ | 1743/2000 [8:11:30<1:11:57, 16.80s/it]
 87%|████████▋ | 1744/2000 [8:11:47<1:11:39, 16.80s/it]
                                                       

 87%|████████▋ | 1744/2000 [8:11:47<1:11:39, 16.80s/it]
 87%|████████▋ | 1745/2000 [8:12:04<1:11:25, 16.80s/it]
                                                       

 87%|████████▋ | 1745/2000 [8:12:04<1:11:25, 16.80s/it]
 87%|████████▋ | 1746/2000 [8:12:21<1:11:06, 16.80s/it]
                                                       

 87%|████████▋ | 1746/2000 [8:12:21<1:11:06, 16.80s/it]
 87%|████████▋ | 1747/2000 [8:12:38<1:10:50, 16.80s/it]
                                                       

 87%|████████▋ | 1747/2000 [8:12:38<1:10:50, 16.80s/it]
 87%|████████▋ | 1748/2000 [8:12:54<1:10:32, 16.79s/it]
                                                       

 87%|████████▋ | 1748/2000 [8:12:54<1:10:32, 16.79s/it]
 87%|████████▋ | 1749/2000 [8:13:11<1:10:19, 16.81s/it]
                                                       

 87%|████████▋ | 1749/2000 [8:13:11<1:10:19, 16.81s/it]
 88%|████████▊ | 1750/2000 [8:13:28<1:09:59, 16.80s/it]
                                                       

 88%|████████▊ | 1750/2000 [8:13:28<1:09:59, 16.80s/it]
 88%|████████▊ | 1751/2000 [8:13:45<1:09:46, 16.81s/it]
                                                       

 88%|████████▊ | 1751/2000 [8:13:45<1:09:46, 16.81s/it]
 88%|████████▊ | 1752/2000 [8:14:02<1:09:26, 16.80s/it]
                                                       

 88%|████████▊ | 1752/2000 [8:14:02<1:09:26, 16.80s/it]
 88%|████████▊ | 1753/2000 [8:14:18<1:09:11, 16.81s/it]
                                                       

 88%|████████▊ | 1753/2000 [8:14:18<1:09:11, 16.81s/it]
 88%|████████▊ | 1754/2000 [8:14:35<1:08:51, 16.80s/it]
                                                       

 88%|████████▊ | 1754/2000 [8:14:35<1:08:51, 16.80s/it]
 88%|████████▊ | 1755/2000 [8:14:52<1:08:32, 16.78s/it]
                                                       

 88%|████████▊ | 1755/2000 [8:14:52<1:08:32, 16.78s/it]
 88%|████████▊ | 1756/2000 [8:15:09<1:08:17, 16.79s/it]
                                                       

 88%|████████▊ | 1756/2000 [8:15:09<1:08:17, 16.79s/it]
 88%|████████▊ | 1757/2000 [8:15:26<1:08:01, 16.80s/it]
                                                       

 88%|████████▊ | 1757/2000 [8:15:26<1:08:01, 16.80s/it]
 88%|████████▊ | 1758/2000 [8:15:42<1:07:46, 16.80s/it]
                                                       

 88%|████████▊ | 1758/2000 [8:15:42<1:07:46, 16.80s/it]
 88%|████████▊ | 1759/2000 [8:15:59<1:07:29, 16.80s/it]
                                                       

 88%|████████▊ | 1759/2000 [8:15:59<1:07:29, 16.80s/it]
 88%|████████▊ | 1760/2000 [8:16:16<1:07:19, 16.83s/it]
                                                       

 88%|████████▊ | 1760/2000 [8:16:16<1:07:19, 16.83s/it]
 88%|████████▊ | 1761/2000 [8:16:36<1:10:07, 17.61s/it]
                                                       

 88%|████████▊ | 1761/2000 [8:16:36<1:10:07, 17.61s/it]
 88%|████████▊ | 1762/2000 [8:16:53<1:09:30, 17.52s/it]
                                                       

 88%|████████▊ | 1762/2000 [8:16:53<1:09:30, 17.52s/it]
 88%|████████▊ | 1763/2000 [8:17:10<1:08:16, 17.28s/it]
                                                       

 88%|████████▊ | 1763/2000 [8:17:10<1:08:16, 17.28s/it]
 88%|████████▊ | 1764/2000 [8:17:27<1:08:27, 17.41s/it]
                                                       

 88%|████████▊ | 1764/2000 [8:17:27<1:08:27, 17.41s/it]
 88%|████████▊ | 1765/2000 [8:17:44<1:07:27, 17.22s/it]
                                                       

 88%|████████▊ | 1765/2000 [8:17:44<1:07:27, 17.22s/it]
 88%|████████▊ | 1766/2000 [8:18:01<1:06:50, 17.14s/it]
                                                       

 88%|████████▊ | 1766/2000 [8:18:01<1:06:50, 17.14s/it]
 88%|████████▊ | 1767/2000 [8:18:18<1:06:09, 17.04s/it]
                                                       

 88%|████████▊ | 1767/2000 [8:18:18<1:06:09, 17.04s/it]
 88%|████████▊ | 1768/2000 [8:18:35<1:05:40, 16.99s/it]
                                                       

 88%|████████▊ | 1768/2000 [8:18:35<1:05:40, 16.99s/it]
 88%|████████▊ | 1769/2000 [8:18:52<1:05:15, 16.95s/it]
                                                       

 88%|████████▊ | 1769/2000 [8:18:52<1:05:15, 16.95s/it]
 88%|████████▊ | 1770/2000 [8:19:08<1:04:54, 16.93s/it]
                                                       

 88%|████████▊ | 1770/2000 [8:19:08<1:04:54, 16.93s/it]
 89%|████████▊ | 1771/2000 [8:19:25<1:04:29, 16.90s/it]
                                                       

 89%|████████▊ | 1771/2000 [8:19:25<1:04:29, 16.90s/it]
 89%|████████▊ | 1772/2000 [8:19:42<1:04:11, 16.89s/it]
                                                       

 89%|████████▊ | 1772/2000 [8:19:42<1:04:11, 16.89s/it]
 89%|████████▊ | 1773/2000 [8:19:59<1:03:49, 16.87s/it]
                                                       

 89%|████████▊ | 1773/2000 [8:19:59<1:03:49, 16.87s/it]
 89%|████████▊ | 1774/2000 [8:20:16<1:03:31, 16.87s/it]
                                                       

 89%|████████▊ | 1774/2000 [8:20:16<1:03:31, 16.87s/it]
 89%|████████▉ | 1775/2000 [8:20:33<1:03:11, 16.85s/it]
                                                       

 89%|████████▉ | 1775/2000 [8:20:33<1:03:11, 16.85s/it]
 89%|████████▉ | 1776/2000 [8:20:49<1:02:48, 16.82s/it]
                                                       

 89%|████████▉ | 1776/2000 [8:20:49<1:02:48, 16.82s/it]
 89%|████████▉ | 1777/2000 [8:21:06<1:02:31, 16.82s/it]
                                                       

 89%|████████▉ | 1777/2000 [8:21:06<1:02:31, 16.82s/it]
 89%|████████▉ | 1778/2000 [8:21:23<1:02:12, 16.81s/it]
                                                       

 89%|████████▉ | 1778/2000 [8:21:23<1:02:12, 16.81s/it]
 89%|████████▉ | 1779/2000 [8:21:40<1:01:56, 16.82s/it]
                                                       

 89%|████████▉ | 1779/2000 [8:21:40<1:01:56, 16.82s/it]
 89%|████████▉ | 1780/2000 [8:21:57<1:01:39, 16.82s/it]
                                                       

 89%|████████▉ | 1780/2000 [8:21:57<1:01:39, 16.82s/it]
 89%|████████▉ | 1781/2000 [8:22:13<1:01:23, 16.82s/it]
                                                       

 89%|████████▉ | 1781/2000 [8:22:13<1:01:23, 16.82s/it]
 89%|████████▉ | 1782/2000 [8:22:30<1:01:07, 16.82s/it]
                                                       

 89%|████████▉ | 1782/2000 [8:22:30<1:01:07, 16.82s/it]
 89%|████████▉ | 1783/2000 [8:22:47<1:00:49, 16.82s/it]
                                                       

 89%|████████▉ | 1783/2000 [8:22:47<1:00:49, 16.82s/it]
 89%|████████▉ | 1784/2000 [8:23:04<1:00:30, 16.81s/it]
                                                       

 89%|████████▉ | 1784/2000 [8:23:04<1:00:30, 16.81s/it]
 89%|████████▉ | 1785/2000 [8:23:21<1:00:15, 16.81s/it]
                                                       

 89%|████████▉ | 1785/2000 [8:23:21<1:00:15, 16.81s/it]
 89%|████████▉ | 1786/2000 [8:23:38<59:59, 16.82s/it]  
                                                     

 89%|████████▉ | 1786/2000 [8:23:38<59:59, 16.82s/it]
 89%|████████▉ | 1787/2000 [8:23:54<59:43, 16.82s/it]
                                                     

 89%|████████▉ | 1787/2000 [8:23:54<59:43, 16.82s/it]
 89%|████████▉ | 1788/2000 [8:24:11<59:26, 16.82s/it]
                                                     

 89%|████████▉ | 1788/2000 [8:24:11<59:26, 16.82s/it]
 89%|████████▉ | 1789/2000 [8:24:28<59:11, 16.83s/it]
                                                     

 89%|████████▉ | 1789/2000 [8:24:28<59:11, 16.83s/it]
 90%|████████▉ | 1790/2000 [8:24:45<58:52, 16.82s/it]
                                                     

 90%|████████▉ | 1790/2000 [8:24:45<58:52, 16.82s/it]
 90%|████████▉ | 1791/2000 [8:25:02<58:31, 16.80s/it]
                                                     

 90%|████████▉ | 1791/2000 [8:25:02<58:31, 16.80s/it]
 90%|████████▉ | 1792/2000 [8:25:18<58:13, 16.80s/it]
                                                     

 90%|████████▉ | 1792/2000 [8:25:18<58:13, 16.80s/it]
 90%|████████▉ | 1793/2000 [8:25:35<57:57, 16.80s/it]
                                                     

 90%|████████▉ | 1793/2000 [8:25:35<57:57, 16.80s/it]
 90%|████████▉ | 1794/2000 [8:25:52<57:40, 16.80s/it]
                                                     

 90%|████████▉ | 1794/2000 [8:25:52<57:40, 16.80s/it]
 90%|████████▉ | 1795/2000 [8:26:09<57:28, 16.82s/it]
                                                     

 90%|████████▉ | 1795/2000 [8:26:09<57:28, 16.82s/it]
 90%|████████▉ | 1796/2000 [8:26:26<57:04, 16.79s/it]
                                                     

 90%|████████▉ | 1796/2000 [8:26:26<57:04, 16.79s/it]
 90%|████████▉ | 1797/2000 [8:26:43<57:21, 16.95s/it]
                                                     

 90%|████████▉ | 1797/2000 [8:26:43<57:21, 16.95s/it]
 90%|████████▉ | 1798/2000 [8:27:00<56:49, 16.88s/it]
                                                     

 90%|████████▉ | 1798/2000 [8:27:00<56:49, 16.88s/it]
 90%|████████▉ | 1799/2000 [8:27:17<56:38, 16.91s/it]
                                                     

 90%|████████▉ | 1799/2000 [8:27:17<56:38, 16.91s/it]
 90%|█████████ | 1800/2000 [8:27:34<56:24, 16.92s/it]
                                                     

 90%|█████████ | 1800/2000 [8:27:34<56:24, 16.92s/it]
 90%|█████████ | 1801/2000 [8:27:50<56:01, 16.89s/it]
                                                     

 90%|█████████ | 1801/2000 [8:27:50<56:01, 16.89s/it]
 90%|█████████ | 1802/2000 [8:28:07<55:39, 16.87s/it]
                                                     

 90%|█████████ | 1802/2000 [8:28:07<55:39, 16.87s/it]
 90%|█████████ | 1803/2000 [8:28:24<55:24, 16.87s/it]
                                                     

 90%|█████████ | 1803/2000 [8:28:24<55:24, 16.87s/it]
 90%|█████████ | 1804/2000 [8:28:41<55:09, 16.88s/it]
                                                     

 90%|█████████ | 1804/2000 [8:28:41<55:09, 16.88s/it]
 90%|█████████ | 1805/2000 [8:28:58<54:50, 16.88s/it]
                                                     

 90%|█████████ | 1805/2000 [8:28:58<54:50, 16.88s/it]
 90%|█████████ | 1806/2000 [8:29:15<54:33, 16.87s/it]
                                                     

 90%|█████████ | 1806/2000 [8:29:15<54:33, 16.87s/it]
 90%|█████████ | 1807/2000 [8:29:32<54:19, 16.89s/it]
                                                     

 90%|█████████ | 1807/2000 [8:29:32<54:19, 16.89s/it]
 90%|█████████ | 1808/2000 [8:29:48<53:55, 16.85s/it]
                                                     

 90%|█████████ | 1808/2000 [8:29:48<53:55, 16.85s/it]
 90%|█████████ | 1809/2000 [8:30:05<53:36, 16.84s/it]
                                                     

 90%|█████████ | 1809/2000 [8:30:05<53:36, 16.84s/it]
 90%|█████████ | 1810/2000 [8:30:22<53:19, 16.84s/it]
                                                     

 90%|█████████ | 1810/2000 [8:30:22<53:19, 16.84s/it]
 91%|█████████ | 1811/2000 [8:30:39<53:05, 16.86s/it]
                                                     

 91%|█████████ | 1811/2000 [8:30:39<53:05, 16.86s/it]
 91%|█████████ | 1812/2000 [8:30:56<52:48, 16.85s/it]
                                                     

 91%|█████████ | 1812/2000 [8:30:56<52:48, 16.85s/it]
 91%|█████████ | 1813/2000 [8:31:13<52:29, 16.84s/it]
                                                     

 91%|█████████ | 1813/2000 [8:31:13<52:29, 16.84s/it]
 91%|█████████ | 1814/2000 [8:31:29<52:13, 16.85s/it]
                                                     

 91%|█████████ | 1814/2000 [8:31:29<52:13, 16.85s/it]
 91%|█████████ | 1815/2000 [8:31:46<51:56, 16.85s/it]
                                                     

 91%|█████████ | 1815/2000 [8:31:46<51:56, 16.85s/it]
 91%|█████████ | 1816/2000 [8:32:03<51:39, 16.84s/it]
                                                     

 91%|█████████ | 1816/2000 [8:32:03<51:39, 16.84s/it]
 91%|█████████ | 1817/2000 [8:32:20<51:20, 16.83s/it]
                                                     

 91%|█████████ | 1817/2000 [8:32:20<51:20, 16.83s/it]
 91%|█████████ | 1818/2000 [8:32:37<51:01, 16.82s/it]
                                                     

 91%|█████████ | 1818/2000 [8:32:37<51:01, 16.82s/it]
 91%|█████████ | 1819/2000 [8:32:54<50:44, 16.82s/it]
                                                     

 91%|█████████ | 1819/2000 [8:32:54<50:44, 16.82s/it]
 91%|█████████ | 1820/2000 [8:33:10<50:32, 16.84s/it]
                                                     

 91%|█████████ | 1820/2000 [8:33:10<50:32, 16.84s/it]
 91%|█████████ | 1821/2000 [8:33:27<50:20, 16.88s/it]
                                                     

 91%|█████████ | 1821/2000 [8:33:27<50:20, 16.88s/it]
 91%|█████████ | 1822/2000 [8:33:44<49:59, 16.85s/it]
                                                     

 91%|█████████ | 1822/2000 [8:33:44<49:59, 16.85s/it]
 91%|█████████ | 1823/2000 [8:34:01<49:44, 16.86s/it]
                                                     

 91%|█████████ | 1823/2000 [8:34:01<49:44, 16.86s/it]
 91%|█████████ | 1824/2000 [8:34:18<49:29, 16.87s/it]
                                                     

 91%|█████████ | 1824/2000 [8:34:18<49:29, 16.87s/it]
 91%|█████████▏| 1825/2000 [8:34:35<49:07, 16.84s/it]
                                                     

 91%|█████████▏| 1825/2000 [8:34:35<49:07, 16.84s/it]
 91%|█████████▏| 1826/2000 [8:34:52<48:47, 16.83s/it]
                                                     

 91%|█████████▏| 1826/2000 [8:34:52<48:47, 16.83s/it]
 91%|█████████▏| 1827/2000 [8:35:08<48:33, 16.84s/it]
                                                     

 91%|█████████▏| 1827/2000 [8:35:08<48:33, 16.84s/it]
 91%|█████████▏| 1828/2000 [8:35:25<48:14, 16.83s/it]
                                                     

 91%|█████████▏| 1828/2000 [8:35:25<48:14, 16.83s/it]
 91%|█████████▏| 1829/2000 [8:35:42<48:00, 16.85s/it]
                                                     

 91%|█████████▏| 1829/2000 [8:35:42<48:00, 16.85s/it]
 92%|█████████▏| 1830/2000 [8:35:59<47:47, 16.87s/it]
                                                     

 92%|█████████▏| 1830/2000 [8:35:59<47:47, 16.87s/it]
 92%|█████████▏| 1831/2000 [8:36:16<47:28, 16.85s/it]
                                                     

 92%|█████████▏| 1831/2000 [8:36:16<47:28, 16.85s/it]
 92%|█████████▏| 1832/2000 [8:36:33<47:06, 16.83s/it]
                                                     

 92%|█████████▏| 1832/2000 [8:36:33<47:06, 16.83s/it]
 92%|█████████▏| 1833/2000 [8:36:50<46:53, 16.84s/it]
                                                     

 92%|█████████▏| 1833/2000 [8:36:50<46:53, 16.84s/it]
 92%|█████████▏| 1834/2000 [8:37:06<46:39, 16.86s/it]
                                                     

 92%|█████████▏| 1834/2000 [8:37:06<46:39, 16.86s/it]
 92%|█████████▏| 1835/2000 [8:37:23<46:20, 16.85s/it]
                                                     

 92%|█████████▏| 1835/2000 [8:37:23<46:20, 16.85s/it]
 92%|█████████▏| 1836/2000 [8:37:40<46:00, 16.83s/it]
                                                     

 92%|█████████▏| 1836/2000 [8:37:40<46:00, 16.83s/it]
 92%|█████████▏| 1837/2000 [8:37:57<45:44, 16.84s/it]
                                                     

 92%|█████████▏| 1837/2000 [8:37:57<45:44, 16.84s/it]
 92%|█████████▏| 1838/2000 [8:38:14<45:31, 16.86s/it]
                                                     

 92%|█████████▏| 1838/2000 [8:38:14<45:31, 16.86s/it]
 92%|█████████▏| 1839/2000 [8:38:31<45:11, 16.84s/it]
                                                     

 92%|█████████▏| 1839/2000 [8:38:31<45:11, 16.84s/it]
 92%|█████████▏| 1840/2000 [8:38:47<44:57, 16.86s/it]
                                                     

 92%|█████████▏| 1840/2000 [8:38:47<44:57, 16.86s/it]
 92%|█████████▏| 1841/2000 [8:39:07<46:50, 17.68s/it]
                                                     

 92%|█████████▏| 1841/2000 [8:39:07<46:50, 17.68s/it]
 92%|█████████▏| 1842/2000 [8:39:24<45:57, 17.45s/it]
                                                     

 92%|█████████▏| 1842/2000 [8:39:24<45:57, 17.45s/it]
 92%|█████████▏| 1843/2000 [8:39:41<45:21, 17.33s/it]
                                                     

 92%|█████████▏| 1843/2000 [8:39:41<45:21, 17.33s/it]
 92%|█████████▏| 1844/2000 [8:39:58<44:37, 17.16s/it]
                                                     

 92%|█████████▏| 1844/2000 [8:39:58<44:37, 17.16s/it]
 92%|█████████▏| 1845/2000 [8:40:15<44:06, 17.08s/it]
                                                     

 92%|█████████▏| 1845/2000 [8:40:15<44:06, 17.08s/it]
 92%|█████████▏| 1846/2000 [8:40:32<43:37, 17.00s/it]
                                                     

 92%|█████████▏| 1846/2000 [8:40:32<43:37, 17.00s/it]
 92%|█████████▏| 1847/2000 [8:40:48<43:17, 16.98s/it]
                                                     

 92%|█████████▏| 1847/2000 [8:40:48<43:17, 16.98s/it]
 92%|█████████▏| 1848/2000 [8:41:05<42:57, 16.96s/it]
                                                     

 92%|█████████▏| 1848/2000 [8:41:05<42:57, 16.96s/it]
 92%|█████████▏| 1849/2000 [8:41:22<42:36, 16.93s/it]
                                                     

 92%|█████████▏| 1849/2000 [8:41:22<42:36, 16.93s/it]
 92%|█████████▎| 1850/2000 [8:41:39<42:19, 16.93s/it]
                                                     

 92%|█████████▎| 1850/2000 [8:41:39<42:19, 16.93s/it]
 93%|█████████▎| 1851/2000 [8:41:56<42:02, 16.93s/it]
                                                     

 93%|█████████▎| 1851/2000 [8:41:56<42:02, 16.93s/it]
 93%|█████████▎| 1852/2000 [8:42:13<41:39, 16.89s/it]
                                                     

 93%|█████████▎| 1852/2000 [8:42:13<41:39, 16.89s/it]
 93%|█████████▎| 1853/2000 [8:42:30<41:20, 16.87s/it]
                                                     

 93%|█████████▎| 1853/2000 [8:42:30<41:20, 16.87s/it]
 93%|█████████▎| 1854/2000 [8:42:47<41:04, 16.88s/it]
                                                     

 93%|█████████▎| 1854/2000 [8:42:47<41:04, 16.88s/it]
 93%|█████████▎| 1855/2000 [8:43:04<40:51, 16.91s/it]
                                                     

 93%|█████████▎| 1855/2000 [8:43:04<40:51, 16.91s/it]
 93%|█████████▎| 1856/2000 [8:43:20<40:31, 16.89s/it]
                                                     

 93%|█████████▎| 1856/2000 [8:43:20<40:31, 16.89s/it]
 93%|█████████▎| 1857/2000 [8:43:37<40:11, 16.86s/it]
                                                     

 93%|█████████▎| 1857/2000 [8:43:37<40:11, 16.86s/it]
 93%|█████████▎| 1858/2000 [8:43:54<39:55, 16.87s/it]
                                                     

 93%|█████████▎| 1858/2000 [8:43:54<39:55, 16.87s/it]
 93%|█████████▎| 1859/2000 [8:44:11<39:43, 16.90s/it]
                                                     

 93%|█████████▎| 1859/2000 [8:44:11<39:43, 16.90s/it]
 93%|█████████▎| 1860/2000 [8:44:28<39:25, 16.90s/it]
                                                     

 93%|█████████▎| 1860/2000 [8:44:28<39:25, 16.90s/it]
 93%|█████████▎| 1861/2000 [8:44:45<39:04, 16.86s/it]
                                                     

 93%|█████████▎| 1861/2000 [8:44:45<39:04, 16.86s/it]
 93%|█████████▎| 1862/2000 [8:45:02<38:44, 16.84s/it]
                                                     

 93%|█████████▎| 1862/2000 [8:45:02<38:44, 16.84s/it]
 93%|█████████▎| 1863/2000 [8:45:18<38:24, 16.82s/it]
                                                     

 93%|█████████▎| 1863/2000 [8:45:18<38:24, 16.82s/it]
 93%|█████████▎| 1864/2000 [8:45:35<38:07, 16.82s/it]
                                                     

 93%|█████████▎| 1864/2000 [8:45:35<38:07, 16.82s/it]
 93%|█████████▎| 1865/2000 [8:45:52<37:47, 16.80s/it]
                                                     

 93%|█████████▎| 1865/2000 [8:45:52<37:47, 16.80s/it]
 93%|█████████▎| 1866/2000 [8:46:09<37:33, 16.82s/it]
                                                     

 93%|█████████▎| 1866/2000 [8:46:09<37:33, 16.82s/it]
 93%|█████████▎| 1867/2000 [8:46:26<37:17, 16.82s/it]
                                                     

 93%|█████████▎| 1867/2000 [8:46:26<37:17, 16.82s/it]
 93%|█████████▎| 1868/2000 [8:46:42<36:58, 16.81s/it]
                                                     

 93%|█████████▎| 1868/2000 [8:46:42<36:58, 16.81s/it]
 93%|█████████▎| 1869/2000 [8:46:59<36:40, 16.80s/it]
                                                     

 93%|█████████▎| 1869/2000 [8:46:59<36:40, 16.80s/it]
 94%|█████████▎| 1870/2000 [8:47:16<36:23, 16.80s/it]
                                                     

 94%|█████████▎| 1870/2000 [8:47:16<36:23, 16.80s/it]
 94%|█████████▎| 1871/2000 [8:47:33<36:06, 16.79s/it]
                                                     

 94%|█████████▎| 1871/2000 [8:47:33<36:06, 16.79s/it]
 94%|█████████▎| 1872/2000 [8:47:49<35:48, 16.79s/it]
                                                     

 94%|█████████▎| 1872/2000 [8:47:49<35:48, 16.79s/it]
 94%|█████████▎| 1873/2000 [8:48:06<35:33, 16.80s/it]
                                                     

 94%|█████████▎| 1873/2000 [8:48:06<35:33, 16.80s/it]
 94%|█████████▎| 1874/2000 [8:48:23<35:17, 16.80s/it]
                                                     

 94%|█████████▎| 1874/2000 [8:48:23<35:17, 16.80s/it]
 94%|█████████▍| 1875/2000 [8:48:40<34:59, 16.80s/it]
                                                     

 94%|█████████▍| 1875/2000 [8:48:40<34:59, 16.80s/it]
 94%|█████████▍| 1876/2000 [8:48:57<34:41, 16.78s/it]
                                                     

 94%|█████████▍| 1876/2000 [8:48:57<34:41, 16.78s/it]
 94%|█████████▍| 1877/2000 [8:49:13<34:25, 16.79s/it]
                                                     

 94%|█████████▍| 1877/2000 [8:49:13<34:25, 16.79s/it]
 94%|█████████▍| 1878/2000 [8:49:30<34:09, 16.80s/it]
                                                     

 94%|█████████▍| 1878/2000 [8:49:30<34:09, 16.80s/it]
 94%|█████████▍| 1879/2000 [8:49:47<33:53, 16.81s/it]
                                                     

 94%|█████████▍| 1879/2000 [8:49:47<33:53, 16.81s/it]
 94%|█████████▍| 1880/2000 [8:50:04<33:42, 16.85s/it]
                                                     

 94%|█████████▍| 1880/2000 [8:50:04<33:42, 16.85s/it]
 94%|█████████▍| 1881/2000 [8:50:21<33:23, 16.84s/it]
                                                     

 94%|█████████▍| 1881/2000 [8:50:21<33:23, 16.84s/it]
 94%|█████████▍| 1882/2000 [8:50:38<33:08, 16.85s/it]
                                                     

 94%|█████████▍| 1882/2000 [8:50:38<33:08, 16.85s/it]
 94%|█████████▍| 1883/2000 [8:50:55<32:54, 16.88s/it]
                                                     

 94%|█████████▍| 1883/2000 [8:50:55<32:54, 16.88s/it]
 94%|█████████▍| 1884/2000 [8:51:11<32:32, 16.83s/it]
                                                     

 94%|█████████▍| 1884/2000 [8:51:11<32:32, 16.83s/it]
 94%|█████████▍| 1885/2000 [8:51:28<32:17, 16.85s/it]
                                                     

 94%|█████████▍| 1885/2000 [8:51:28<32:17, 16.85s/it]
 94%|█████████▍| 1886/2000 [8:51:45<32:04, 16.88s/it]
                                                     

 94%|█████████▍| 1886/2000 [8:51:45<32:04, 16.88s/it]
 94%|█████████▍| 1887/2000 [8:52:02<31:47, 16.88s/it]
                                                     

 94%|█████████▍| 1887/2000 [8:52:02<31:47, 16.88s/it]
 94%|█████████▍| 1888/2000 [8:52:19<31:28, 16.86s/it]
                                                     

 94%|█████████▍| 1888/2000 [8:52:19<31:28, 16.86s/it]
 94%|█████████▍| 1889/2000 [8:52:36<31:08, 16.83s/it]
                                                     

 94%|█████████▍| 1889/2000 [8:52:36<31:08, 16.83s/it]
 94%|█████████▍| 1890/2000 [8:52:53<30:50, 16.82s/it]
                                                     

 94%|█████████▍| 1890/2000 [8:52:53<30:50, 16.82s/it]
 95%|█████████▍| 1891/2000 [8:53:10<30:40, 16.89s/it]
                                                     

 95%|█████████▍| 1891/2000 [8:53:10<30:40, 16.89s/it]
 95%|█████████▍| 1892/2000 [8:53:26<30:19, 16.84s/it]
                                                     

 95%|█████████▍| 1892/2000 [8:53:26<30:19, 16.84s/it]
 95%|█████████▍| 1893/2000 [8:53:43<30:02, 16.85s/it]
                                                     

 95%|█████████▍| 1893/2000 [8:53:43<30:02, 16.85s/it]
 95%|█████████▍| 1894/2000 [8:54:00<29:43, 16.83s/it]
                                                     

 95%|█████████▍| 1894/2000 [8:54:00<29:43, 16.83s/it]
 95%|█████████▍| 1895/2000 [8:54:17<29:24, 16.81s/it]
                                                     

 95%|█████████▍| 1895/2000 [8:54:17<29:24, 16.81s/it]
 95%|█████████▍| 1896/2000 [8:54:34<29:08, 16.81s/it]
                                                     

 95%|█████████▍| 1896/2000 [8:54:34<29:08, 16.81s/it]
 95%|█████████▍| 1897/2000 [8:54:50<28:50, 16.80s/it]
                                                     

 95%|█████████▍| 1897/2000 [8:54:50<28:50, 16.80s/it]
 95%|█████████▍| 1898/2000 [8:55:07<28:34, 16.80s/it]
                                                     

 95%|█████████▍| 1898/2000 [8:55:07<28:34, 16.80s/it]
 95%|█████████▍| 1899/2000 [8:55:24<28:18, 16.82s/it]
                                                     

 95%|█████████▍| 1899/2000 [8:55:24<28:18, 16.82s/it]
 95%|█████████▌| 1900/2000 [8:55:41<28:00, 16.80s/it]
                                                     

 95%|█████████▌| 1900/2000 [8:55:41<28:00, 16.80s/it]
 95%|█████████▌| 1901/2000 [8:55:58<27:44, 16.81s/it]
                                                     

 95%|█████████▌| 1901/2000 [8:55:58<27:44, 16.81s/it]
 95%|█████████▌| 1902/2000 [8:56:14<27:28, 16.82s/it]
                                                     

 95%|█████████▌| 1902/2000 [8:56:14<27:28, 16.82s/it]
 95%|█████████▌| 1903/2000 [8:56:31<27:11, 16.82s/it]
                                                     

 95%|█████████▌| 1903/2000 [8:56:31<27:11, 16.82s/it]
 95%|█████████▌| 1904/2000 [8:56:48<26:53, 16.81s/it]
                                                     

 95%|█████████▌| 1904/2000 [8:56:48<26:53, 16.81s/it]
 95%|█████████▌| 1905/2000 [8:57:05<26:35, 16.80s/it]
                                                     

 95%|█████████▌| 1905/2000 [8:57:05<26:35, 16.80s/it]
 95%|█████████▌| 1906/2000 [8:57:22<26:18, 16.80s/it]
                                                     

 95%|█████████▌| 1906/2000 [8:57:22<26:18, 16.80s/it]
 95%|█████████▌| 1907/2000 [8:57:38<26:02, 16.80s/it]
                                                     

 95%|█████████▌| 1907/2000 [8:57:38<26:02, 16.80s/it]
 95%|█████████▌| 1908/2000 [8:57:55<25:47, 16.82s/it]
                                                     

 95%|█████████▌| 1908/2000 [8:57:55<25:47, 16.82s/it]
 95%|█████████▌| 1909/2000 [8:58:12<25:34, 16.86s/it]
                                                     

 95%|█████████▌| 1909/2000 [8:58:12<25:34, 16.86s/it]
 96%|█████████▌| 1910/2000 [8:58:29<25:17, 16.86s/it]
                                                     

 96%|█████████▌| 1910/2000 [8:58:29<25:17, 16.86s/it]
 96%|█████████▌| 1911/2000 [8:58:46<24:59, 16.84s/it]
                                                     

 96%|█████████▌| 1911/2000 [8:58:46<24:59, 16.84s/it]
 96%|█████████▌| 1912/2000 [8:59:03<24:43, 16.85s/it]
                                                     

 96%|█████████▌| 1912/2000 [8:59:03<24:43, 16.85s/it]
 96%|█████████▌| 1913/2000 [8:59:20<24:26, 16.85s/it]
                                                     

 96%|█████████▌| 1913/2000 [8:59:20<24:26, 16.85s/it]
 96%|█████████▌| 1914/2000 [8:59:36<24:08, 16.84s/it]
                                                     

 96%|█████████▌| 1914/2000 [8:59:36<24:08, 16.84s/it]
 96%|█████████▌| 1915/2000 [8:59:53<23:49, 16.82s/it]
                                                     

 96%|█████████▌| 1915/2000 [8:59:53<23:49, 16.82s/it]
 96%|█████████▌| 1916/2000 [9:00:10<23:32, 16.82s/it]
                                                     

 96%|█████████▌| 1916/2000 [9:00:10<23:32, 16.82s/it]
 96%|█████████▌| 1917/2000 [9:00:27<23:15, 16.81s/it]
                                                     

 96%|█████████▌| 1917/2000 [9:00:27<23:15, 16.81s/it]
 96%|█████████▌| 1918/2000 [9:00:44<23:00, 16.84s/it]
                                                     

 96%|█████████▌| 1918/2000 [9:00:44<23:00, 16.84s/it]
 96%|█████████▌| 1919/2000 [9:01:00<22:42, 16.83s/it]
                                                     

 96%|█████████▌| 1919/2000 [9:01:00<22:42, 16.83s/it]
 96%|█████████▌| 1920/2000 [9:01:17<22:27, 16.84s/it]
                                                     

 96%|█████████▌| 1920/2000 [9:01:17<22:27, 16.84s/it]
 96%|█████████▌| 1921/2000 [9:01:37<23:09, 17.59s/it]
                                                     

 96%|█████████▌| 1921/2000 [9:01:37<23:09, 17.59s/it]
 96%|█████████▌| 1922/2000 [9:01:54<22:40, 17.44s/it]
                                                     

 96%|█████████▌| 1922/2000 [9:01:54<22:40, 17.44s/it]
 96%|█████████▌| 1923/2000 [9:02:11<22:12, 17.30s/it]
                                                     

 96%|█████████▌| 1923/2000 [9:02:11<22:12, 17.30s/it]
 96%|█████████▌| 1924/2000 [9:02:28<21:45, 17.18s/it]
                                                     

 96%|█████████▌| 1924/2000 [9:02:28<21:45, 17.18s/it]
 96%|█████████▋| 1925/2000 [9:02:44<21:19, 17.07s/it]
                                                     

 96%|█████████▋| 1925/2000 [9:02:44<21:19, 17.07s/it]
 96%|█████████▋| 1926/2000 [9:03:02<21:05, 17.09s/it]
                                                     

 96%|█████████▋| 1926/2000 [9:03:02<21:05, 17.09s/it]
 96%|█████████▋| 1927/2000 [9:03:19<20:43, 17.03s/it]
                                                     

 96%|█████████▋| 1927/2000 [9:03:19<20:43, 17.03s/it]
 96%|█████████▋| 1928/2000 [9:03:35<20:23, 17.00s/it]
                                                     

 96%|█████████▋| 1928/2000 [9:03:35<20:23, 17.00s/it]
 96%|█████████▋| 1929/2000 [9:03:52<20:03, 16.95s/it]
                                                     

 96%|█████████▋| 1929/2000 [9:03:52<20:03, 16.95s/it]
 96%|█████████▋| 1930/2000 [9:04:09<19:43, 16.90s/it]
                                                     

 96%|█████████▋| 1930/2000 [9:04:09<19:43, 16.90s/it]
 97%|█████████▋| 1931/2000 [9:04:26<19:25, 16.89s/it]
                                                     

 97%|█████████▋| 1931/2000 [9:04:26<19:25, 16.89s/it]
 97%|█████████▋| 1932/2000 [9:04:43<19:09, 16.91s/it]
                                                     

 97%|█████████▋| 1932/2000 [9:04:43<19:09, 16.91s/it]
 97%|█████████▋| 1933/2000 [9:05:00<18:51, 16.89s/it]
                                                     

 97%|█████████▋| 1933/2000 [9:05:00<18:51, 16.89s/it]
 97%|█████████▋| 1934/2000 [9:05:16<18:32, 16.85s/it]
                                                     

 97%|█████████▋| 1934/2000 [9:05:16<18:32, 16.85s/it]
 97%|█████████▋| 1935/2000 [9:05:33<18:15, 16.85s/it]
                                                     

 97%|█████████▋| 1935/2000 [9:05:33<18:15, 16.85s/it]
 97%|█████████▋| 1936/2000 [9:05:50<17:59, 16.86s/it]
                                                     

 97%|█████████▋| 1936/2000 [9:05:50<17:59, 16.86s/it]
 97%|█████████▋| 1937/2000 [9:06:07<17:41, 16.85s/it]
                                                     

 97%|█████████▋| 1937/2000 [9:06:07<17:41, 16.85s/it]
 97%|█████████▋| 1938/2000 [9:06:24<17:24, 16.85s/it]
                                                     

 97%|█████████▋| 1938/2000 [9:06:24<17:24, 16.85s/it]
 97%|█████████▋| 1939/2000 [9:06:41<17:09, 16.88s/it]
                                                     

 97%|█████████▋| 1939/2000 [9:06:41<17:09, 16.88s/it]
 97%|█████████▋| 1940/2000 [9:06:58<16:52, 16.87s/it]
                                                     

 97%|█████████▋| 1940/2000 [9:06:58<16:52, 16.87s/it]
 97%|█████████▋| 1941/2000 [9:07:14<16:34, 16.85s/it]
                                                     

 97%|█████████▋| 1941/2000 [9:07:14<16:34, 16.85s/it]
 97%|█████████▋| 1942/2000 [9:07:32<16:22, 16.94s/it]
                                                     

 97%|█████████▋| 1942/2000 [9:07:32<16:22, 16.94s/it]
 97%|█████████▋| 1943/2000 [9:07:49<16:04, 16.93s/it]
                                                     

 97%|█████████▋| 1943/2000 [9:07:49<16:04, 16.93s/it]
 97%|█████████▋| 1944/2000 [9:08:05<15:47, 16.92s/it]
                                                     

 97%|█████████▋| 1944/2000 [9:08:05<15:47, 16.92s/it]
 97%|█████████▋| 1945/2000 [9:08:22<15:29, 16.90s/it]
                                                     

 97%|█████████▋| 1945/2000 [9:08:22<15:29, 16.90s/it]
 97%|█████████▋| 1946/2000 [9:08:39<15:12, 16.90s/it]
                                                     

 97%|█████████▋| 1946/2000 [9:08:39<15:12, 16.90s/it]
 97%|█████████▋| 1947/2000 [9:08:56<14:54, 16.87s/it]
                                                     

 97%|█████████▋| 1947/2000 [9:08:56<14:54, 16.87s/it]
 97%|█████████▋| 1948/2000 [9:09:13<14:37, 16.88s/it]
                                                     

 97%|█████████▋| 1948/2000 [9:09:13<14:37, 16.88s/it]
 97%|█████████▋| 1949/2000 [9:09:30<14:22, 16.91s/it]
                                                     

 97%|█████████▋| 1949/2000 [9:09:30<14:22, 16.91s/it]
 98%|█████████▊| 1950/2000 [9:09:47<14:04, 16.88s/it]
                                                     

 98%|█████████▊| 1950/2000 [9:09:47<14:04, 16.88s/it]
 98%|█████████▊| 1951/2000 [9:10:04<13:46, 16.87s/it]
                                                     

 98%|█████████▊| 1951/2000 [9:10:04<13:46, 16.87s/it]
 98%|█████████▊| 1952/2000 [9:10:20<13:30, 16.88s/it]
                                                     

 98%|█████████▊| 1952/2000 [9:10:20<13:30, 16.88s/it]
 98%|█████████▊| 1953/2000 [9:10:37<13:14, 16.90s/it]
                                                     

 98%|█████████▊| 1953/2000 [9:10:37<13:14, 16.90s/it]
 98%|█████████▊| 1954/2000 [9:10:54<12:56, 16.88s/it]
                                                     

 98%|█████████▊| 1954/2000 [9:10:54<12:56, 16.88s/it]
 98%|█████████▊| 1955/2000 [9:11:11<12:40, 16.89s/it]
                                                     

 98%|█████████▊| 1955/2000 [9:11:11<12:40, 16.89s/it]
 98%|█████████▊| 1956/2000 [9:11:28<12:22, 16.87s/it]
                                                     

 98%|█████████▊| 1956/2000 [9:11:28<12:22, 16.87s/it]
 98%|█████████▊| 1957/2000 [9:11:45<12:05, 16.88s/it]
                                                     

 98%|█████████▊| 1957/2000 [9:11:45<12:05, 16.88s/it]
 98%|█████████▊| 1958/2000 [9:12:02<11:49, 16.89s/it]
                                                     

 98%|█████████▊| 1958/2000 [9:12:02<11:49, 16.89s/it]
 98%|█████████▊| 1959/2000 [9:12:19<11:32, 16.88s/it]
                                                     

 98%|█████████▊| 1959/2000 [9:12:19<11:32, 16.88s/it]
 98%|█████████▊| 1960/2000 [9:12:35<11:13, 16.85s/it]
                                                     

 98%|█████████▊| 1960/2000 [9:12:35<11:13, 16.85s/it]
 98%|█████████▊| 1961/2000 [9:12:52<10:57, 16.86s/it]
                                                     

 98%|█████████▊| 1961/2000 [9:12:52<10:57, 16.86s/it]
 98%|█████████▊| 1962/2000 [9:13:09<10:41, 16.87s/it]
                                                     

 98%|█████████▊| 1962/2000 [9:13:09<10:41, 16.87s/it]
 98%|█████████▊| 1963/2000 [9:13:26<10:24, 16.89s/it]
                                                     

 98%|█████████▊| 1963/2000 [9:13:26<10:24, 16.89s/it]
 98%|█████████▊| 1964/2000 [9:13:43<10:07, 16.87s/it]
                                                     

 98%|█████████▊| 1964/2000 [9:13:43<10:07, 16.87s/it]
 98%|█████████▊| 1965/2000 [9:14:00<09:49, 16.85s/it]
                                                     

 98%|█████████▊| 1965/2000 [9:14:00<09:49, 16.85s/it]
 98%|█████████▊| 1966/2000 [9:14:17<09:33, 16.86s/it]
                                                     

 98%|█████████▊| 1966/2000 [9:14:17<09:33, 16.86s/it]
 98%|█████████▊| 1967/2000 [9:14:34<09:16, 16.87s/it]
                                                     

 98%|█████████▊| 1967/2000 [9:14:34<09:16, 16.87s/it]
 98%|█████████▊| 1968/2000 [9:14:50<08:58, 16.84s/it]
                                                     

 98%|█████████▊| 1968/2000 [9:14:50<08:58, 16.84s/it]
 98%|█████████▊| 1969/2000 [9:15:07<08:41, 16.83s/it]
                                                     

 98%|█████████▊| 1969/2000 [9:15:07<08:41, 16.83s/it]
 98%|█████████▊| 1970/2000 [9:15:24<08:25, 16.85s/it]
                                                     

 98%|█████████▊| 1970/2000 [9:15:24<08:25, 16.85s/it]
 99%|█████████▊| 1971/2000 [9:15:41<08:09, 16.88s/it]
                                                     

 99%|█████████▊| 1971/2000 [9:15:41<08:09, 16.88s/it]
 99%|█████████▊| 1972/2000 [9:15:58<07:52, 16.87s/it]
                                                     

 99%|█████████▊| 1972/2000 [9:15:58<07:52, 16.87s/it]
 99%|█████████▊| 1973/2000 [9:16:15<07:34, 16.82s/it]
                                                     

 99%|█████████▊| 1973/2000 [9:16:15<07:34, 16.82s/it]
 99%|█████████▊| 1974/2000 [9:16:32<07:20, 16.94s/it]
                                                     

 99%|█████████▊| 1974/2000 [9:16:32<07:20, 16.94s/it]
 99%|█████████▉| 1975/2000 [9:16:49<07:03, 16.93s/it]
                                                     

 99%|█████████▉| 1975/2000 [9:16:49<07:03, 16.93s/it]
 99%|█████████▉| 1976/2000 [9:17:06<06:46, 16.92s/it]
                                                     

 99%|█████████▉| 1976/2000 [9:17:06<06:46, 16.92s/it]
 99%|█████████▉| 1977/2000 [9:17:22<06:28, 16.90s/it]
                                                     

 99%|█████████▉| 1977/2000 [9:17:22<06:28, 16.90s/it]
 99%|█████████▉| 1978/2000 [9:17:39<06:11, 16.90s/it]
                                                     

 99%|█████████▉| 1978/2000 [9:17:39<06:11, 16.90s/it]
 99%|█████████▉| 1979/2000 [9:17:56<05:54, 16.88s/it]
                                                     

 99%|█████████▉| 1979/2000 [9:17:56<05:54, 16.88s/it]
 99%|█████████▉| 1980/2000 [9:18:13<05:37, 16.85s/it]
                                                     

 99%|█████████▉| 1980/2000 [9:18:13<05:37, 16.85s/it]
 99%|█████████▉| 1981/2000 [9:18:30<05:20, 16.86s/it]
                                                     

 99%|█████████▉| 1981/2000 [9:18:30<05:20, 16.86s/it]
 99%|█████████▉| 1982/2000 [9:18:47<05:03, 16.88s/it]
                                                     

 99%|█████████▉| 1982/2000 [9:18:47<05:03, 16.88s/it]
 99%|█████████▉| 1983/2000 [9:19:04<04:46, 16.86s/it]
                                                     

 99%|█████████▉| 1983/2000 [9:19:04<04:46, 16.86s/it]
 99%|█████████▉| 1984/2000 [9:19:20<04:29, 16.86s/it]
                                                     

 99%|█████████▉| 1984/2000 [9:19:20<04:29, 16.86s/it]
 99%|█████████▉| 1985/2000 [9:19:37<04:13, 16.88s/it]
                                                     

 99%|█████████▉| 1985/2000 [9:19:37<04:13, 16.88s/it]
 99%|█████████▉| 1986/2000 [9:19:54<03:56, 16.86s/it]
                                                     

 99%|█████████▉| 1986/2000 [9:19:54<03:56, 16.86s/it]
 99%|█████████▉| 1987/2000 [9:20:11<03:39, 16.88s/it]
                                                     

 99%|█████████▉| 1987/2000 [9:20:11<03:39, 16.88s/it]
 99%|█████████▉| 1988/2000 [9:20:28<03:22, 16.89s/it]
                                                     

 99%|█████████▉| 1988/2000 [9:20:28<03:22, 16.89s/it]
 99%|█████████▉| 1989/2000 [9:20:45<03:05, 16.86s/it]
                                                     

 99%|█████████▉| 1989/2000 [9:20:45<03:05, 16.86s/it]
100%|█████████▉| 1990/2000 [9:21:02<02:48, 16.83s/it]
                                                     

100%|█████████▉| 1990/2000 [9:21:02<02:48, 16.83s/it]
100%|█████████▉| 1991/2000 [9:21:18<02:31, 16.82s/it]
                                                     

100%|█████████▉| 1991/2000 [9:21:18<02:31, 16.82s/it]
100%|█████████▉| 1992/2000 [9:21:35<02:14, 16.80s/it]
                                                     

100%|█████████▉| 1992/2000 [9:21:35<02:14, 16.80s/it]
100%|█████████▉| 1993/2000 [9:21:52<01:57, 16.79s/it]
                                                     

100%|█████████▉| 1993/2000 [9:21:52<01:57, 16.79s/it]
100%|█████████▉| 1994/2000 [9:22:09<01:40, 16.79s/it]
                                                     

100%|█████████▉| 1994/2000 [9:22:09<01:40, 16.79s/it]
100%|█████████▉| 1995/2000 [9:22:25<01:23, 16.77s/it]
                                                     

100%|█████████▉| 1995/2000 [9:22:25<01:23, 16.77s/it]
100%|█████████▉| 1996/2000 [9:22:42<01:07, 16.78s/it]
                                                     

100%|█████████▉| 1996/2000 [9:22:42<01:07, 16.78s/it]
100%|█████████▉| 1997/2000 [9:22:59<00:50, 16.79s/it]
                                                     

100%|█████████▉| 1997/2000 [9:22:59<00:50, 16.79s/it]
100%|█████████▉| 1998/2000 [9:23:16<00:33, 16.79s/it]
                                                     

100%|█████████▉| 1998/2000 [9:23:16<00:33, 16.79s/it]
100%|█████████▉| 1999/2000 [9:23:32<00:16, 16.78s/it]
                                                     

100%|█████████▉| 1999/2000 [9:23:33<00:16, 16.78s/it]
100%|██████████| 2000/2000 [9:23:49<00:00, 16.82s/it]
                                                     

100%|██████████| 2000/2000 [9:23:49<00:00, 16.82s/it]
                                                     

100%|██████████| 2000/2000 [9:24:14<00:00, 16.82s/it]
100%|██████████| 2000/2000 [9:24:14<00:00, 16.93s/it]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.99it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.53it/s]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.language_model.lm_head.weight', 'model.language_model.model.embed_tokens.weight', 'model.language_model.model.layers.0.input_layernorm.weight', 'model.language_model.model.layers.0.mlp.down_proj.weight', 'model.language_model.model.layers.0.mlp.gate_proj.weight', 'model.language_model.model.layers.0.mlp.up_proj.weight', 'model.language_model.model.layers.0.post_attention_layernorm.weight', 'model.language_model.model.layers.0.self_attn.k_proj.weight', 'model.language_model.model.layers.0.self_attn.o_proj.weight', 'model.language_model.model.layers.0.self_attn.q_proj.weight', 'model.language_model.model.layers.0.self_attn.v_proj.weight', 'model.language_model.model.layers.1.input_layernorm.weight', 'model.language_model.model.layers.1.mlp.down_proj.weight', 'model.language_model.model.layers.1.mlp.gate_proj.weight', 'model.language_model.model.layers.1.mlp.up_proj.weight', 'model.language_model.model.layers.1.post_attention_layernorm.weight', 'model.language_model.model.layers.1.self_attn.k_proj.weight', 'model.language_model.model.layers.1.self_attn.o_proj.weight', 'model.language_model.model.layers.1.self_attn.q_proj.weight', 'model.language_model.model.layers.1.self_attn.v_proj.weight', 'model.language_model.model.layers.10.input_layernorm.weight', 'model.language_model.model.layers.10.mlp.down_proj.weight', 'model.language_model.model.layers.10.mlp.gate_proj.weight', 'model.language_model.model.layers.10.mlp.up_proj.weight', 'model.language_model.model.layers.10.post_attention_layernorm.weight', 'model.language_model.model.layers.10.self_attn.k_proj.weight', 'model.language_model.model.layers.10.self_attn.o_proj.weight', 'model.language_model.model.layers.10.self_attn.q_proj.weight', 'model.language_model.model.layers.10.self_attn.v_proj.weight', 'model.language_model.model.layers.11.input_layernorm.weight', 'model.language_model.model.layers.11.mlp.down_proj.weight', 'model.language_model.model.layers.11.mlp.gate_proj.weight', 'model.language_model.model.layers.11.mlp.up_proj.weight', 'model.language_model.model.layers.11.post_attention_layernorm.weight', 'model.language_model.model.layers.11.self_attn.k_proj.weight', 'model.language_model.model.layers.11.self_attn.o_proj.weight', 'model.language_model.model.layers.11.self_attn.q_proj.weight', 'model.language_model.model.layers.11.self_attn.v_proj.weight', 'model.language_model.model.layers.12.input_layernorm.weight', 'model.language_model.model.layers.12.mlp.down_proj.weight', 'model.language_model.model.layers.12.mlp.gate_proj.weight', 'model.language_model.model.layers.12.mlp.up_proj.weight', 'model.language_model.model.layers.12.post_attention_layernorm.weight', 'model.language_model.model.layers.12.self_attn.k_proj.weight', 'model.language_model.model.layers.12.self_attn.o_proj.weight', 'model.language_model.model.layers.12.self_attn.q_proj.weight', 'model.language_model.model.layers.12.self_attn.v_proj.weight', 'model.language_model.model.layers.13.input_layernorm.weight', 'model.language_model.model.layers.13.mlp.down_proj.weight', 'model.language_model.model.layers.13.mlp.gate_proj.weight', 'model.language_model.model.layers.13.mlp.up_proj.weight', 'model.language_model.model.layers.13.post_attention_layernorm.weight', 'model.language_model.model.layers.13.self_attn.k_proj.weight', 'model.language_model.model.layers.13.self_attn.o_proj.weight', 'model.language_model.model.layers.13.self_attn.q_proj.weight', 'model.language_model.model.layers.13.self_attn.v_proj.weight', 'model.language_model.model.layers.14.input_layernorm.weight', 'model.language_model.model.layers.14.mlp.down_proj.weight', 'model.language_model.model.layers.14.mlp.gate_proj.weight', 'model.language_model.model.layers.14.mlp.up_proj.weight', 'model.language_model.model.layers.14.post_attention_layernorm.weight', 'model.language_model.model.layers.14.self_attn.k_proj.weight', 'model.language_model.model.layers.14.self_attn.o_proj.weight', 'model.language_model.model.layers.14.self_attn.q_proj.weight', 'model.language_model.model.layers.14.self_attn.v_proj.weight', 'model.language_model.model.layers.15.input_layernorm.weight', 'model.language_model.model.layers.15.mlp.down_proj.weight', 'model.language_model.model.layers.15.mlp.gate_proj.weight', 'model.language_model.model.layers.15.mlp.up_proj.weight', 'model.language_model.model.layers.15.post_attention_layernorm.weight', 'model.language_model.model.layers.15.self_attn.k_proj.weight', 'model.language_model.model.layers.15.self_attn.o_proj.weight', 'model.language_model.model.layers.15.self_attn.q_proj.weight', 'model.language_model.model.layers.15.self_attn.v_proj.weight', 'model.language_model.model.layers.16.input_layernorm.weight', 'model.language_model.model.layers.16.mlp.down_proj.weight', 'model.language_model.model.layers.16.mlp.gate_proj.weight', 'model.language_model.model.layers.16.mlp.up_proj.weight', 'model.language_model.model.layers.16.post_attention_layernorm.weight', 'model.language_model.model.layers.16.self_attn.k_proj.weight', 'model.language_model.model.layers.16.self_attn.o_proj.weight', 'model.language_model.model.layers.16.self_attn.q_proj.weight', 'model.language_model.model.layers.16.self_attn.v_proj.weight', 'model.language_model.model.layers.17.input_layernorm.weight', 'model.language_model.model.layers.17.mlp.down_proj.weight', 'model.language_model.model.layers.17.mlp.gate_proj.weight', 'model.language_model.model.layers.17.mlp.up_proj.weight', 'model.language_model.model.layers.17.post_attention_layernorm.weight', 'model.language_model.model.layers.17.self_attn.k_proj.weight', 'model.language_model.model.layers.17.self_attn.o_proj.weight', 'model.language_model.model.layers.17.self_attn.q_proj.weight', 'model.language_model.model.layers.17.self_attn.v_proj.weight', 'model.language_model.model.layers.18.input_layernorm.weight', 'model.language_model.model.layers.18.mlp.down_proj.weight', 'model.language_model.model.layers.18.mlp.gate_proj.weight', 'model.language_model.model.layers.18.mlp.up_proj.weight', 'model.language_model.model.layers.18.post_attention_layernorm.weight', 'model.language_model.model.layers.18.self_attn.k_proj.weight', 'model.language_model.model.layers.18.self_attn.o_proj.weight', 'model.language_model.model.layers.18.self_attn.q_proj.weight', 'model.language_model.model.layers.18.self_attn.v_proj.weight', 'model.language_model.model.layers.19.input_layernorm.weight', 'model.language_model.model.layers.19.mlp.down_proj.weight', 'model.language_model.model.layers.19.mlp.gate_proj.weight', 'model.language_model.model.layers.19.mlp.up_proj.weight', 'model.language_model.model.layers.19.post_attention_layernorm.weight', 'model.language_model.model.layers.19.self_attn.k_proj.weight', 'model.language_model.model.layers.19.self_attn.o_proj.weight', 'model.language_model.model.layers.19.self_attn.q_proj.weight', 'model.language_model.model.layers.19.self_attn.v_proj.weight', 'model.language_model.model.layers.2.input_layernorm.weight', 'model.language_model.model.layers.2.mlp.down_proj.weight', 'model.language_model.model.layers.2.mlp.gate_proj.weight', 'model.language_model.model.layers.2.mlp.up_proj.weight', 'model.language_model.model.layers.2.post_attention_layernorm.weight', 'model.language_model.model.layers.2.self_attn.k_proj.weight', 'model.language_model.model.layers.2.self_attn.o_proj.weight', 'model.language_model.model.layers.2.self_attn.q_proj.weight', 'model.language_model.model.layers.2.self_attn.v_proj.weight', 'model.language_model.model.layers.20.input_layernorm.weight', 'model.language_model.model.layers.20.mlp.down_proj.weight', 'model.language_model.model.layers.20.mlp.gate_proj.weight', 'model.language_model.model.layers.20.mlp.up_proj.weight', 'model.language_model.model.layers.20.post_attention_layernorm.weight', 'model.language_model.model.layers.20.self_attn.k_proj.weight', 'model.language_model.model.layers.20.self_attn.o_proj.weight', 'model.language_model.model.layers.20.self_attn.q_proj.weight', 'model.language_model.model.layers.20.self_attn.v_proj.weight', 'model.language_model.model.layers.21.input_layernorm.weight', 'model.language_model.model.layers.21.mlp.down_proj.weight', 'model.language_model.model.layers.21.mlp.gate_proj.weight', 'model.language_model.model.layers.21.mlp.up_proj.weight', 'model.language_model.model.layers.21.post_attention_layernorm.weight', 'model.language_model.model.layers.21.self_attn.k_proj.weight', 'model.language_model.model.layers.21.self_attn.o_proj.weight', 'model.language_model.model.layers.21.self_attn.q_proj.weight', 'model.language_model.model.layers.21.self_attn.v_proj.weight', 'model.language_model.model.layers.22.input_layernorm.weight', 'model.language_model.model.layers.22.mlp.down_proj.weight', 'model.language_model.model.layers.22.mlp.gate_proj.weight', 'model.language_model.model.layers.22.mlp.up_proj.weight', 'model.language_model.model.layers.22.post_attention_layernorm.weight', 'model.language_model.model.layers.22.self_attn.k_proj.weight', 'model.language_model.model.layers.22.self_attn.o_proj.weight', 'model.language_model.model.layers.22.self_attn.q_proj.weight', 'model.language_model.model.layers.22.self_attn.v_proj.weight', 'model.language_model.model.layers.23.input_layernorm.weight', 'model.language_model.model.layers.23.mlp.down_proj.weight', 'model.language_model.model.layers.23.mlp.gate_proj.weight', 'model.language_model.model.layers.23.mlp.up_proj.weight', 'model.language_model.model.layers.23.post_attention_layernorm.weight', 'model.language_model.model.layers.23.self_attn.k_proj.weight', 'model.language_model.model.layers.23.self_attn.o_proj.weight', 'model.language_model.model.layers.23.self_attn.q_proj.weight', 'model.language_model.model.layers.23.self_attn.v_proj.weight', 'model.language_model.model.layers.24.input_layernorm.weight', 'model.language_model.model.layers.24.mlp.down_proj.weight', 'model.language_model.model.layers.24.mlp.gate_proj.weight', 'model.language_model.model.layers.24.mlp.up_proj.weight', 'model.language_model.model.layers.24.post_attention_layernorm.weight', 'model.language_model.model.layers.24.self_attn.k_proj.weight', 'model.language_model.model.layers.24.self_attn.o_proj.weight', 'model.language_model.model.layers.24.self_attn.q_proj.weight', 'model.language_model.model.layers.24.self_attn.v_proj.weight', 'model.language_model.model.layers.25.input_layernorm.weight', 'model.language_model.model.layers.25.mlp.down_proj.weight', 'model.language_model.model.layers.25.mlp.gate_proj.weight', 'model.language_model.model.layers.25.mlp.up_proj.weight', 'model.language_model.model.layers.25.post_attention_layernorm.weight', 'model.language_model.model.layers.25.self_attn.k_proj.weight', 'model.language_model.model.layers.25.self_attn.o_proj.weight', 'model.language_model.model.layers.25.self_attn.q_proj.weight', 'model.language_model.model.layers.25.self_attn.v_proj.weight', 'model.language_model.model.layers.26.input_layernorm.weight', 'model.language_model.model.layers.26.mlp.down_proj.weight', 'model.language_model.model.layers.26.mlp.gate_proj.weight', 'model.language_model.model.layers.26.mlp.up_proj.weight', 'model.language_model.model.layers.26.post_attention_layernorm.weight', 'model.language_model.model.layers.26.self_attn.k_proj.weight', 'model.language_model.model.layers.26.self_attn.o_proj.weight', 'model.language_model.model.layers.26.self_attn.q_proj.weight', 'model.language_model.model.layers.26.self_attn.v_proj.weight', 'model.language_model.model.layers.27.input_layernorm.weight', 'model.language_model.model.layers.27.mlp.down_proj.weight', 'model.language_model.model.layers.27.mlp.gate_proj.weight', 'model.language_model.model.layers.27.mlp.up_proj.weight', 'model.language_model.model.layers.27.post_attention_layernorm.weight', 'model.language_model.model.layers.27.self_attn.k_proj.weight', 'model.language_model.model.layers.27.self_attn.o_proj.weight', 'model.language_model.model.layers.27.self_attn.q_proj.weight', 'model.language_model.model.layers.27.self_attn.v_proj.weight', 'model.language_model.model.layers.28.input_layernorm.weight', 'model.language_model.model.layers.28.mlp.down_proj.weight', 'model.language_model.model.layers.28.mlp.gate_proj.weight', 'model.language_model.model.layers.28.mlp.up_proj.weight', 'model.language_model.model.layers.28.post_attention_layernorm.weight', 'model.language_model.model.layers.28.self_attn.k_proj.weight', 'model.language_model.model.layers.28.self_attn.o_proj.weight', 'model.language_model.model.layers.28.self_attn.q_proj.weight', 'model.language_model.model.layers.28.self_attn.v_proj.weight', 'model.language_model.model.layers.29.input_layernorm.weight', 'model.language_model.model.layers.29.mlp.down_proj.weight', 'model.language_model.model.layers.29.mlp.gate_proj.weight', 'model.language_model.model.layers.29.mlp.up_proj.weight', 'model.language_model.model.layers.29.post_attention_layernorm.weight', 'model.language_model.model.layers.29.self_attn.k_proj.weight', 'model.language_model.model.layers.29.self_attn.o_proj.weight', 'model.language_model.model.layers.29.self_attn.q_proj.weight', 'model.language_model.model.layers.29.self_attn.v_proj.weight', 'model.language_model.model.layers.3.input_layernorm.weight', 'model.language_model.model.layers.3.mlp.down_proj.weight', 'model.language_model.model.layers.3.mlp.gate_proj.weight', 'model.language_model.model.layers.3.mlp.up_proj.weight', 'model.language_model.model.layers.3.post_attention_layernorm.weight', 'model.language_model.model.layers.3.self_attn.k_proj.weight', 'model.language_model.model.layers.3.self_attn.o_proj.weight', 'model.language_model.model.layers.3.self_attn.q_proj.weight', 'model.language_model.model.layers.3.self_attn.v_proj.weight', 'model.language_model.model.layers.30.input_layernorm.weight', 'model.language_model.model.layers.30.mlp.down_proj.weight', 'model.language_model.model.layers.30.mlp.gate_proj.weight', 'model.language_model.model.layers.30.mlp.up_proj.weight', 'model.language_model.model.layers.30.post_attention_layernorm.weight', 'model.language_model.model.layers.30.self_attn.k_proj.weight', 'model.language_model.model.layers.30.self_attn.o_proj.weight', 'model.language_model.model.layers.30.self_attn.q_proj.weight', 'model.language_model.model.layers.30.self_attn.v_proj.weight', 'model.language_model.model.layers.31.input_layernorm.weight', 'model.language_model.model.layers.31.mlp.down_proj.weight', 'model.language_model.model.layers.31.mlp.gate_proj.weight', 'model.language_model.model.layers.31.mlp.up_proj.weight', 'model.language_model.model.layers.31.post_attention_layernorm.weight', 'model.language_model.model.layers.31.self_attn.k_proj.weight', 'model.language_model.model.layers.31.self_attn.o_proj.weight', 'model.language_model.model.layers.31.self_attn.q_proj.weight', 'model.language_model.model.layers.31.self_attn.v_proj.weight', 'model.language_model.model.layers.4.input_layernorm.weight', 'model.language_model.model.layers.4.mlp.down_proj.weight', 'model.language_model.model.layers.4.mlp.gate_proj.weight', 'model.language_model.model.layers.4.mlp.up_proj.weight', 'model.language_model.model.layers.4.post_attention_layernorm.weight', 'model.language_model.model.layers.4.self_attn.k_proj.weight', 'model.language_model.model.layers.4.self_attn.o_proj.weight', 'model.language_model.model.layers.4.self_attn.q_proj.weight', 'model.language_model.model.layers.4.self_attn.v_proj.weight', 'model.language_model.model.layers.5.input_layernorm.weight', 'model.language_model.model.layers.5.mlp.down_proj.weight', 'model.language_model.model.layers.5.mlp.gate_proj.weight', 'model.language_model.model.layers.5.mlp.up_proj.weight', 'model.language_model.model.layers.5.post_attention_layernorm.weight', 'model.language_model.model.layers.5.self_attn.k_proj.weight', 'model.language_model.model.layers.5.self_attn.o_proj.weight', 'model.language_model.model.layers.5.self_attn.q_proj.weight', 'model.language_model.model.layers.5.self_attn.v_proj.weight', 'model.language_model.model.layers.6.input_layernorm.weight', 'model.language_model.model.layers.6.mlp.down_proj.weight', 'model.language_model.model.layers.6.mlp.gate_proj.weight', 'model.language_model.model.layers.6.mlp.up_proj.weight', 'model.language_model.model.layers.6.post_attention_layernorm.weight', 'model.language_model.model.layers.6.self_attn.k_proj.weight', 'model.language_model.model.layers.6.self_attn.o_proj.weight', 'model.language_model.model.layers.6.self_attn.q_proj.weight', 'model.language_model.model.layers.6.self_attn.v_proj.weight', 'model.language_model.model.layers.7.input_layernorm.weight', 'model.language_model.model.layers.7.mlp.down_proj.weight', 'model.language_model.model.layers.7.mlp.gate_proj.weight', 'model.language_model.model.layers.7.mlp.up_proj.weight', 'model.language_model.model.layers.7.post_attention_layernorm.weight', 'model.language_model.model.layers.7.self_attn.k_proj.weight', 'model.language_model.model.layers.7.self_attn.o_proj.weight', 'model.language_model.model.layers.7.self_attn.q_proj.weight', 'model.language_model.model.layers.7.self_attn.v_proj.weight', 'model.language_model.model.layers.8.input_layernorm.weight', 'model.language_model.model.layers.8.mlp.down_proj.weight', 'model.language_model.model.layers.8.mlp.gate_proj.weight', 'model.language_model.model.layers.8.mlp.up_proj.weight', 'model.language_model.model.layers.8.post_attention_layernorm.weight', 'model.language_model.model.layers.8.self_attn.k_proj.weight', 'model.language_model.model.layers.8.self_attn.o_proj.weight', 'model.language_model.model.layers.8.self_attn.q_proj.weight', 'model.language_model.model.layers.8.self_attn.v_proj.weight', 'model.language_model.model.layers.9.input_layernorm.weight', 'model.language_model.model.layers.9.mlp.down_proj.weight', 'model.language_model.model.layers.9.mlp.gate_proj.weight', 'model.language_model.model.layers.9.mlp.up_proj.weight', 'model.language_model.model.layers.9.post_attention_layernorm.weight', 'model.language_model.model.layers.9.self_attn.k_proj.weight', 'model.language_model.model.layers.9.self_attn.o_proj.weight', 'model.language_model.model.layers.9.self_attn.q_proj.weight', 'model.language_model.model.layers.9.self_attn.v_proj.weight', 'model.language_model.model.norm.weight', 'model.multi_modal_projector.linear_1.bias', 'model.multi_modal_projector.linear_1.weight', 'model.multi_modal_projector.linear_2.bias', 'model.multi_modal_projector.linear_2.weight', 'model.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_model.pre_layrnorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LoRA adapter + projector saved to: /home/zl986/backdoor-test/llava-driving-ft-lora-8
